{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "91054ad3",
      "metadata": {},
      "source": [
        "# Facial Expression, Valence, and Arousal Recognition\n",
        "\n",
        "Multitask learning pipeline for classifying facial expressions and regressing valence/arousal scores from the provided dataset. The notebook follows a modular Keras workflow with reusable components for data loading, training, evaluation, and qualitative analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a3fdba1",
      "metadata": {},
      "source": [
        "## Project Overview\n",
        "\n",
        "- **Goal:** Train CNN baselines (transfer-learned) that predict eight categorical emotions together with valence and arousal scores.\n",
        "- **Dataset inputs:** Cropped 224x224 RGB images, per-image expression labels, valence/arousal values, and 68-point facial landmarks stored in `.npy` files.\n",
        "- **Outputs:** Quantitative comparison across at least two CNN backbones, plus qualitative inspection of correct/incorrect predictions.\n",
        "- **Structure:** The workflow uses an idiomatic Keras setup with data modules, model factories, experiment runner, and analysis utilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e4003ef6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: install dependencies if running in a fresh environment.\n",
        "# Execute only once per runtime or replace with your preferred versions.\n",
        "# !pip install -q numpy pandas scikit-learn matplotlib seaborn tensorflow tensorflow-addons\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f7e142c",
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "import math\n",
        "import random\n",
        "from dataclasses import dataclass, asdict\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    f1_score,\n",
        "    mean_absolute_error,\n",
        "    mean_squared_error,\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "SEED = 2024\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "DATA_ROOT = Path('Dataset')\n",
        "ANNOTATION_DIR = DATA_ROOT / 'annotations'\n",
        "IMAGES_DIR = DATA_ROOT / 'images'\n",
        "IMAGE_SIZE = (224, 224)\n",
        "NUM_CLASSES = 8\n",
        "BATCH_SIZE = 32\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "CLASS_ID_TO_NAME = {\n",
        "    0: 'Neutral',\n",
        "    1: 'Happy',\n",
        "    2: 'Sad',\n",
        "    3: 'Surprise',\n",
        "    4: 'Fear',\n",
        "    5: 'Disgust',\n",
        "    6: 'Anger',\n",
        "    7: 'Contempt',\n",
        "}\n",
        "CLASS_NAMES = [CLASS_ID_TO_NAME[i] for i in range(NUM_CLASSES)]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e664c2c5",
      "metadata": {},
      "source": [
        "## Dataset Ingestion\n",
        "\n",
        "Build a structured dataframe that links each image to its expression, valence, arousal, and landmark annotation paths. The loader keeps the implementation modular so we can cache the dataframe and reuse it across experiments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f41acb69",
      "metadata": {},
      "outputs": [],
      "source": [
        "class AffectDatasetBuilder:\n",
        "    \"Utility class that scans the dataset folders and returns a pandas dataframe.\"\n",
        "\n",
        "    def __init__(self, annotation_dir: Path, image_dir: Path, class_map: Dict[int, str]):\n",
        "        self.annotation_dir = annotation_dir\n",
        "        self.image_dir = image_dir\n",
        "        self.class_map = class_map\n",
        "\n",
        "    @staticmethod\n",
        "    def _as_scalar(array: np.ndarray) -> float:\n",
        "        if array.ndim == 0:\n",
        "            return float(array)\n",
        "        if array.size == 1:\n",
        "            return float(array.reshape(-1)[0])\n",
        "        return array.astype(float)\n",
        "\n",
        "    def build_dataframe(self, drop_uncertain: bool = True) -> pd.DataFrame:\n",
        "        records: List[Dict[str, object]] = []\n",
        "        for exp_path in sorted(self.annotation_dir.glob('*_exp.npy')):\n",
        "            sample_id = exp_path.stem.replace('_exp', '')\n",
        "            image_path = self.image_dir / f\"{sample_id}.jpg\"\n",
        "            if not image_path.exists():\n",
        "                continue\n",
        "            val_path = self.annotation_dir / f\"{sample_id}_val.npy\"\n",
        "            aro_path = self.annotation_dir / f\"{sample_id}_aro.npy\"\n",
        "            lnd_path = self.annotation_dir / f\"{sample_id}_lnd.npy\"\n",
        "            expression = self._as_scalar(np.load(exp_path))\n",
        "            valence = self._as_scalar(np.load(val_path))\n",
        "            arousal = self._as_scalar(np.load(aro_path))\n",
        "            record = {\n",
        "                'sample_id': sample_id,\n",
        "                'image_path': image_path.as_posix(),\n",
        "                'expression': int(expression),\n",
        "                'valence': float(valence),\n",
        "                'arousal': float(arousal),\n",
        "                'landmarks_path': lnd_path.as_posix(),\n",
        "            }\n",
        "            records.append(record)\n",
        "\n",
        "        df = pd.DataFrame.from_records(records)\n",
        "        if drop_uncertain:\n",
        "            df = df[(df['valence'] >= -1.0) & (df['arousal'] >= -1.0)]\n",
        "        df['expression_label'] = df['expression'].map(self.class_map)\n",
        "        df = df.sort_values('sample_id').reset_index(drop=True)\n",
        "        return df\n",
        "\n",
        "    def describe(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        summary = df.describe(include='all').transpose()\n",
        "        return summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "4a38353e",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sample_id</th>\n",
              "      <th>image_path</th>\n",
              "      <th>expression</th>\n",
              "      <th>valence</th>\n",
              "      <th>arousal</th>\n",
              "      <th>landmarks_path</th>\n",
              "      <th>expression_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Dataset/images/0.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.176846</td>\n",
              "      <td>-0.077640</td>\n",
              "      <td>Dataset/annotations/0_lnd.npy</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Dataset/images/1.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.367789</td>\n",
              "      <td>0.183895</td>\n",
              "      <td>Dataset/annotations/1_lnd.npy</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10</td>\n",
              "      <td>Dataset/images/10.jpg</td>\n",
              "      <td>7</td>\n",
              "      <td>-0.648471</td>\n",
              "      <td>0.658149</td>\n",
              "      <td>Dataset/annotations/10_lnd.npy</td>\n",
              "      <td>Contempt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>100</td>\n",
              "      <td>Dataset/images/100.jpg</td>\n",
              "      <td>3</td>\n",
              "      <td>0.150794</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>Dataset/annotations/100_lnd.npy</td>\n",
              "      <td>Surprise</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1001</td>\n",
              "      <td>Dataset/images/1001.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.135501</td>\n",
              "      <td>0.004839</td>\n",
              "      <td>Dataset/annotations/1001_lnd.npy</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  sample_id               image_path  expression   valence   arousal  \\\n",
              "0         0     Dataset/images/0.jpg           0 -0.176846 -0.077640   \n",
              "1         1     Dataset/images/1.jpg           0 -0.367789  0.183895   \n",
              "2        10    Dataset/images/10.jpg           7 -0.648471  0.658149   \n",
              "3       100   Dataset/images/100.jpg           3  0.150794  0.666667   \n",
              "4      1001  Dataset/images/1001.jpg           0 -0.135501  0.004839   \n",
              "\n",
              "                     landmarks_path expression_label  \n",
              "0     Dataset/annotations/0_lnd.npy          Neutral  \n",
              "1     Dataset/annotations/1_lnd.npy          Neutral  \n",
              "2    Dataset/annotations/10_lnd.npy         Contempt  \n",
              "3   Dataset/annotations/100_lnd.npy         Surprise  \n",
              "4  Dataset/annotations/1001_lnd.npy          Neutral  "
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset_builder = AffectDatasetBuilder(ANNOTATION_DIR, IMAGES_DIR, CLASS_ID_TO_NAME)\n",
        "metadata_df = dataset_builder.build_dataframe()\n",
        "metadata_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "54362e91",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total usable samples: 3,999\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x400 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "print(f'Total usable samples: {len(metadata_df):,}')\n",
        "metadata_df['expression_label'].value_counts().plot(kind='bar', figsize=(8, 4), title='Label Distribution')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09a3a33f",
      "metadata": {},
      "source": [
        "## Train / Validation / Test Split\n",
        "\n",
        "The dataset ships with training and test annotations, but we create an explicit validation set for model selection. Adjust the split ratio or the stratification strategy if you are using a predefined partition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "fc208cf9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: 3199 | Val: 400 | Test: 400\n"
          ]
        }
      ],
      "source": [
        "train_df, holdout_df = train_test_split(\n",
        "    metadata_df, test_size=0.2, stratify=metadata_df['expression'], random_state=SEED\n",
        ")\n",
        "val_df, test_df = train_test_split(\n",
        "    holdout_df, test_size=0.5, stratify=holdout_df['expression'], random_state=SEED\n",
        ")\n",
        "print(f'Train: {len(train_df)} | Val: {len(val_df)} | Test: {len(test_df)}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3227e69d",
      "metadata": {},
      "source": [
        "## Data Module\n",
        "\n",
        "Build a reusable tf.data pipeline with on-the-fly augmentation for the training split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "5cbbd17e",
      "metadata": {},
      "outputs": [],
      "source": [
        "class AffectDataModule:\n",
        "    \"Prepares tf.data.Dataset objects for training, validation, and testing.\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        train_df: pd.DataFrame,\n",
        "        val_df: pd.DataFrame,\n",
        "        test_df: pd.DataFrame,\n",
        "        image_size: Tuple[int, int] = IMAGE_SIZE,\n",
        "        batch_size: int = BATCH_SIZE,\n",
        "        augment: bool = True,\n",
        "    ):\n",
        "        self.splits = {\n",
        "            'train': train_df.copy(),\n",
        "            'val': val_df.copy(),\n",
        "            'test': test_df.copy(),\n",
        "        }\n",
        "        self.image_size = image_size\n",
        "        self.batch_size = batch_size\n",
        "        self.augment = augment\n",
        "        self.augmenter = tf.keras.Sequential(\n",
        "            [\n",
        "                tf.keras.layers.RandomFlip('horizontal'),\n",
        "                tf.keras.layers.RandomRotation(0.1),\n",
        "                tf.keras.layers.RandomZoom(0.1),\n",
        "                tf.keras.layers.RandomBrightness(0.1),\n",
        "            ],\n",
        "            name='augmenter',\n",
        "        )\n",
        "\n",
        "    def _load_image(self, path: tf.Tensor) -> tf.Tensor:\n",
        "        image_bytes = tf.io.read_file(path)\n",
        "        image = tf.image.decode_jpeg(image_bytes, channels=3)\n",
        "        image = tf.image.resize(tf.cast(image, tf.float32), self.image_size)\n",
        "        image = image / 255.0\n",
        "        return image\n",
        "\n",
        "    def _format_example(self, training: bool):\n",
        "        def formatter(path, expr, val, aro):\n",
        "            image = self._load_image(path)\n",
        "            if training and self.augment:\n",
        "                image = self.augmenter(image, training=True)\n",
        "            targets = {\n",
        "                'expression_head': tf.cast(expr, tf.int32),\n",
        "                'valence_head': tf.cast(tf.expand_dims(val, -1), tf.float32),\n",
        "                'arousal_head': tf.cast(tf.expand_dims(aro, -1), tf.float32),\n",
        "            }\n",
        "            return image, targets\n",
        "\n",
        "        return formatter\n",
        "\n",
        "    def get_dataset(self, split: str, with_labels: bool = True, training: bool = False) -> tf.data.Dataset:\n",
        "        df = self.splits[split]\n",
        "        paths = df['image_path'].astype(str).values\n",
        "        expressions = df['expression'].astype(np.int32).values\n",
        "        valence = df['valence'].astype(np.float32).values\n",
        "        arousal = df['arousal'].astype(np.float32).values\n",
        "\n",
        "        ds = tf.data.Dataset.from_tensor_slices((paths, expressions, valence, arousal))\n",
        "        if training:\n",
        "            ds = ds.shuffle(buffer_size=len(df), seed=SEED)\n",
        "        if with_labels:\n",
        "            ds = ds.map(self._format_example(training), num_parallel_calls=AUTOTUNE)\n",
        "        else:\n",
        "            def only_images(path, expr, val, aro):\n",
        "                image, _ = self._format_example(False)(path, expr, val, aro)\n",
        "                return image\n",
        "\n",
        "            ds = ds.map(only_images, num_parallel_calls=AUTOTUNE)\n",
        "        ds = ds.batch(self.batch_size).prefetch(AUTOTUNE)\n",
        "        return ds\n",
        "\n",
        "    def summary(self) -> pd.DataFrame:\n",
        "        return pd.DataFrame({k: [len(v)] for k, v in self.splits.items()}, index=['num_samples']).T\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "6919d027",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-09-27 14:34:23.266816: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M3\n",
            "2025-09-27 14:34:23.266985: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
            "2025-09-27 14:34:23.266997: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
            "2025-09-27 14:34:23.267157: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
            "2025-09-27 14:34:23.267170: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>num_samples</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>train</th>\n",
              "      <td>3199</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>val</th>\n",
              "      <td>400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>test</th>\n",
              "      <td>400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       num_samples\n",
              "train         3199\n",
              "val            400\n",
              "test           400"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_module = AffectDataModule(train_df, val_df, test_df)\n",
        "data_module.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4f99c6e",
      "metadata": {},
      "source": [
        "## Model Factory\n",
        "\n",
        "Create flexible Keras models that share a convolutional backbone and branch into three heads. The classification head predicts the discrete emotion, while two regression heads output valence and arousal scores in the range [-1, 1]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3d86656",
      "metadata": {},
      "outputs": [],
      "source": [
        "BACKBONE_REGISTRY = {\n",
        "    'mobilenet_v2': {\n",
        "        'builder': tf.keras.applications.MobileNetV2,\n",
        "        'preprocess': tf.keras.applications.mobilenet_v2.preprocess_input,\n",
        "        'default_unfreeze': 100,\n",
        "    },\n",
        "    'resnet50': {\n",
        "        'builder': tf.keras.applications.ResNet50,\n",
        "        'preprocess': tf.keras.applications.resnet.preprocess_input,\n",
        "        'default_unfreeze': 140,\n",
        "    },\n",
        "    'efficientnet_b0': {\n",
        "        'builder': tf.keras.applications.EfficientNetB0,\n",
        "        'preprocess': tf.keras.applications.efficientnet.preprocess_input,\n",
        "        'default_unfreeze': 210,\n",
        "    },\n",
        "    'vgg16': {\n",
        "        'builder': tf.keras.applications.VGG16,\n",
        "        'preprocess': tf.keras.applications.vgg16.preprocess_input,\n",
        "        'default_unfreeze': 15,\n",
        "    },\n",
        "}\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class BaselineConfig:\n",
        "    name: str\n",
        "    backbone: str\n",
        "    initial_epochs: int = 12\n",
        "    fine_tune_epochs: int = 8\n",
        "    learning_rate: float = 1e-4\n",
        "    fine_tune_learning_rate: float = 1e-5\n",
        "    dropout: float = 0.3\n",
        "    loss_weights: Dict[str, float] | None = None\n",
        "    freeze_base: bool = True\n",
        "    fine_tune_at: Optional[int] = None\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.loss_weights is None:\n",
        "            self.loss_weights = {\n",
        "                'expression_head': 1.0,\n",
        "                'valence_head': 0.5,\n",
        "                'arousal_head': 0.5,\n",
        "            }\n",
        "        if self.fine_tune_at is None:\n",
        "            registry_entry = BACKBONE_REGISTRY.get(self.backbone)\n",
        "            if registry_entry:\n",
        "                self.fine_tune_at = registry_entry['default_unfreeze']\n",
        "\n",
        "\n",
        "class ModelFactory:\n",
        "    \"Factory that builds and compiles the multitask Keras model.\"\n",
        "\n",
        "    @staticmethod\n",
        "    def build(config: BaselineConfig, image_size: Tuple[int, int] = IMAGE_SIZE) -> tf.keras.Model:\n",
        "        entry = BACKBONE_REGISTRY[config.backbone]\n",
        "        preprocess = entry['preprocess']\n",
        "        backbone_fn = entry['builder']\n",
        "\n",
        "        inputs = tf.keras.Input(shape=(*image_size, 3), name='image')\n",
        "        x = tf.keras.layers.Lambda(lambda img: preprocess(img * 255.0), name='preprocess')(inputs)\n",
        "        base_model = backbone_fn(include_top=False, input_tensor=x, weights='imagenet')\n",
        "        base_model.trainable = not config.freeze_base\n",
        "\n",
        "        features = tf.keras.layers.GlobalAveragePooling2D(name='avg_pool')(base_model.output)\n",
        "        features = tf.keras.layers.Dropout(config.dropout, name='feature_dropout')(features)\n",
        "        features = tf.keras.layers.Dense(512, activation='relu', name='feature_projection')(features)\n",
        "        features = tf.keras.layers.BatchNormalization(name='feature_norm')(features)\n",
        "\n",
        "        expression_head = tf.keras.layers.Dense(\n",
        "            NUM_CLASSES,\n",
        "            activation='softmax',\n",
        "            name='expression_head',\n",
        "        )(features)\n",
        "        valence_head = tf.keras.layers.Dense(1, activation='tanh', name='valence_head')(features)\n",
        "        arousal_head = tf.keras.layers.Dense(1, activation='tanh', name='arousal_head')(features)\n",
        "\n",
        "        model = tf.keras.Model(inputs=inputs, outputs=[expression_head, valence_head, arousal_head], name=f\"{config.name}_model\")\n",
        "\n",
        "        losses = {\n",
        "            'expression_head': tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "            'valence_head': tf.keras.losses.MeanSquaredError(),\n",
        "            'arousal_head': tf.keras.losses.MeanSquaredError(),\n",
        "        }\n",
        "        metrics = {\n",
        "            'expression_head': [\n",
        "                tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy'),\n",
        "                tf.keras.metrics.SparseTopKCategoricalAccuracy(k=3, name='top3'),\n",
        "            ],\n",
        "            'valence_head': [\n",
        "                tf.keras.metrics.MeanAbsoluteError(name='mae'),\n",
        "                tf.keras.metrics.RootMeanSquaredError(name='rmse'),\n",
        "            ],\n",
        "            'arousal_head': [\n",
        "                tf.keras.metrics.MeanAbsoluteError(name='mae'),\n",
        "                tf.keras.metrics.RootMeanSquaredError(name='rmse'),\n",
        "            ],\n",
        "        }\n",
        "\n",
        "        model.compile(\n",
        "            optimizer=tf.keras.optimizers.Adam(learning_rate=config.learning_rate),\n",
        "            loss=losses,\n",
        "            loss_weights=config.loss_weights,\n",
        "            metrics=metrics,\n",
        "        )\n",
        "        return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e400ea47",
      "metadata": {},
      "source": [
        "## Experiment Runner\n",
        "\n",
        "The runner encapsulates training (with optional fine-tuning), logging, and artifact management for each baseline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "404c8ab6",
      "metadata": {},
      "outputs": [],
      "source": [
        "class ExperimentRunner:\n",
        "    def __init__(\n",
        "        self,\n",
        "        config: BaselineConfig,\n",
        "        data_module: AffectDataModule,\n",
        "        class_names: List[str],\n",
        "        output_dir: Path = Path('experiments'),\n",
        "    ):\n",
        "        self.config = config\n",
        "        self.data_module = data_module\n",
        "        self.class_names = class_names\n",
        "        self.output_dir = output_dir / config.name\n",
        "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
        "        self.history: Dict[str, List[float]] = {}\n",
        "        self.model: Optional[tf.keras.Model] = None\n",
        "\n",
        "    def _get_callbacks(self) -> List[tf.keras.callbacks.Callback]:\n",
        "        checkpoint_path = self.output_dir / 'checkpoint.keras'\n",
        "        callbacks = [\n",
        "            tf.keras.callbacks.ModelCheckpoint(\n",
        "                filepath=checkpoint_path,\n",
        "                monitor='val_expression_head_accuracy',\n",
        "                mode='max',\n",
        "                save_best_only=True,\n",
        "                save_weights_only=False,\n",
        "            ),\n",
        "            tf.keras.callbacks.ReduceLROnPlateau(\n",
        "                monitor='val_expression_head_accuracy',\n",
        "                mode='max',\n",
        "                factor=0.5,\n",
        "                patience=3,\n",
        "                verbose=1,\n",
        "            ),\n",
        "            tf.keras.callbacks.EarlyStopping(\n",
        "                monitor='val_expression_head_accuracy',\n",
        "                mode='max',\n",
        "                patience=5,\n",
        "                restore_best_weights=True,\n",
        "                verbose=1,\n",
        "            ),\n",
        "        ]\n",
        "        return callbacks\n",
        "\n",
        "    def _train_phase(self, epochs: int, fine_tune: bool = False) -> tf.keras.callbacks.History:\n",
        "        train_ds = self.data_module.get_dataset('train', with_labels=True, training=True)\n",
        "        val_ds = self.data_module.get_dataset('val', with_labels=True, training=False)\n",
        "        callbacks = self._get_callbacks()\n",
        "        history = self.model.fit(\n",
        "            train_ds,\n",
        "            validation_data=val_ds,\n",
        "            epochs=epochs,\n",
        "            callbacks=callbacks,\n",
        "            verbose=1,\n",
        "        )\n",
        "        for key, values in history.history.items():\n",
        "            self.history.setdefault(key, []).extend(values)\n",
        "        phase = 'fine_tune' if fine_tune else 'initial'\n",
        "        with open(self.output_dir / f'history_{phase}.json', 'w') as fp:\n",
        "            json.dump(history.history, fp, indent=2)\n",
        "        return history\n",
        "\n",
        "    def run(self) -> Dict[str, object]:\n",
        "        self.model = ModelFactory.build(self.config)\n",
        "        self._train_phase(self.config.initial_epochs)\n",
        "\n",
        "        if self.config.fine_tune_epochs > 0:\n",
        "            base_layer_name = self.model.layers[1].name\n",
        "            base_model = self.model.get_layer(base_layer_name)\n",
        "            base_model.trainable = True\n",
        "            if self.config.fine_tune_at is not None:\n",
        "                for layer in base_model.layers[: self.config.fine_tune_at]:\n",
        "                    layer.trainable = False\n",
        "            self.model.compile(\n",
        "                optimizer=tf.keras.optimizers.Adam(self.config.fine_tune_learning_rate),\n",
        "                loss=self.model.loss,\n",
        "                loss_weights=self.model.loss_weights,\n",
        "                metrics=self.model.metrics,\n",
        "            )\n",
        "            self._train_phase(self.config.fine_tune_epochs, fine_tune=True)\n",
        "\n",
        "        results_path = self.output_dir / 'training_log.json'\n",
        "        with open(results_path, 'w') as fp:\n",
        "            json.dump(self.history, fp, indent=2)\n",
        "\n",
        "        return {\n",
        "            'config': asdict(self.config),\n",
        "            'history': self.history,\n",
        "            'checkpoint_dir': str(self.output_dir),\n",
        "        }\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7ad1848",
      "metadata": {},
      "source": [
        "## Baseline Experiments\n",
        "\n",
        "Configure at least two CNN backbones (e.g., MobileNetV2 and ResNet50). Feel free to adjust epochs, learning rates, or fine-tuning depth when running on your hardware."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "9706418a",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/bq/lhhwtq996cvgk2b1gt041m080000gn/T/ipykernel_17085/1311879163.py:62: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "  base_model = backbone_fn(include_top=False, input_tensor=x, weights='imagenet')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===== Running baseline: MobileNetV2 (mobilenet_v2) =====\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-09-27 14:34:24.970993: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 354ms/step - arousal_head_loss: 0.5609 - arousal_head_mae: 0.6044 - arousal_head_rmse: 0.7489 - expression_head_accuracy: 0.1338 - expression_head_loss: 2.5269 - expression_head_top3: 0.3686 - loss: 3.1043 - valence_head_loss: 0.5938 - valence_head_mae: 0.6289 - valence_head_rmse: 0.7706 - val_arousal_head_loss: 0.5039 - val_arousal_head_mae: 0.5866 - val_arousal_head_rmse: 0.7091 - val_expression_head_accuracy: 0.1525 - val_expression_head_loss: 2.4738 - val_expression_head_top3: 0.3875 - val_loss: 3.2169 - val_valence_head_loss: 0.9973 - val_valence_head_mae: 0.8572 - val_valence_head_rmse: 0.9983 - learning_rate: 2.0000e-04\n",
            "Epoch 2/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 354ms/step - arousal_head_loss: 0.5609 - arousal_head_mae: 0.6044 - arousal_head_rmse: 0.7489 - expression_head_accuracy: 0.1338 - expression_head_loss: 2.5269 - expression_head_top3: 0.3686 - loss: 3.1043 - valence_head_loss: 0.5938 - valence_head_mae: 0.6289 - valence_head_rmse: 0.7706 - val_arousal_head_loss: 0.5039 - val_arousal_head_mae: 0.5866 - val_arousal_head_rmse: 0.7091 - val_expression_head_accuracy: 0.1525 - val_expression_head_loss: 2.4738 - val_expression_head_top3: 0.3875 - val_loss: 3.2169 - val_valence_head_loss: 0.9973 - val_valence_head_mae: 0.8572 - val_valence_head_rmse: 0.9983 - learning_rate: 2.0000e-04\n",
            "Epoch 2/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 260ms/step - arousal_head_loss: 0.3043 - arousal_head_mae: 0.4411 - arousal_head_rmse: 0.5517 - expression_head_accuracy: 0.1316 - expression_head_loss: 2.3298 - expression_head_top3: 0.3864 - loss: 2.6729 - valence_head_loss: 0.3820 - valence_head_mae: 0.4961 - valence_head_rmse: 0.6181 - val_arousal_head_loss: 0.2729 - val_arousal_head_mae: 0.4041 - val_arousal_head_rmse: 0.5172 - val_expression_head_accuracy: 0.1650 - val_expression_head_loss: 2.6363 - val_expression_head_top3: 0.3850 - val_loss: 3.0049 - val_valence_head_loss: 0.5033 - val_valence_head_mae: 0.5753 - val_valence_head_rmse: 0.7092 - learning_rate: 2.0000e-04\n",
            "Epoch 3/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 260ms/step - arousal_head_loss: 0.3043 - arousal_head_mae: 0.4411 - arousal_head_rmse: 0.5517 - expression_head_accuracy: 0.1316 - expression_head_loss: 2.3298 - expression_head_top3: 0.3864 - loss: 2.6729 - valence_head_loss: 0.3820 - valence_head_mae: 0.4961 - valence_head_rmse: 0.6181 - val_arousal_head_loss: 0.2729 - val_arousal_head_mae: 0.4041 - val_arousal_head_rmse: 0.5172 - val_expression_head_accuracy: 0.1650 - val_expression_head_loss: 2.6363 - val_expression_head_top3: 0.3850 - val_loss: 3.0049 - val_valence_head_loss: 0.5033 - val_valence_head_mae: 0.5753 - val_valence_head_rmse: 0.7092 - learning_rate: 2.0000e-04\n",
            "Epoch 3/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 539ms/step - arousal_head_loss: 0.2293 - arousal_head_mae: 0.3845 - arousal_head_rmse: 0.4788 - expression_head_accuracy: 0.1275 - expression_head_loss: 2.2426 - expression_head_top3: 0.3867 - loss: 2.5173 - valence_head_loss: 0.3203 - valence_head_mae: 0.4555 - valence_head_rmse: 0.5659 - val_arousal_head_loss: 0.2795 - val_arousal_head_mae: 0.4233 - val_arousal_head_rmse: 0.5279 - val_expression_head_accuracy: 0.1475 - val_expression_head_loss: 2.4498 - val_expression_head_top3: 0.3950 - val_loss: 2.8259 - val_valence_head_loss: 0.4677 - val_valence_head_mae: 0.5465 - val_valence_head_rmse: 0.6817 - learning_rate: 2.0000e-04\n",
            "Epoch 4/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 699ms/step - arousal_head_loss: 0.1986 - arousal_head_mae: 0.3641 - arousal_head_rmse: 0.4457 - expression_head_accuracy: 0.1350 - expression_head_loss: 2.2216 - expression_head_top3: 0.3770 - loss: 2.4644 - valence_head_loss: 0.2872 - valence_head_mae: 0.4308 - valence_head_rmse: 0.5359 - val_arousal_head_loss: 0.2622 - val_arousal_head_mae: 0.3945 - val_arousal_head_rmse: 0.5058 - val_expression_head_accuracy: 0.1600 - val_expression_head_loss: 2.6029 - val_expression_head_top3: 0.3750 - val_loss: 2.9601 - val_valence_head_loss: 0.4939 - val_valence_head_mae: 0.5602 - val_valence_head_rmse: 0.7005 - learning_rate: 2.0000e-04\n",
            "Epoch 5/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 208ms/step - arousal_head_loss: 0.1889 - arousal_head_mae: 0.3522 - arousal_head_rmse: 0.4345 - expression_head_accuracy: 0.1314 - expression_head_loss: 2.2026 - expression_head_top3: 0.3844 - loss: 2.4399 - valence_head_loss: 0.2859 - valence_head_mae: 0.4273 - valence_head_rmse: 0.5344\n",
            "Epoch 5: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-05.\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 228ms/step - arousal_head_loss: 0.1921 - arousal_head_mae: 0.3566 - arousal_head_rmse: 0.4383 - expression_head_accuracy: 0.1307 - expression_head_loss: 2.1958 - expression_head_top3: 0.3823 - loss: 2.4326 - valence_head_loss: 0.2813 - valence_head_mae: 0.4234 - valence_head_rmse: 0.5304 - val_arousal_head_loss: 0.2217 - val_arousal_head_mae: 0.3761 - val_arousal_head_rmse: 0.4669 - val_expression_head_accuracy: 0.1525 - val_expression_head_loss: 2.4591 - val_expression_head_top3: 0.3825 - val_loss: 2.7687 - val_valence_head_loss: 0.4178 - val_valence_head_mae: 0.5082 - val_valence_head_rmse: 0.6444 - learning_rate: 2.0000e-04\n",
            "Epoch 6/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 227ms/step - arousal_head_loss: 0.1827 - arousal_head_mae: 0.3527 - arousal_head_rmse: 0.4274 - expression_head_accuracy: 0.1422 - expression_head_loss: 2.1542 - expression_head_top3: 0.4089 - loss: 2.3789 - valence_head_loss: 0.2668 - valence_head_mae: 0.4177 - valence_head_rmse: 0.5165 - val_arousal_head_loss: 0.2206 - val_arousal_head_mae: 0.3703 - val_arousal_head_rmse: 0.4634 - val_expression_head_accuracy: 0.1700 - val_expression_head_loss: 2.2100 - val_expression_head_top3: 0.4400 - val_loss: 2.5182 - val_valence_head_loss: 0.4146 - val_valence_head_mae: 0.5146 - val_valence_head_rmse: 0.6414 - learning_rate: 1.0000e-04\n",
            "Epoch 7/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 259ms/step - arousal_head_loss: 0.1719 - arousal_head_mae: 0.3432 - arousal_head_rmse: 0.4146 - expression_head_accuracy: 0.1410 - expression_head_loss: 2.1440 - expression_head_top3: 0.3998 - loss: 2.3598 - valence_head_loss: 0.2596 - valence_head_mae: 0.4111 - valence_head_rmse: 0.5095 - val_arousal_head_loss: 0.2048 - val_arousal_head_mae: 0.3560 - val_arousal_head_rmse: 0.4461 - val_expression_head_accuracy: 0.1850 - val_expression_head_loss: 2.2158 - val_expression_head_top3: 0.4600 - val_loss: 2.5604 - val_valence_head_loss: 0.5224 - val_valence_head_mae: 0.5793 - val_valence_head_rmse: 0.7193 - learning_rate: 1.0000e-04\n",
            "Epoch 8/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 410ms/step - arousal_head_loss: 0.1717 - arousal_head_mae: 0.3439 - arousal_head_rmse: 0.4144 - expression_head_accuracy: 0.1425 - expression_head_loss: 2.1388 - expression_head_top3: 0.3989 - loss: 2.3527 - valence_head_loss: 0.2561 - valence_head_mae: 0.4105 - valence_head_rmse: 0.5061 - val_arousal_head_loss: 0.1948 - val_arousal_head_mae: 0.3552 - val_arousal_head_rmse: 0.4370 - val_expression_head_accuracy: 0.2075 - val_expression_head_loss: 2.1604 - val_expression_head_top3: 0.4400 - val_loss: 2.4236 - val_valence_head_loss: 0.3528 - val_valence_head_mae: 0.4676 - val_valence_head_rmse: 0.5922 - learning_rate: 1.0000e-04\n",
            "Epoch 9/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 307ms/step - arousal_head_loss: 0.1725 - arousal_head_mae: 0.3449 - arousal_head_rmse: 0.4153 - expression_head_accuracy: 0.1513 - expression_head_loss: 2.1163 - expression_head_top3: 0.4089 - loss: 2.3283 - valence_head_loss: 0.2514 - valence_head_mae: 0.4028 - valence_head_rmse: 0.5014 - val_arousal_head_loss: 0.1961 - val_arousal_head_mae: 0.3566 - val_arousal_head_rmse: 0.4384 - val_expression_head_accuracy: 0.1525 - val_expression_head_loss: 2.1674 - val_expression_head_top3: 0.4425 - val_loss: 2.5357 - val_valence_head_loss: 0.5599 - val_valence_head_mae: 0.5999 - val_valence_head_rmse: 0.7452 - learning_rate: 1.0000e-04\n",
            "Epoch 10/10\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 308ms/step - arousal_head_loss: 0.1735 - arousal_head_mae: 0.3477 - arousal_head_rmse: 0.4165 - expression_head_accuracy: 0.1447 - expression_head_loss: 2.1254 - expression_head_top3: 0.4011 - loss: 2.3365 - valence_head_loss: 0.2486 - valence_head_mae: 0.4029 - valence_head_rmse: 0.4987 - val_arousal_head_loss: 0.1947 - val_arousal_head_mae: 0.3496 - val_arousal_head_rmse: 0.4358 - val_expression_head_accuracy: 0.1700 - val_expression_head_loss: 2.1854 - val_expression_head_top3: 0.4550 - val_loss: 2.4449 - val_valence_head_loss: 0.3404 - val_valence_head_mae: 0.4603 - val_valence_head_rmse: 0.5820 - learning_rate: 1.0000e-04\n",
            "Restoring model weights from the end of the best epoch: 8.\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'Lambda' object has no attribute 'layers'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[12], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m===== Running baseline: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39mbackbone\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) =====\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m runner \u001b[38;5;241m=\u001b[39m ExperimentRunner(config\u001b[38;5;241m=\u001b[39mconfig, data_module\u001b[38;5;241m=\u001b[39mdata_module, class_names\u001b[38;5;241m=\u001b[39mCLASS_NAMES)\n\u001b[0;32m---> 12\u001b[0m summary \u001b[38;5;241m=\u001b[39m \u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m experiment_summaries\u001b[38;5;241m.\u001b[39mappend(summary)\n\u001b[1;32m     14\u001b[0m trained_models[config\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m runner\u001b[38;5;241m.\u001b[39mmodel\n",
            "Cell \u001b[0;32mIn[11], line 71\u001b[0m, in \u001b[0;36mExperimentRunner.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     69\u001b[0m base_model\u001b[38;5;241m.\u001b[39mtrainable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mfine_tune_at \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 71\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m[: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mfine_tune_at]:\n\u001b[1;32m     72\u001b[0m         layer\u001b[38;5;241m.\u001b[39mtrainable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m     74\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mfine_tune_learning_rate),\n\u001b[1;32m     75\u001b[0m     loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mloss,\n\u001b[1;32m     76\u001b[0m     loss_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mloss_weights,\n\u001b[1;32m     77\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmetrics,\n\u001b[1;32m     78\u001b[0m )\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Lambda' object has no attribute 'layers'"
          ]
        }
      ],
      "source": [
        "baseline_setups = [\n",
        "    BaselineConfig(name='MobileNetV2', backbone='mobilenet_v2', initial_epochs=10, fine_tune_epochs=6, learning_rate=2e-4, fine_tune_learning_rate=1e-5),\n",
        "    BaselineConfig(name='ResNet50', backbone='resnet50', initial_epochs=10, fine_tune_epochs=6, learning_rate=1e-4, fine_tune_learning_rate=5e-6),\n",
        "]\n",
        "\n",
        "experiment_summaries = []\n",
        "trained_models: Dict[str, tf.keras.Model] = {}\n",
        "\n",
        "for config in baseline_setups:\n",
        "    print(f\"===== Running baseline: {config.name} ({config.backbone}) =====\")\n",
        "    runner = ExperimentRunner(config=config, data_module=data_module, class_names=CLASS_NAMES)\n",
        "    summary = runner.run()\n",
        "    experiment_summaries.append(summary)\n",
        "    trained_models[config.name] = runner.model\n",
        "    print(f\"Completed {config.name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learning Curves\n",
        "\n",
        "Plot metrics for each baseline to understand convergence behaviours and overfitting patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_learning_curves(history: Dict[str, List[float]], metric_keys: List[str], title: str):\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    for idx, key in enumerate(metric_keys, start=1):\n",
        "        plt.subplot(1, len(metric_keys), idx)\n",
        "        values = history.get(key)\n",
        "        val_key = f\"val_{key}\"\n",
        "        if not values:\n",
        "            continue\n",
        "        plt.plot(values, label=key)\n",
        "        if val_key in history:\n",
        "            plt.plot(history[val_key], label=val_key)\n",
        "        plt.title(key)\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.legend()\n",
        "    plt.suptitle(title)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "for summary in experiment_summaries:\n",
        "    name = summary['config']['name']\n",
        "    plot_learning_curves(summary['history'], ['expression_head_accuracy', 'valence_head_mae', 'arousal_head_mae'], title=f'{name} learning curves')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation Utilities\n",
        "\n",
        "Quantitative evaluation includes emotion classification accuracy/F1 plus regression MAE and RMSE for valence and arousal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def collect_predictions(model: tf.keras.Model, data_module: AffectDataModule, split: str) -> Dict[str, np.ndarray]:\n",
        "    labeled_ds = data_module.get_dataset(split, with_labels=True, training=False)\n",
        "    inference_ds = data_module.get_dataset(split, with_labels=False, training=False)\n",
        "    predictions = model.predict(inference_ds, verbose=1)\n",
        "    if isinstance(predictions, list):\n",
        "        expr_pred, val_pred, aro_pred = predictions\n",
        "    else:\n",
        "        raise ValueError('Unexpected prediction output.')\n",
        "\n",
        "    true_expr = []\n",
        "    true_val = []\n",
        "    true_aro = []\n",
        "    for _, targets in labeled_ds.unbatch():\n",
        "        true_expr.append(int(targets['expression_head'].numpy()))\n",
        "        true_val.append(float(targets['valence_head'].numpy().squeeze()))\n",
        "        true_aro.append(float(targets['arousal_head'].numpy().squeeze()))\n",
        "\n",
        "    return {\n",
        "        'true_expr': np.array(true_expr),\n",
        "        'true_val': np.array(true_val),\n",
        "        'true_aro': np.array(true_aro),\n",
        "        'pred_expr': np.argmax(expr_pred, axis=1),\n",
        "        'pred_expr_proba': expr_pred,\n",
        "        'pred_val': val_pred.squeeze(),\n",
        "        'pred_aro': aro_pred.squeeze(),\n",
        "    }\n",
        "\n",
        "\n",
        "def evaluate_split(model: tf.keras.Model, data_module: AffectDataModule, split: str, class_names: List[str]) -> Dict[str, float]:\n",
        "    outputs = collect_predictions(model, data_module, split)\n",
        "    metrics = {\n",
        "        'accuracy': accuracy_score(outputs['true_expr'], outputs['pred_expr']),\n",
        "        'macro_f1': f1_score(outputs['true_expr'], outputs['pred_expr'], average='macro'),\n",
        "        'valence_mae': mean_absolute_error(outputs['true_val'], outputs['pred_val']),\n",
        "        'valence_rmse': math.sqrt(mean_squared_error(outputs['true_val'], outputs['pred_val'])),\n",
        "        'arousal_mae': mean_absolute_error(outputs['true_aro'], outputs['pred_aro']),\n",
        "        'arousal_rmse': math.sqrt(mean_squared_error(outputs['true_aro'], outputs['pred_aro'])),\n",
        "    }\n",
        "    report = classification_report(outputs['true_expr'], outputs['pred_expr'], target_names=class_names, digits=4)\n",
        "    metrics['classification_report'] = report\n",
        "    cm = confusion_matrix(outputs['true_expr'], outputs['pred_expr'], normalize='true')\n",
        "    metrics['confusion_matrix'] = cm.tolist()\n",
        "    return metrics\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(cm: np.ndarray, class_names: List[str], title: str):\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    sns.heatmap(cm, annot=True, fmt='.2f', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title(title)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "evaluation_records = []\n",
        "\n",
        "for summary in experiment_summaries:\n",
        "    name = summary['config']['name']\n",
        "    model = trained_models[name]\n",
        "    print(f\"Evaluating {name} on validation set...\")\n",
        "    eval_metrics = evaluate_split(model, data_module, split='val', class_names=CLASS_NAMES)\n",
        "    record = {\n",
        "        'model': name,\n",
        "        'accuracy': eval_metrics['accuracy'],\n",
        "        'macro_f1': eval_metrics['macro_f1'],\n",
        "        'valence_mae': eval_metrics['valence_mae'],\n",
        "        'valence_rmse': eval_metrics['valence_rmse'],\n",
        "        'arousal_mae': eval_metrics['arousal_mae'],\n",
        "        'arousal_rmse': eval_metrics['arousal_rmse'],\n",
        "    }\n",
        "    evaluation_records.append(record)\n",
        "    plot_confusion_matrix(np.array(eval_metrics['confusion_matrix']), CLASS_NAMES, title=f'{name} confusion matrix (val)')\n",
        "    print(eval_metrics['classification_report'])\n",
        "\n",
        "comparison_df = pd.DataFrame(evaluation_records)\n",
        "comparison_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Qualitative Analysis\n",
        "\n",
        "Visualise a mix of correctly and incorrectly predicted samples along with the predicted probabilities and regression outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def select_examples(df: pd.DataFrame, outputs: Dict[str, np.ndarray], correct: bool = True, k: int = 8) -> pd.DataFrame:\n",
        "    matches = outputs['pred_expr'] == outputs['true_expr']\n",
        "    idx = np.where(matches if correct else ~matches)[0]\n",
        "    if len(idx) == 0:\n",
        "        return pd.DataFrame()\n",
        "    chosen = np.random.choice(idx, size=min(k, len(idx)), replace=False)\n",
        "    subset = df.iloc[chosen].copy()\n",
        "    subset['true_expr'] = outputs['true_expr'][chosen]\n",
        "    subset['pred_expr'] = outputs['pred_expr'][chosen]\n",
        "    subset['pred_conf'] = outputs['pred_expr_proba'][chosen, subset['pred_expr']]\n",
        "    subset['true_val'] = outputs['true_val'][chosen]\n",
        "    subset['pred_val'] = outputs['pred_val'][chosen]\n",
        "    subset['true_aro'] = outputs['true_aro'][chosen]\n",
        "    subset['pred_aro'] = outputs['pred_aro'][chosen]\n",
        "    return subset\n",
        "\n",
        "\n",
        "def visualize_examples(examples: pd.DataFrame, title: str):\n",
        "    if examples.empty:\n",
        "        print('No samples available for visualization.')\n",
        "        return\n",
        "    num_cols = 4\n",
        "    num_rows = math.ceil(len(examples) / num_cols)\n",
        "    plt.figure(figsize=(4 * num_cols, 4 * num_rows))\n",
        "    for idx, (_, row) in enumerate(examples.iterrows()):\n",
        "        ax = plt.subplot(num_rows, num_cols, idx + 1)\n",
        "        image = tf.keras.utils.load_img(row['image_path'])\n",
        "        plt.imshow(image)\n",
        "        ax.axis('off')\n",
        "        title_lines = [\n",
        "            f\"True: {CLASS_ID_TO_NAME[int(row['true_expr'])]}\",\n",
        "            f\"Pred: {CLASS_ID_TO_NAME[int(row['pred_expr'])]} ({row['pred_conf']:.2f})\",\n",
        "            f\"Valence: {row['true_val']:.2f} → {row['pred_val']:.2f}\",\n",
        "            f\"Arousal: {row['true_aro']:.2f} → {row['pred_aro']:.2f}\",\n",
        "        ]\n",
        "        ax.set_title(''.join(title_lines), fontsize=9)\n",
        "    plt.suptitle(title)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Example usage after evaluation\n",
        "# outputs = collect_predictions(trained_models['MobileNetV2'], data_module, split='val')\n",
        "# correct_samples = select_examples(val_df.reset_index(drop=True), outputs, correct=True, k=8)\n",
        "# visualize_examples(correct_samples, 'Correct predictions - MobileNetV2')\n",
        "# incorrect_samples = select_examples(val_df.reset_index(drop=True), outputs, correct=False, k=8)\n",
        "# visualize_examples(incorrect_samples, 'Incorrect predictions - MobileNetV2')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Landmark Visualisation (Optional)\n",
        "\n",
        "Landmarks can help diagnose mispredictions. We keep a helper that overlays the 68 points on top of an image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_landmarks(image_path: str, landmark_path: str):\n",
        "    points = np.load(landmark_path)\n",
        "    image = tf.keras.utils.load_img(image_path)\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    plt.imshow(image)\n",
        "    if points.ndim == 1:\n",
        "        points = points.reshape(-1, 2)\n",
        "    plt.scatter(points[:, 0], points[:, 1], s=10, c='lime')\n",
        "    plt.axis('off')\n",
        "    plt.title('Facial landmarks')\n",
        "    plt.show()\n",
        "\n",
        "# Example usage:\n",
        "# sample_row = metadata_df.sample(1).iloc[0]\n",
        "# plot_landmarks(sample_row['image_path'], sample_row['landmarks_path'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "\n",
        "- Experiment with additional backbones (EfficientNet, VGG) and hyperparameters.\n",
        "- Add mixed-precision training or gradient accumulation when using GPUs.\n",
        "- Log experiments to TensorBoard or Weights & Biases for deeper insight.\n",
        "- Deploy the best-performing model for inference on unseen test data."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "DL",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
