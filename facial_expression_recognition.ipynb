{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de17ca0a",
   "metadata": {},
   "source": [
    "# Facial Expression Recognition with Multi-Task Learning\n",
    "\n",
    "This notebook implements a comprehensive facial expression recognition system using multi-task learning to predict both discrete emotions (8 classes) and continuous valence/arousal values. We'll compare multiple state-of-the-art CNN architectures:\n",
    "\n",
    "- **VGG16** (baseline/classic architecture)\n",
    "- **EfficientNet-B0/B2** (efficient scaling)\n",
    "- **ConvNeXt-Tiny** (modern CNN with transformer-like design)\n",
    "\n",
    "## Dataset Overview\n",
    "\n",
    "The dataset contains facial images with annotations for:\n",
    "- **Expression labels**: 8 emotion classes (0-7)\n",
    "- **Valence**: Continuous values from -1 to 1 (negative to positive emotions)\n",
    "- **Arousal**: Continuous values from -1 to 1 (calm to excited)\n",
    "- **Landmarks**: 68 facial landmark points (optional for this assignment)\n",
    "\n",
    "## Evaluation Metrics\n",
    "\n",
    "### Classification Metrics:\n",
    "- Accuracy, F1-macro, Cohen's κ, ROC-AUC, PR-AUC, Krippendorff's α\n",
    "\n",
    "### Regression Metrics:\n",
    "- RMSE, Pearson correlation, SAGR, Concordance Correlation Coefficient (CCC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba528808",
   "metadata": {},
   "source": [
    "## 0. Environment & Paths\n",
    "\n",
    "Setting up the environment with required packages, paths, and configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2455d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T17:57:40.419602Z",
     "iopub.status.busy": "2025-09-27T17:57:40.419114Z",
     "iopub.status.idle": "2025-09-27T17:59:12.893473Z",
     "shell.execute_reply": "2025-09-27T17:59:12.892800Z",
     "shell.execute_reply.started": "2025-09-27T17:57:40.419569Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install timm torchmetrics scikit-learn matplotlib seaborn tqdm pillow\n",
    "!pip install krippendorff  # For Krippendorff's alpha\n",
    "\n",
    "# Core imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "import timm\n",
    "\n",
    "# Sklearn for metrics and preprocessing\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import (accuracy_score, f1_score, cohen_kappa_score, \n",
    "                           roc_auc_score, average_precision_score, \n",
    "                           confusion_matrix, classification_report,\n",
    "                           mean_squared_error, r2_score)\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# Other utilities\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from scipy.stats import pearsonr\n",
    "import krippendorff\n",
    "\n",
    "print(\"All packages imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ceba575",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T17:59:12.895372Z",
     "iopub.status.busy": "2025-09-27T17:59:12.894664Z",
     "iopub.status.idle": "2025-09-27T17:59:12.914450Z",
     "shell.execute_reply": "2025-09-27T17:59:12.913895Z",
     "shell.execute_reply.started": "2025-09-27T17:59:12.895348Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "SEED = 4\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False  # Set to True for performance if input sizes are consistent\n",
    "\n",
    "# Set up device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Data paths\n",
    "DATA_DIR = Path('/kaggle/input/dataset/Dataset')\n",
    "IMAGES_DIR = DATA_DIR / 'images'\n",
    "ANNOTATIONS_DIR = DATA_DIR / 'annotations'\n",
    "\n",
    "# Create output directories\n",
    "ARTIFACTS_DIR = Path('./artifacts')\n",
    "FIGURES_DIR = Path('./figures')\n",
    "ARTIFACTS_DIR.mkdir(exist_ok=True)\n",
    "FIGURES_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Images directory: {IMAGES_DIR}\")\n",
    "print(f\"Annotations directory: {ANNOTATIONS_DIR}\")\n",
    "print(f\"Images directory exists: {IMAGES_DIR.exists()}\")\n",
    "print(f\"Annotations directory exists: {ANNOTATIONS_DIR.exists()}\")\n",
    "\n",
    "# Expression labels mapping (assuming 8 basic emotions)\n",
    "EMOTION_LABELS = {\n",
    "    0: 'Neutral',\n",
    "    1: 'Happy', \n",
    "    2: 'Sad',\n",
    "    3: 'Angry',\n",
    "    4: 'Fearful',\n",
    "    5: 'Disgusted',\n",
    "    6: 'Surprised',\n",
    "    7: 'Contemptuous'\n",
    "}\n",
    "\n",
    "print(f\"Emotion labels: {EMOTION_LABELS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e9ec39",
   "metadata": {},
   "source": [
    "## 1. Read Dataset & Build Index\n",
    "\n",
    "Loading all annotation files and building a comprehensive DataFrame with image paths and all target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5a95fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T17:59:12.915570Z",
     "iopub.status.busy": "2025-09-27T17:59:12.915348Z",
     "iopub.status.idle": "2025-09-27T18:00:23.494287Z",
     "shell.execute_reply": "2025-09-27T18:00:23.493533Z",
     "shell.execute_reply.started": "2025-09-27T17:59:12.915555Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_annotations(annotations_dir, images_dir):\n",
    "    \"\"\"\n",
    "    Load all annotation files and build a comprehensive DataFrame\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    \n",
    "    # Get all annotation files\n",
    "    annotation_files = list(annotations_dir.glob('*.npy'))\n",
    "    \n",
    "    # Group files by image ID\n",
    "    image_ids = set()\n",
    "    for f in annotation_files:\n",
    "        # Extract image ID (before the first underscore)\n",
    "        image_id = f.stem.split('_')[0]\n",
    "        image_ids.add(image_id)\n",
    "    \n",
    "    print(f\"Found {len(image_ids)} unique images\")\n",
    "    print(f\"Found {len(annotation_files)} annotation files\")\n",
    "    \n",
    "    for image_id in tqdm(sorted(image_ids, key=int), desc=\"Loading annotations\"):\n",
    "        # Construct file paths\n",
    "        exp_file = annotations_dir / f\"{image_id}_exp.npy\"\n",
    "        val_file = annotations_dir / f\"{image_id}_val.npy\"\n",
    "        aro_file = annotations_dir / f\"{image_id}_aro.npy\"\n",
    "        lnd_file = annotations_dir / f\"{image_id}_lnd.npy\"\n",
    "        \n",
    "        # Construct image path\n",
    "        image_path = images_dir / f\"{image_id}.jpg\"\n",
    "        \n",
    "        # Check if image file exists\n",
    "        if not image_path.exists():\n",
    "            continue\n",
    "            \n",
    "        # Load annotations if they exist\n",
    "        try:\n",
    "            expression = np.load(exp_file) if exp_file.exists() else None\n",
    "            valence = np.load(val_file) if val_file.exists() else None\n",
    "            arousal = np.load(aro_file) if aro_file.exists() else None\n",
    "            landmarks = np.load(lnd_file) if lnd_file.exists() else None\n",
    "            \n",
    "            # Skip if essential annotations are missing\n",
    "            if expression is None or valence is None or arousal is None:\n",
    "                continue\n",
    "                \n",
    "            data_list.append({\n",
    "                'image_id': int(image_id),\n",
    "                'image_path': str(image_path),\n",
    "                'expression': int(expression),\n",
    "                'valence': float(valence),\n",
    "                'arousal': float(arousal),\n",
    "                'landmarks': landmarks.tolist() if landmarks is not None else None\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading annotations for image {image_id}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    df = pd.DataFrame(data_list)\n",
    "    print(f\"Successfully loaded {len(df)} samples\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load the dataset\n",
    "print(\"Loading dataset annotations...\")\n",
    "df = load_annotations(ANNOTATIONS_DIR, IMAGES_DIR)\n",
    "\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "print(f\"\\nBasic statistics:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37291344",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T18:00:23.496077Z",
     "iopub.status.busy": "2025-09-27T18:00:23.495724Z",
     "iopub.status.idle": "2025-09-27T18:00:26.076570Z",
     "shell.execute_reply": "2025-09-27T18:00:26.075726Z",
     "shell.execute_reply.started": "2025-09-27T18:00:23.496057Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Analyze the dataset\n",
    "print(\"Dataset Analysis:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Expression distribution\n",
    "print(\"\\nExpression distribution:\")\n",
    "expr_counts = df['expression'].value_counts().sort_index()\n",
    "for expr, count in expr_counts.items():\n",
    "    emotion_name = EMOTION_LABELS.get(expr, f'Unknown_{expr}')\n",
    "    print(f\"  {expr} ({emotion_name}): {count} samples ({count/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Valence and arousal statistics\n",
    "print(f\"\\nValence range: [{df['valence'].min():.3f}, {df['valence'].max():.3f}]\")\n",
    "print(f\"Arousal range: [{df['arousal'].min():.3f}, {df['arousal'].max():.3f}]\")\n",
    "\n",
    "# Check for uncertain annotations (valence/arousal == -2)\n",
    "uncertain_val = (df['valence'] == -2).sum()\n",
    "uncertain_aro = (df['arousal'] == -2).sum()\n",
    "print(f\"\\nUncertain annotations:\")\n",
    "print(f\"  Valence (-2): {uncertain_val} samples ({uncertain_val/len(df)*100:.1f}%)\")\n",
    "print(f\"  Arousal (-2): {uncertain_aro} samples ({uncertain_aro/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Visualize distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Expression distribution\n",
    "axes[0, 0].bar(range(len(expr_counts)), expr_counts.values)\n",
    "axes[0, 0].set_xlabel('Expression Label')\n",
    "axes[0, 0].set_ylabel('Count')\n",
    "axes[0, 0].set_title('Expression Distribution')\n",
    "axes[0, 0].set_xticks(range(len(expr_counts)))\n",
    "axes[0, 0].set_xticklabels([EMOTION_LABELS.get(i, f'Unk_{i}') for i in expr_counts.index], rotation=45)\n",
    "\n",
    "# Valence distribution\n",
    "axes[0, 1].hist(df['valence'], bins=30, alpha=0.7, edgecolor='black')\n",
    "axes[0, 1].set_xlabel('Valence')\n",
    "axes[0, 1].set_ylabel('Count')\n",
    "axes[0, 1].set_title('Valence Distribution')\n",
    "axes[0, 1].axvline(x=0, color='red', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Arousal distribution\n",
    "axes[1, 0].hist(df['arousal'], bins=30, alpha=0.7, edgecolor='black')\n",
    "axes[1, 0].set_xlabel('Arousal')\n",
    "axes[1, 0].set_ylabel('Count')\n",
    "axes[1, 0].set_title('Arousal Distribution')\n",
    "axes[1, 0].axvline(x=0, color='red', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Valence-Arousal scatter\n",
    "scatter = axes[1, 1].scatter(df['valence'], df['arousal'], c=df['expression'], cmap='tab10', alpha=0.6, s=20)\n",
    "axes[1, 1].set_xlabel('Valence')\n",
    "axes[1, 1].set_ylabel('Arousal')\n",
    "axes[1, 1].set_title('Valence-Arousal Space (colored by expression)')\n",
    "axes[1, 1].axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "axes[1, 1].axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "plt.colorbar(scatter, ax=axes[1, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'dataset_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nDataset loaded successfully! Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d9f423",
   "metadata": {},
   "source": [
    "## 2. Train/Val/Test Split\n",
    "\n",
    "Cleaning the dataset and creating stratified splits while preserving class distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ceb2bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T18:00:26.077554Z",
     "iopub.status.busy": "2025-09-27T18:00:26.077303Z",
     "iopub.status.idle": "2025-09-27T18:00:26.108696Z",
     "shell.execute_reply": "2025-09-27T18:00:26.108133Z",
     "shell.execute_reply.started": "2025-09-27T18:00:26.077535Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Data cleaning and preprocessing\n",
    "print(\"Data Preprocessing:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create a copy for processing\n",
    "df_clean = df.copy()\n",
    "\n",
    "print(f\"Original dataset size: {len(df_clean)}\")\n",
    "\n",
    "# Filter out uncertain annotations for regression targets (valence/arousal == -2)\n",
    "# We'll keep these samples for classification but mark them for regression exclusion\n",
    "df_clean['valid_for_regression'] = ((df_clean['valence'] != -2) & (df_clean['arousal'] != -2))\n",
    "\n",
    "print(f\"Samples valid for regression: {df_clean['valid_for_regression'].sum()}\")\n",
    "print(f\"Samples valid only for classification: {(~df_clean['valid_for_regression']).sum()}\")\n",
    "\n",
    "# For regression, replace uncertain values with NaN (we'll handle this in the dataset class)\n",
    "df_regression = df_clean.copy()\n",
    "df_regression.loc[df_regression['valence'] == -2, 'valence'] = np.nan\n",
    "df_regression.loc[df_regression['arousal'] == -2, 'arousal'] = np.nan\n",
    "\n",
    "# Check class balance after filtering\n",
    "print(\"\\nClass distribution after cleaning:\")\n",
    "expr_counts = df_clean['expression'].value_counts().sort_index()\n",
    "for expr, count in expr_counts.items():\n",
    "    emotion_name = EMOTION_LABELS.get(expr, f'Unknown_{expr}')\n",
    "    print(f\"  {expr} ({emotion_name}): {count} samples ({count/len(df_clean)*100:.1f}%)\")\n",
    "\n",
    "# Check for minimum class size to ensure stratification is possible\n",
    "min_class_count = expr_counts.min()\n",
    "print(f\"\\nMinimum class count: {min_class_count}\")\n",
    "\n",
    "if min_class_count < 2:\n",
    "    print(\"WARNING: Some classes have very few samples. Consider class merging or different split strategy.\")\n",
    "\n",
    "# Stratified train/val/test split\n",
    "# First split: 80% train+val, 20% test\n",
    "X = df_clean[['image_path', 'valence', 'arousal', 'valid_for_regression']].copy()\n",
    "y = df_clean['expression'].copy()\n",
    "\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=SEED, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# Second split: 80% train, 20% val (of the remaining 80%)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_trainval, y_trainval, \n",
    "    test_size=0.25,  # 0.25 * 0.8 = 0.2 of total\n",
    "    random_state=SEED, \n",
    "    stratify=y_trainval\n",
    ")\n",
    "\n",
    "print(f\"\\nSplit sizes:\")\n",
    "print(f\"  Train: {len(X_train)} samples ({len(X_train)/len(df_clean)*100:.1f}%)\")\n",
    "print(f\"  Val: {len(X_val)} samples ({len(X_val)/len(df_clean)*100:.1f}%)\")\n",
    "print(f\"  Test: {len(X_test)} samples ({len(X_test)/len(df_clean)*100:.1f}%)\")\n",
    "\n",
    "# Verify stratification worked\n",
    "print(\"\\nClass distribution in splits:\")\n",
    "for split_name, y_split in [('Train', y_train), ('Val', y_val), ('Test', y_test)]:\n",
    "    counts = y_split.value_counts().sort_index()\n",
    "    print(f\"\\n{split_name}:\")\n",
    "    for expr, count in counts.items():\n",
    "        emotion_name = EMOTION_LABELS.get(expr, f'Unknown_{expr}')\n",
    "        print(f\"  {expr} ({emotion_name}): {count} ({count/len(y_split)*100:.1f}%)\")\n",
    "\n",
    "# Create split DataFrames\n",
    "train_df = pd.concat([X_train, y_train], axis=1)\n",
    "train_df.columns = list(X_train.columns) + ['expression']\n",
    "\n",
    "val_df = pd.concat([X_val, y_val], axis=1)\n",
    "val_df.columns = list(X_val.columns) + ['expression']\n",
    "\n",
    "test_df = pd.concat([X_test, y_test], axis=1)\n",
    "test_df.columns = list(X_test.columns) + ['expression']\n",
    "\n",
    "print(f\"\\nFinal split DataFrames created:\")\n",
    "print(f\"  train_df: {train_df.shape}\")\n",
    "print(f\"  val_df: {val_df.shape}\")\n",
    "print(f\"  test_df: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6743205c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T18:00:26.109584Z",
     "iopub.status.busy": "2025-09-27T18:00:26.109372Z",
     "iopub.status.idle": "2025-09-27T18:00:28.642934Z",
     "shell.execute_reply": "2025-09-27T18:00:28.642230Z",
     "shell.execute_reply.started": "2025-09-27T18:00:26.109568Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save split CSVs for reproducibility\n",
    "print(\"Saving split CSVs...\")\n",
    "\n",
    "# Add split information to original dataframe\n",
    "df_with_splits = df_clean.copy()\n",
    "df_with_splits['split'] = 'unknown'\n",
    "\n",
    "# Set split labels\n",
    "train_indices = train_df.index if hasattr(train_df, 'index') else X_train.index\n",
    "val_indices = val_df.index if hasattr(val_df, 'index') else X_val.index\n",
    "test_indices = test_df.index if hasattr(test_df, 'index') else X_test.index\n",
    "\n",
    "df_with_splits.loc[train_indices, 'split'] = 'train'\n",
    "df_with_splits.loc[val_indices, 'split'] = 'val' \n",
    "df_with_splits.loc[test_indices, 'split'] = 'test'\n",
    "\n",
    "# Save individual split files\n",
    "train_df_full = df_clean.loc[train_indices].copy()\n",
    "val_df_full = df_clean.loc[val_indices].copy()\n",
    "test_df_full = df_clean.loc[test_indices].copy()\n",
    "\n",
    "train_df_full.to_csv(ARTIFACTS_DIR / 'train_split.csv', index=False)\n",
    "val_df_full.to_csv(ARTIFACTS_DIR / 'val_split.csv', index=False)\n",
    "test_df_full.to_csv(ARTIFACTS_DIR / 'test_split.csv', index=False)\n",
    "\n",
    "# Save combined file with split information\n",
    "df_with_splits.to_csv(ARTIFACTS_DIR / 'dataset_with_splits.csv', index=False)\n",
    "\n",
    "print(\"Split CSVs saved to artifacts directory:\")\n",
    "print(f\"  - train_split.csv: {len(train_df_full)} samples\")\n",
    "print(f\"  - val_split.csv: {len(val_df_full)} samples\") \n",
    "print(f\"  - test_split.csv: {len(test_df_full)} samples\")\n",
    "print(f\"  - dataset_with_splits.csv: {len(df_with_splits)} samples\")\n",
    "\n",
    "# Visualize split distributions\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "splits = [('Train', train_df_full), ('Validation', val_df_full), ('Test', test_df_full)]\n",
    "colors = ['skyblue', 'lightgreen', 'lightcoral']\n",
    "\n",
    "for i, (split_name, split_df) in enumerate(splits):\n",
    "    expr_counts = split_df['expression'].value_counts().sort_index()\n",
    "    \n",
    "    bars = axes[i].bar(range(len(expr_counts)), expr_counts.values, color=colors[i], alpha=0.8)\n",
    "    axes[i].set_xlabel('Expression Label')\n",
    "    axes[i].set_ylabel('Count')\n",
    "    axes[i].set_title(f'{split_name} Set Distribution\\n({len(split_df)} samples)')\n",
    "    axes[i].set_xticks(range(len(expr_counts)))\n",
    "    axes[i].set_xticklabels([EMOTION_LABELS.get(j, f'Unk_{j}') for j in expr_counts.index], rotation=45)\n",
    "    \n",
    "    # Add count labels on bars\n",
    "    for bar, count in zip(bars, expr_counts.values):\n",
    "        axes[i].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                    str(count), ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'split_distributions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nData preprocessing and splitting completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0960554d",
   "metadata": {},
   "source": [
    "## 3. Transforms\n",
    "\n",
    "Defining data augmentation strategies for training and standard transforms for validation/testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d69e286",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T18:00:28.644110Z",
     "iopub.status.busy": "2025-09-27T18:00:28.643879Z",
     "iopub.status.idle": "2025-09-27T18:00:32.592507Z",
     "shell.execute_reply": "2025-09-27T18:00:32.591612Z",
     "shell.execute_reply.started": "2025-09-27T18:00:28.644092Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define transforms for different models\n",
    "# ImageNet normalization values\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "def get_transforms(backbone_name='default', input_size=224):\n",
    "    \"\"\"\n",
    "    Get appropriate transforms for different backbones\n",
    "    \"\"\"\n",
    "    \n",
    "    # Common base transforms\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),  # Resize first\n",
    "        transforms.RandomResizedCrop(input_size, scale=(0.8, 1.0), ratio=(0.9, 1.1)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "        transforms.RandomRotation(degrees=10),\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "        # Removed lambda function that was causing pickle errors\n",
    "    ])\n",
    "    \n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize((input_size + 32, input_size + 32)),  # Slightly larger than target\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "    ])\n",
    "    \n",
    "    test_transform = val_transform  # Same as validation\n",
    "    \n",
    "    return train_transform, val_transform, test_transform\n",
    "\n",
    "# Create transforms for different models\n",
    "transforms_dict = {\n",
    "    'vgg16': get_transforms('vgg16', 224),\n",
    "    'efficientnet_b0': get_transforms('efficientnet_b0', 224),\n",
    "    'efficientnet_b2': get_transforms('efficientnet_b2', 260),  # EfficientNet-B2 uses 260x260\n",
    "    'convnext_tiny': get_transforms('convnext_tiny', 224)\n",
    "}\n",
    "\n",
    "print(\"Transforms defined for all models:\")\n",
    "for model_name in transforms_dict.keys():\n",
    "    print(f\"  - {model_name}\")\n",
    "\n",
    "# Visualize some augmented examples\n",
    "def visualize_transforms(df, transforms_tuple, num_samples=8):\n",
    "    \"\"\"\n",
    "    Visualize the effect of data augmentation\n",
    "    \"\"\"\n",
    "    train_transform, val_transform, _ = transforms_tuple\n",
    "    \n",
    "    # Sample some images\n",
    "    sample_df = df.sample(min(num_samples, len(df)), random_state=SEED)\n",
    "    \n",
    "    fig, axes = plt.subplots(3, num_samples, figsize=(2*num_samples, 6))\n",
    "    if num_samples == 1:\n",
    "        axes = axes.reshape(3, 1)\n",
    "    \n",
    "    for i, (_, row) in enumerate(sample_df.iterrows()):\n",
    "        if i >= num_samples:\n",
    "            break\n",
    "            \n",
    "        # Load original image\n",
    "        try:\n",
    "            image = Image.open(row['image_path']).convert('RGB')\n",
    "            \n",
    "            # Original image\n",
    "            axes[0, i].imshow(image)\n",
    "            axes[0, i].set_title(f'Original\\n{EMOTION_LABELS[row[\"expression\"]]}', fontsize=9)\n",
    "            axes[0, i].axis('off')\n",
    "            \n",
    "            # Validation transform (minimal processing)\n",
    "            val_img = val_transform(image)\n",
    "            val_img_display = val_img.permute(1, 2, 0)\n",
    "            # Denormalize for display\n",
    "            val_img_display = val_img_display * torch.tensor(IMAGENET_STD) + torch.tensor(IMAGENET_MEAN)\n",
    "            val_img_display = torch.clamp(val_img_display, 0, 1)\n",
    "            \n",
    "            axes[1, i].imshow(val_img_display)\n",
    "            axes[1, i].set_title('Val Transform', fontsize=9)\n",
    "            axes[1, i].axis('off')\n",
    "            \n",
    "            # Training transform (with augmentation)\n",
    "            train_img = train_transform(image)\n",
    "            train_img_display = train_img.permute(1, 2, 0)\n",
    "            # Denormalize for display\n",
    "            train_img_display = train_img_display * torch.tensor(IMAGENET_STD) + torch.tensor(IMAGENET_MEAN)\n",
    "            train_img_display = torch.clamp(train_img_display, 0, 1)\n",
    "            \n",
    "            axes[2, i].imshow(train_img_display)\n",
    "            axes[2, i].set_title('Train Transform', fontsize=9)\n",
    "            axes[2, i].axis('off')\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {row['image_path']}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIGURES_DIR / 'transform_examples.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Visualize transforms using VGG16 transforms (224x224)\n",
    "print(\"\\nVisualizing data augmentation effects:\")\n",
    "try:\n",
    "    visualize_transforms(train_df_full, transforms_dict['vgg16'], num_samples=6)\n",
    "except Exception as e:\n",
    "    print(f\"Could not visualize transforms: {e}\")\n",
    "    print(\"This might be due to missing image files - will continue anyway.\")\n",
    "\n",
    "print(\"Transforms setup completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c679e1",
   "metadata": {},
   "source": [
    "## 4. PyTorch Dataset & DataLoader\n",
    "\n",
    "Custom dataset class for multi-task learning with facial expression recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1b9b41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T18:00:32.593887Z",
     "iopub.status.busy": "2025-09-27T18:00:32.593397Z",
     "iopub.status.idle": "2025-09-27T18:00:32.641689Z",
     "shell.execute_reply": "2025-09-27T18:00:32.641097Z",
     "shell.execute_reply.started": "2025-09-27T18:00:32.593861Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FERDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for Facial Expression Recognition with Multi-Task Learning\n",
    "    Returns: image, expression_label, valence_arousal_values\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df, transform=None, include_regression=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df: DataFrame with columns ['image_path', 'expression', 'valence', 'arousal', 'valid_for_regression']\n",
    "            transform: torchvision transforms to apply to images\n",
    "            include_regression: whether to include regression targets (valence/arousal)\n",
    "        \"\"\"\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "        self.include_regression = include_regression\n",
    "        \n",
    "        # Convert to numpy for faster access\n",
    "        self.image_paths = df['image_path'].values\n",
    "        self.expressions = df['expression'].values\n",
    "        \n",
    "        if include_regression:\n",
    "            self.valences = df['valence'].values\n",
    "            self.arousals = df['arousal'].values\n",
    "            self.valid_for_regression = df['valid_for_regression'].values\n",
    "        \n",
    "        print(f\"Dataset initialized with {len(self.df)} samples\")\n",
    "        if include_regression:\n",
    "            valid_regression_count = sum(self.valid_for_regression)\n",
    "            print(f\"  - {valid_regression_count} samples valid for regression\")\n",
    "            print(f\"  - {len(self.df) - valid_regression_count} samples for classification only\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        image_path = self.image_paths[idx]\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {image_path}: {e}\")\n",
    "            # Return a black image as fallback\n",
    "            image = Image.new('RGB', (224, 224), color='black')\n",
    "        \n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "            # Default: just convert to tensor\n",
    "            image = transforms.ToTensor()(image)\n",
    "        \n",
    "        # Get expression label\n",
    "        expression = torch.tensor(self.expressions[idx], dtype=torch.long)\n",
    "        \n",
    "        if self.include_regression:\n",
    "            # Get valence and arousal\n",
    "            valence = self.valences[idx]\n",
    "            arousal = self.arousals[idx]\n",
    "            valid_regression = self.valid_for_regression[idx]\n",
    "            \n",
    "            # Handle uncertain annotations (-2) or NaN values\n",
    "            if not valid_regression or np.isnan(valence) or np.isnan(arousal):\n",
    "                # Use neutral values for uncertain cases\n",
    "                va_values = torch.tensor([0.0, 0.0], dtype=torch.float32)\n",
    "                valid_regression = False\n",
    "            else:\n",
    "                va_values = torch.tensor([valence, arousal], dtype=torch.float32)\n",
    "            \n",
    "            return {\n",
    "                'image': image,\n",
    "                'expression': expression,\n",
    "                'va_values': va_values,\n",
    "                'valid_regression': torch.tensor(valid_regression, dtype=torch.bool),\n",
    "                'image_path': image_path\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'image': image,\n",
    "                'expression': expression,\n",
    "                'image_path': image_path\n",
    "            }\n",
    "\n",
    "def create_dataloaders(train_df, val_df, test_df, transforms_tuple, batch_size=32, num_workers=0):\n",
    "    \"\"\"\n",
    "    Create DataLoaders for train, validation, and test sets\n",
    "    Note: num_workers set to 0 to avoid pickle issues with transforms\n",
    "    \"\"\"\n",
    "    train_transform, val_transform, test_transform = transforms_tuple\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = FERDataset(train_df, transform=train_transform, include_regression=True)\n",
    "    val_dataset = FERDataset(val_df, transform=val_transform, include_regression=True)\n",
    "    test_dataset = FERDataset(test_df, transform=test_transform, include_regression=True)\n",
    "    \n",
    "    # Create dataloaders with num_workers=0 to avoid pickle issues\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "        drop_last=True  # For consistent batch sizes during training\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=torch.cuda.is_available()\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=torch.cuda.is_available()\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, (train_dataset, val_dataset, test_dataset)\n",
    "\n",
    "# Test the dataset with a small sample\n",
    "print(\"Testing dataset implementation...\")\n",
    "try:\n",
    "    # Create a small test dataset\n",
    "    test_df_sample = train_df_full.head(5).copy()\n",
    "    test_dataset = FERDataset(test_df_sample, transform=transforms_dict['vgg16'][0], include_regression=True)\n",
    "    \n",
    "    # Test loading a sample\n",
    "    sample = test_dataset[0]\n",
    "    print(f\"Sample loaded successfully:\")\n",
    "    print(f\"  Image shape: {sample['image'].shape}\")\n",
    "    print(f\"  Expression: {sample['expression']} ({EMOTION_LABELS[sample['expression'].item()]})\")\n",
    "    print(f\"  VA values: {sample['va_values']}\")\n",
    "    print(f\"  Valid for regression: {sample['valid_regression']}\")\n",
    "    print(f\"  Image path: {sample['image_path']}\")\n",
    "    \n",
    "    print(\"Dataset implementation test passed!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Dataset test failed: {e}\")\n",
    "    print(\"This might be due to missing image files - will continue anyway.\")\n",
    "\n",
    "print(\"Dataset and DataLoader classes defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f265a3f5",
   "metadata": {},
   "source": [
    "## 5. Metric Implementations\n",
    "\n",
    "Comprehensive evaluation metrics for both classification and regression tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e46738",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T18:00:32.642614Z",
     "iopub.status.busy": "2025-09-27T18:00:32.642410Z",
     "iopub.status.idle": "2025-09-27T18:00:32.696380Z",
     "shell.execute_reply": "2025-09-27T18:00:32.695808Z",
     "shell.execute_reply.started": "2025-09-27T18:00:32.642590Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def concordance_correlation_coefficient(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate Lin's Concordance Correlation Coefficient (CCC)\n",
    "    CCC = 2 * ρ * σ_x * σ_y / (σ_x² + σ_y² + (μ_x - μ_y)²)\n",
    "    \"\"\"\n",
    "    # Convert to numpy if needed\n",
    "    if torch.is_tensor(y_true):\n",
    "        y_true = y_true.cpu().numpy()\n",
    "    if torch.is_tensor(y_pred):\n",
    "        y_pred = y_pred.cpu().numpy()\n",
    "    \n",
    "    # Remove NaN values\n",
    "    mask = ~(np.isnan(y_true) | np.isnan(y_pred))\n",
    "    if not mask.any():\n",
    "        return np.nan\n",
    "        \n",
    "    y_true_clean = y_true[mask]\n",
    "    y_pred_clean = y_pred[mask]\n",
    "    \n",
    "    if len(y_true_clean) < 2:\n",
    "        return np.nan\n",
    "    \n",
    "    # Calculate means\n",
    "    mean_true = np.mean(y_true_clean)\n",
    "    mean_pred = np.mean(y_pred_clean)\n",
    "    \n",
    "    # Calculate variances\n",
    "    var_true = np.var(y_true_clean)\n",
    "    var_pred = np.var(y_pred_clean)\n",
    "    \n",
    "    # Calculate covariance\n",
    "    covariance = np.mean((y_true_clean - mean_true) * (y_pred_clean - mean_pred))\n",
    "    \n",
    "    # Calculate CCC\n",
    "    denominator = var_true + var_pred + (mean_true - mean_pred)**2\n",
    "    if denominator == 0:\n",
    "        return np.nan\n",
    "        \n",
    "    ccc = 2 * covariance / denominator\n",
    "    return ccc\n",
    "\n",
    "def sign_agreement_rate(y_true, y_pred, threshold=1e-6):\n",
    "    \"\"\"\n",
    "    Calculate Sign Agreement Rate (SAGR)\n",
    "    Measures agreement in the sign of predictions, ignoring values close to zero\n",
    "    \"\"\"\n",
    "    if torch.is_tensor(y_true):\n",
    "        y_true = y_true.cpu().numpy()\n",
    "    if torch.is_tensor(y_pred):\n",
    "        y_pred = y_pred.cpu().numpy()\n",
    "    \n",
    "    # Remove NaN values\n",
    "    mask = ~(np.isnan(y_true) | np.isnan(y_pred))\n",
    "    if not mask.any():\n",
    "        return np.nan\n",
    "        \n",
    "    y_true_clean = y_true[mask]\n",
    "    y_pred_clean = y_pred[mask]\n",
    "    \n",
    "    # Remove values close to zero (neutral zone)\n",
    "    non_neutral_mask = (np.abs(y_true_clean) > threshold) & (np.abs(y_pred_clean) > threshold)\n",
    "    \n",
    "    if not non_neutral_mask.any():\n",
    "        return np.nan\n",
    "    \n",
    "    y_true_filtered = y_true_clean[non_neutral_mask]\n",
    "    y_pred_filtered = y_pred_clean[non_neutral_mask]\n",
    "    \n",
    "    # Calculate sign agreement\n",
    "    sign_agreement = np.sign(y_true_filtered) == np.sign(y_pred_filtered)\n",
    "    return np.mean(sign_agreement)\n",
    "\n",
    "def krippendorff_alpha_nominal(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate Krippendorff's alpha for nominal data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if torch.is_tensor(y_true):\n",
    "            y_true = y_true.cpu().numpy()\n",
    "        if torch.is_tensor(y_pred):\n",
    "            y_pred = y_pred.cpu().numpy()\n",
    "        \n",
    "        # Create reliability data matrix (2 coders x n observations)\n",
    "        reliability_data = np.array([y_true, y_pred])\n",
    "        \n",
    "        # Calculate Krippendorff's alpha\n",
    "        alpha = krippendorff.alpha(reliability_data, level_of_measurement='nominal')\n",
    "        return alpha\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating Krippendorff's alpha: {e}\")\n",
    "        return np.nan\n",
    "\n",
    "class MetricsCalculator:\n",
    "    \"\"\"\n",
    "    Comprehensive metrics calculator for multi-task learning\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=8):\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "    def calculate_classification_metrics(self, y_true, y_pred, y_prob=None):\n",
    "        \"\"\"\n",
    "        Calculate all classification metrics\n",
    "        \"\"\"\n",
    "        if torch.is_tensor(y_true):\n",
    "            y_true = y_true.cpu().numpy()\n",
    "        if torch.is_tensor(y_pred):\n",
    "            y_pred = y_pred.cpu().numpy()\n",
    "        if y_prob is not None and torch.is_tensor(y_prob):\n",
    "            y_prob = y_prob.cpu().numpy()\n",
    "        \n",
    "        metrics = {}\n",
    "        \n",
    "        try:\n",
    "            # Basic metrics\n",
    "            metrics['accuracy'] = accuracy_score(y_true, y_pred)\n",
    "            metrics['f1_macro'] = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "            metrics['f1_weighted'] = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "            \n",
    "            # Cohen's Kappa\n",
    "            metrics['cohen_kappa'] = cohen_kappa_score(y_true, y_pred)\n",
    "            \n",
    "            # Krippendorff's alpha (nominal)\n",
    "            metrics['krippendorff_alpha'] = krippendorff_alpha_nominal(y_true, y_pred)\n",
    "            \n",
    "            if y_prob is not None:\n",
    "                try:\n",
    "                    # Multi-class ROC-AUC (One-vs-Rest)\n",
    "                    lb = LabelBinarizer()\n",
    "                    y_true_bin = lb.fit_transform(y_true)\n",
    "                    if y_true_bin.shape[1] == 1:  # Binary case\n",
    "                        y_true_bin = np.hstack([1 - y_true_bin, y_true_bin])\n",
    "                    \n",
    "                    metrics['roc_auc_ovr'] = roc_auc_score(y_true_bin, y_prob, average='macro', multi_class='ovr')\n",
    "                    metrics['roc_auc_weighted'] = roc_auc_score(y_true_bin, y_prob, average='weighted', multi_class='ovr')\n",
    "                    \n",
    "                    # Average Precision (PR-AUC)\n",
    "                    metrics['pr_auc_macro'] = average_precision_score(y_true_bin, y_prob, average='macro')\n",
    "                    metrics['pr_auc_weighted'] = average_precision_score(y_true_bin, y_prob, average='weighted')\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error calculating AUC metrics: {e}\")\n",
    "                    metrics['roc_auc_ovr'] = np.nan\n",
    "                    metrics['roc_auc_weighted'] = np.nan\n",
    "                    metrics['pr_auc_macro'] = np.nan\n",
    "                    metrics['pr_auc_weighted'] = np.nan\n",
    "            \n",
    "            # Per-class metrics\n",
    "            class_report = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n",
    "            for i in range(self.num_classes):\n",
    "                if str(i) in class_report:\n",
    "                    metrics[f'f1_class_{i}'] = class_report[str(i)]['f1-score']\n",
    "                    metrics[f'precision_class_{i}'] = class_report[str(i)]['precision']\n",
    "                    metrics[f'recall_class_{i}'] = class_report[str(i)]['recall']\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating classification metrics: {e}\")\n",
    "            # Return default values\n",
    "            for key in ['accuracy', 'f1_macro', 'f1_weighted', 'cohen_kappa', 'krippendorff_alpha']:\n",
    "                metrics[key] = np.nan\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def calculate_regression_metrics(self, y_true, y_pred, mask=None):\n",
    "        \"\"\"\n",
    "        Calculate all regression metrics\n",
    "        \"\"\"\n",
    "        if torch.is_tensor(y_true):\n",
    "            y_true = y_true.cpu().numpy()\n",
    "        if torch.is_tensor(y_pred):\n",
    "            y_pred = y_pred.cpu().numpy()\n",
    "        if mask is not None and torch.is_tensor(mask):\n",
    "            mask = mask.cpu().numpy()\n",
    "        \n",
    "        # Apply mask if provided (for filtering out uncertain annotations)\n",
    "        if mask is not None:\n",
    "            y_true = y_true[mask]\n",
    "            y_pred = y_pred[mask]\n",
    "        \n",
    "        metrics = {}\n",
    "        \n",
    "        if len(y_true) == 0:\n",
    "            return {k: np.nan for k in ['rmse', 'mae', 'pearson_r', 'pearson_p', 'sagr', 'ccc']}\n",
    "        \n",
    "        try:\n",
    "            # RMSE and MAE\n",
    "            metrics['rmse'] = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "            metrics['mae'] = np.mean(np.abs(y_true - y_pred))\n",
    "            \n",
    "            # Pearson correlation\n",
    "            if len(y_true) > 1 and np.var(y_true) > 0 and np.var(y_pred) > 0:\n",
    "                pearson_r, pearson_p = pearsonr(y_true, y_pred)\n",
    "                metrics['pearson_r'] = pearson_r\n",
    "                metrics['pearson_p'] = pearson_p\n",
    "            else:\n",
    "                metrics['pearson_r'] = np.nan\n",
    "                metrics['pearson_p'] = np.nan\n",
    "            \n",
    "            # Sign Agreement Rate (SAGR)\n",
    "            metrics['sagr'] = sign_agreement_rate(y_true, y_pred)\n",
    "            \n",
    "            # Concordance Correlation Coefficient (CCC)\n",
    "            metrics['ccc'] = concordance_correlation_coefficient(y_true, y_pred)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating regression metrics: {e}\")\n",
    "            metrics = {k: np.nan for k in ['rmse', 'mae', 'pearson_r', 'pearson_p', 'sagr', 'ccc']}\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def calculate_all_metrics(self, predictions, targets, valid_regression_mask=None):\n",
    "        \"\"\"\n",
    "        Calculate all metrics for multi-task predictions\n",
    "        \n",
    "        Args:\n",
    "            predictions: dict with 'classification' and 'regression' keys\n",
    "            targets: dict with 'classification' and 'regression' keys\n",
    "            valid_regression_mask: mask for valid regression samples\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        # Classification metrics\n",
    "        if 'classification' in predictions and 'classification' in targets:\n",
    "            cls_pred = predictions['classification']\n",
    "            cls_target = targets['classification']\n",
    "            cls_prob = predictions.get('classification_prob', None)\n",
    "            \n",
    "            results['classification'] = self.calculate_classification_metrics(\n",
    "                cls_target, cls_pred, cls_prob\n",
    "            )\n",
    "        \n",
    "        # Regression metrics (separate for valence and arousal)\n",
    "        if 'regression' in predictions and 'regression' in targets:\n",
    "            reg_pred = predictions['regression']  # Shape: (N, 2) for [valence, arousal]\n",
    "            reg_target = targets['regression']    # Shape: (N, 2) for [valence, arousal]\n",
    "            \n",
    "            # Valence metrics\n",
    "            results['valence'] = self.calculate_regression_metrics(\n",
    "                reg_target[:, 0], reg_pred[:, 0], valid_regression_mask\n",
    "            )\n",
    "            \n",
    "            # Arousal metrics\n",
    "            results['arousal'] = self.calculate_regression_metrics(\n",
    "                reg_target[:, 1], reg_pred[:, 1], valid_regression_mask\n",
    "            )\n",
    "            \n",
    "            # Combined V-A metrics (mean of valence and arousal)\n",
    "            va_ccc = (results['valence']['ccc'] + results['arousal']['ccc']) / 2\n",
    "            results['combined_ccc'] = va_ccc\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Initialize metrics calculator\n",
    "metrics_calculator = MetricsCalculator(num_classes=len(EMOTION_LABELS))\n",
    "\n",
    "# Test metrics with dummy data\n",
    "print(\"Testing metrics implementation...\")\n",
    "try:\n",
    "    # Create dummy data\n",
    "    n_samples = 100\n",
    "    y_true_cls = np.random.randint(0, 8, n_samples)\n",
    "    y_pred_cls = np.random.randint(0, 8, n_samples) \n",
    "    y_prob_cls = np.random.rand(n_samples, 8)\n",
    "    y_prob_cls = y_prob_cls / y_prob_cls.sum(axis=1, keepdims=True)  # Normalize\n",
    "    \n",
    "    y_true_reg = np.random.uniform(-1, 1, (n_samples, 2))\n",
    "    y_pred_reg = np.random.uniform(-1, 1, (n_samples, 2))\n",
    "    valid_mask = np.random.choice([True, False], n_samples, p=[0.8, 0.2])\n",
    "    \n",
    "    # Test classification metrics\n",
    "    cls_metrics = metrics_calculator.calculate_classification_metrics(\n",
    "        y_true_cls, y_pred_cls, y_prob_cls\n",
    "    )\n",
    "    print(\"Classification metrics calculated successfully:\")\n",
    "    for k, v in list(cls_metrics.items())[:5]:  # Show first 5\n",
    "        print(f\"  {k}: {v:.4f}\")\n",
    "    \n",
    "    # Test regression metrics\n",
    "    val_metrics = metrics_calculator.calculate_regression_metrics(\n",
    "        y_true_reg[:, 0], y_pred_reg[:, 0], valid_mask\n",
    "    )\n",
    "    print(\"\\\\nRegression metrics calculated successfully:\")\n",
    "    for k, v in val_metrics.items():\n",
    "        print(f\"  {k}: {v:.4f}\")\n",
    "    \n",
    "    print(\"\\\\nMetrics implementation test passed!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Metrics test failed: {e}\")\n",
    "\n",
    "print(\"\\\\nMetrics calculator ready for use!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3890b58",
   "metadata": {},
   "source": [
    "## 6. Multi-Task Heads & Model Wrappers\n",
    "\n",
    "Building multi-task learning models with shared backbones and separate heads for classification and regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff589fb7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T18:00:32.699192Z",
     "iopub.status.busy": "2025-09-27T18:00:32.698942Z",
     "iopub.status.idle": "2025-09-27T18:01:07.338293Z",
     "shell.execute_reply": "2025-09-27T18:01:07.337561Z",
     "shell.execute_reply.started": "2025-09-27T18:00:32.699177Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MultiTaskHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-task head with separate branches for classification and regression\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, num_classes=8, dropout_rate=0.3):\n",
    "        super(MultiTaskHead, self).__init__()\n",
    "        \n",
    "        # Shared layers\n",
    "        self.shared_layers = nn.Sequential(\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        \n",
    "        # Classification head (expression recognition)\n",
    "        self.classification_head = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Regression head (valence/arousal prediction)\n",
    "        self.regression_head = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, 2),  # 2 outputs: valence and arousal\n",
    "            nn.Tanh()  # Output between -1 and 1\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Shared feature extraction\n",
    "        shared_features = self.shared_layers(x)\n",
    "        \n",
    "        # Task-specific predictions\n",
    "        classification_logits = self.classification_head(shared_features)\n",
    "        regression_outputs = self.regression_head(shared_features)\n",
    "        \n",
    "        return classification_logits, regression_outputs\n",
    "\n",
    "class MultiTaskModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-task learning model with various backbone architectures\n",
    "    \"\"\"\n",
    "    def __init__(self, backbone_name='vgg16', num_classes=8, pretrained=True, dropout_rate=0.3):\n",
    "        super(MultiTaskModel, self).__init__()\n",
    "        \n",
    "        self.backbone_name = backbone_name\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Initialize backbone\n",
    "        if backbone_name == 'vgg16':\n",
    "            self.backbone = models.vgg16_bn(pretrained=pretrained)\n",
    "            feature_dim = self.backbone.classifier[0].in_features\n",
    "            # Remove the classifier\n",
    "            self.backbone.classifier = nn.Identity()\n",
    "            \n",
    "        elif backbone_name == 'efficientnet_b0':\n",
    "            self.backbone = timm.create_model('efficientnet_b0', pretrained=pretrained)\n",
    "            feature_dim = self.backbone.classifier.in_features\n",
    "            # Remove the classifier\n",
    "            self.backbone.classifier = nn.Identity()\n",
    "            \n",
    "        elif backbone_name == 'efficientnet_b2':\n",
    "            self.backbone = timm.create_model('efficientnet_b2', pretrained=pretrained)\n",
    "            feature_dim = self.backbone.classifier.in_features\n",
    "            # Remove the classifier\n",
    "            self.backbone.classifier = nn.Identity()\n",
    "            \n",
    "        elif backbone_name == 'convnext_tiny':\n",
    "            self.backbone = timm.create_model('convnext_tiny', pretrained=pretrained)\n",
    "            feature_dim = self.backbone.head.in_features\n",
    "            # Remove the head\n",
    "            self.backbone.head = nn.Identity()\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported backbone: {backbone_name}\")\n",
    "        \n",
    "        # Multi-task head\n",
    "        self.head = MultiTaskHead(feature_dim, num_classes, dropout_rate)\n",
    "        \n",
    "        # Store feature dimension for reference\n",
    "        self.feature_dim = feature_dim\n",
    "        \n",
    "        print(f\"Initialized {backbone_name} with feature dimension: {feature_dim}\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Extract features using backbone\n",
    "        features = self.backbone(x)\n",
    "        \n",
    "        # Flatten if necessary (for VGG16)\n",
    "        if len(features.shape) > 2:\n",
    "            features = torch.flatten(features, 1)\n",
    "        \n",
    "        # Multi-task predictions\n",
    "        classification_logits, regression_outputs = self.head(features)\n",
    "        \n",
    "        return classification_logits, regression_outputs\n",
    "    \n",
    "    def freeze_backbone(self):\n",
    "        \"\"\"Freeze backbone parameters\"\"\"\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "        print(f\"Frozen {self.backbone_name} backbone\")\n",
    "    \n",
    "    def unfreeze_backbone(self):\n",
    "        \"\"\"Unfreeze backbone parameters\"\"\"\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = True\n",
    "        print(f\"Unfrozen {self.backbone_name} backbone\")\n",
    "    \n",
    "    def freeze_early_layers(self, freeze_ratio=0.5):\n",
    "        \"\"\"Freeze early layers of the backbone\"\"\"\n",
    "        total_params = sum(1 for _ in self.backbone.parameters())\n",
    "        freeze_count = int(total_params * freeze_ratio)\n",
    "        \n",
    "        for i, param in enumerate(self.backbone.parameters()):\n",
    "            if i < freeze_count:\n",
    "                param.requires_grad = False\n",
    "            else:\n",
    "                param.requires_grad = True\n",
    "        \n",
    "        print(f\"Frozen first {freeze_count}/{total_params} layers of {self.backbone_name}\")\n",
    "    \n",
    "    def get_parameter_count(self):\n",
    "        \"\"\"Get the number of trainable parameters\"\"\"\n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        \n",
    "        return {\n",
    "            'total_parameters': total_params,\n",
    "            'trainable_parameters': trainable_params,\n",
    "            'frozen_parameters': total_params - trainable_params\n",
    "        }\n",
    "\n",
    "def create_model(backbone_name, num_classes=8, pretrained=True, dropout_rate=0.3):\n",
    "    \"\"\"\n",
    "    Factory function to create multi-task models\n",
    "    \"\"\"\n",
    "    model = MultiTaskModel(\n",
    "        backbone_name=backbone_name,\n",
    "        num_classes=num_classes,\n",
    "        pretrained=pretrained,\n",
    "        dropout_rate=dropout_rate\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Test model creation\n",
    "print(\"Testing model creation...\")\n",
    "test_models = {}\n",
    "\n",
    "for backbone_name in ['vgg16', 'efficientnet_b0', 'efficientnet_b2', 'convnext_tiny']:\n",
    "    try:\n",
    "        print(f\"\\\\nCreating {backbone_name} model...\")\n",
    "        model = create_model(backbone_name, num_classes=len(EMOTION_LABELS))\n",
    "        \n",
    "        # Test forward pass\n",
    "        dummy_input = torch.randn(2, 3, 224, 224)\n",
    "        if backbone_name == 'efficientnet_b2':\n",
    "            dummy_input = torch.randn(2, 3, 260, 260)  # EfficientNet-B2 uses 260x260\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            cls_logits, reg_outputs = model(dummy_input)\n",
    "        \n",
    "        print(f\"  Classification output shape: {cls_logits.shape}\")\n",
    "        print(f\"  Regression output shape: {reg_outputs.shape}\")\n",
    "        \n",
    "        # Get parameter count\n",
    "        param_info = model.get_parameter_count()\n",
    "        print(f\"  Total parameters: {param_info['total_parameters']:,}\")\n",
    "        print(f\"  Trainable parameters: {param_info['trainable_parameters']:,}\")\n",
    "        \n",
    "        test_models[backbone_name] = model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error creating {backbone_name}: {e}\")\n",
    "\n",
    "print(f\"\\\\nSuccessfully created {len(test_models)} models!\")\n",
    "print(\"Available models:\", list(test_models.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3f84d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unified Accuracy-First Phased Trainer (replaces earlier trainer classes)\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "@dataclass\n",
    "class PhaseConfig:\n",
    "    name: str\n",
    "    epochs: int\n",
    "    freeze_backbone: bool\n",
    "    backbone_lr: float\n",
    "    head_lr: float\n",
    "    lambda_reg_schedule: Dict[int, float]\n",
    "    weight_decay: float\n",
    "    label_smoothing: float = 0.05\n",
    "    mixup_alpha: Optional[float] = None\n",
    "    cutmix_alpha: Optional[float] = None\n",
    "    grad_clip: float = 1.0\n",
    "    cosine_T_max: Optional[int] = None\n",
    "    cosine_min_lr: float = 1e-6\n",
    "\n",
    "PHASES: List[PhaseConfig] = [\n",
    "    PhaseConfig(\n",
    "        name=\"warmup_cls_only\",\n",
    "        epochs=6,\n",
    "        freeze_backbone=True,\n",
    "        backbone_lr=0.0,\n",
    "        head_lr=1e-3,\n",
    "        lambda_reg_schedule={0: 0.0},\n",
    "        weight_decay=0.01,\n",
    "        label_smoothing=0.05,\n",
    "        mixup_alpha=0.2,\n",
    "    ),\n",
    "    PhaseConfig(\n",
    "        name=\"multitask_finetune\",\n",
    "        epochs=24,\n",
    "        freeze_backbone=False,\n",
    "        backbone_lr=5e-5,\n",
    "        head_lr=2.5e-4,\n",
    "        lambda_reg_schedule={0:0.0, 2:0.01, 4:0.03, 6:0.05},\n",
    "        weight_decay=0.005,\n",
    "        label_smoothing=0.03,\n",
    "        mixup_alpha=0.1,\n",
    "        cutmix_alpha=0.1,\n",
    "        cosine_T_max=24,\n",
    "        cosine_min_lr=1e-6,\n",
    "    ),\n",
    "]\n",
    "\n",
    "def get_lambda_reg(phase: PhaseConfig, local_epoch: int) -> float:\n",
    "    keys = sorted(phase.lambda_reg_schedule.keys())\n",
    "    current = phase.lambda_reg_schedule[keys[0]]\n",
    "    for k in keys:\n",
    "        if local_epoch >= k:\n",
    "            current = phase.lambda_reg_schedule[k]\n",
    "        else:\n",
    "            break\n",
    "    return current\n",
    "\n",
    "class SmoothedCELoss(nn.Module):\n",
    "    def __init__(self, smoothing=0.05):\n",
    "        super().__init__()\n",
    "        self.smoothing = smoothing\n",
    "    def forward(self, logits, target):\n",
    "        if self.smoothing <= 0:\n",
    "            return F.cross_entropy(logits, target)\n",
    "        n_classes = logits.size(-1)\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(log_probs)\n",
    "            true_dist.fill_(self.smoothing / (n_classes - 1))\n",
    "            true_dist.scatter_(1, target.unsqueeze(1), 1 - self.smoothing)\n",
    "        return (-true_dist * log_probs).sum(dim=-1).mean()\n",
    "\n",
    "def apply_mixup_cutmix(images, targets, num_classes, mixup_alpha=None, cutmix_alpha=None):\n",
    "    if mixup_alpha is None and cutmix_alpha is None:\n",
    "        return images, targets, None\n",
    "    lam = 1.0\n",
    "    mixed_targets = None\n",
    "    batch_size = images.size(0)\n",
    "    index = torch.randperm(batch_size).to(images.device)\n",
    "    if cutmix_alpha and torch.rand(1).item() < 0.5:\n",
    "        lam = np.random.beta(cutmix_alpha, cutmix_alpha)\n",
    "        W, H = images.size(3), images.size(2)\n",
    "        cut_rat = np.sqrt(1. - lam)\n",
    "        cut_w = int(W * cut_rat)\n",
    "        cut_h = int(H * cut_rat)\n",
    "        cx = np.random.randint(W)\n",
    "        cy = np.random.randint(H)\n",
    "        x1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "        x2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "        y1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "        y2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "        images[:, :, y1:y2, x1:x2] = images[index, :, y1:y2, x1:x2]\n",
    "        lam = 1 - ((x2 - x1) * (y2 - y1) / (W * H + 1e-6))\n",
    "    elif mixup_alpha:\n",
    "        lam = np.random.beta(mixup_alpha, mixup_alpha)\n",
    "        images = lam * images + (1 - lam) * images[index, :]\n",
    "    mixed_targets = (targets, targets[index], lam)\n",
    "    return images, targets, mixed_targets\n",
    "\n",
    "class AccuracyFirstTrainer:\n",
    "    def __init__(self, model, train_loader, val_loader, device, num_classes=8):\n",
    "        self.model = model.to(device)\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.device = device\n",
    "        self.num_classes = num_classes\n",
    "        self.best_state = None\n",
    "        self.best_acc = 0.0\n",
    "        self.history = []\n",
    "\n",
    "    def _build_optimizer(self, phase: PhaseConfig):\n",
    "        head_params = list(self.model.head.parameters()) if hasattr(self.model, 'head') else []\n",
    "        backbone_params = [p for n,p in self.model.named_parameters() if 'head' not in n]\n",
    "        param_groups = []\n",
    "        if phase.backbone_lr > 0 and not phase.freeze_backbone:\n",
    "            param_groups.append({'params': backbone_params, 'lr': phase.backbone_lr})\n",
    "        else:\n",
    "            for p in backbone_params:\n",
    "                p.requires_grad = not phase.freeze_backbone\n",
    "        if head_params:\n",
    "            param_groups.append({'params': head_params, 'lr': phase.head_lr})\n",
    "        optimizer = torch.optim.AdamW(param_groups, weight_decay=phase.weight_decay, betas=(0.9,0.999))\n",
    "        if phase.cosine_T_max:\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "                optimizer, T_max=phase.cosine_T_max, eta_min=phase.cosine_min_lr\n",
    "            )\n",
    "        else:\n",
    "            scheduler = None\n",
    "        return optimizer, scheduler\n",
    "\n",
    "    def _classification_loss(self, logits, targets, phase: PhaseConfig, mix_targets=None):\n",
    "        criterion = SmoothedCELoss(phase.label_smoothing)\n",
    "        if mix_targets is None:\n",
    "            return criterion(logits, targets)\n",
    "        t1, t2, lam = mix_targets\n",
    "        return lam * criterion(logits, t1) + (1 - lam) * criterion(logits, t2)\n",
    "\n",
    "    def _regression_loss(self, preds, targets, valid_mask):\n",
    "        if valid_mask.sum() == 0:\n",
    "            return preds.new_tensor(0.0)\n",
    "        diff = preds[valid_mask] - targets[valid_mask]\n",
    "        return F.smooth_l1_loss(diff, torch.zeros_like(diff))\n",
    "\n",
    "    def train_phase(self, phase: PhaseConfig, global_epoch_start: int):\n",
    "        print(f\"\\n=== Phase: {phase.name} ({phase.epochs} epochs) ===\")\n",
    "        optimizer, scheduler = self._build_optimizer(phase)\n",
    "        for local_epoch in range(phase.epochs):\n",
    "            lambda_reg = get_lambda_reg(phase, local_epoch)\n",
    "            self.model.train()\n",
    "            train_cls_loss = train_reg_loss = 0.0\n",
    "            correct = total = 0\n",
    "            for batch in self.train_loader:\n",
    "                images = batch['image'].to(self.device)\n",
    "                labels = batch['expression'].to(self.device)\n",
    "                va = batch['va_values'].to(self.device)\n",
    "                valid_reg = batch['valid_regression'].to(self.device)\n",
    "\n",
    "                images, labels, mix_targets = apply_mixup_cutmix(\n",
    "                    images, labels, self.num_classes,\n",
    "                    mixup_alpha=phase.mixup_alpha, cutmix_alpha=phase.cutmix_alpha\n",
    "                )\n",
    "\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                cls_logits, reg_out = self.model(images)\n",
    "                cls_loss = self._classification_loss(cls_logits, labels, phase, mix_targets)\n",
    "                reg_loss = self._regression_loss(reg_out, va, valid_reg) if lambda_reg > 0 else cls_logits.new_tensor(0.0)\n",
    "                loss = cls_loss + lambda_reg * reg_loss\n",
    "                loss.backward()\n",
    "                if phase.grad_clip:\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), phase.grad_clip)\n",
    "                optimizer.step()\n",
    "                train_cls_loss += cls_loss.item()\n",
    "                train_reg_loss += reg_loss.item()\n",
    "                preds = cls_logits.argmax(1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "            if scheduler: scheduler.step()\n",
    "            train_acc = correct / max(1,total)\n",
    "            val_acc, val_f1 = self.validate()\n",
    "            print(f\"Epoch {global_epoch_start + local_epoch+1} | TrainAcc {train_acc:.3f} | ValAcc {val_acc:.3f} | ValF1 {val_f1:.3f} | λ={lambda_reg:.3f}\")\n",
    "            self.history.append({\n",
    "                'phase': phase.name,\n",
    "                'epoch': global_epoch_start + local_epoch + 1,\n",
    "                'train_acc': train_acc,\n",
    "                'val_acc': val_acc,\n",
    "                'val_f1': val_f1,\n",
    "                'lambda_reg': lambda_reg\n",
    "            })\n",
    "            if val_acc > self.best_acc:\n",
    "                self.best_acc = val_acc\n",
    "                self.best_state = {k:v.cpu().clone() for k,v in self.model.state_dict().items()}\n",
    "                print(f\"  ✅ New best accuracy: {val_acc:.3f}\")\n",
    "        return global_epoch_start + phase.epochs\n",
    "\n",
    "    def validate(self):\n",
    "        self.model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        with torch.no_grad():\n",
    "            for batch in self.val_loader:\n",
    "                images = batch['image'].to(self.device)\n",
    "                labels = batch['expression'].to(self.device)\n",
    "                cls_logits, _ = self.model(images)\n",
    "                preds = cls_logits.argmax(1)\n",
    "                all_preds.append(preds.cpu())\n",
    "                all_labels.append(labels.cpu())\n",
    "        all_preds = torch.cat(all_preds)\n",
    "        all_labels = torch.cat(all_labels)\n",
    "        acc = (all_preds == all_labels).float().mean().item()\n",
    "        try:\n",
    "            f1 = f1_score(all_labels.numpy(), all_preds.numpy(), average='weighted')\n",
    "        except Exception:\n",
    "            f1 = 0.0\n",
    "        return acc, f1\n",
    "\n",
    "    def run(self):\n",
    "        global_epoch = 0\n",
    "        for phase in PHASES:\n",
    "            global_epoch = self.train_phase(phase, global_epoch)\n",
    "        if self.best_state:\n",
    "            self.model.load_state_dict(self.best_state)\n",
    "        print(f\"\\nBest validation accuracy achieved: {self.best_acc:.3f}\")\n",
    "        return self.history\n",
    "\n",
    "print(\"Unified AccuracyFirstTrainer ready. Usage example:\\n\"\n",
    "      \"model = create_model('vgg16', num_classes=len(EMOTION_LABELS), pretrained=True)\\n\"\n",
    "      \"train_loader, val_loader, _, _ = create_dataloaders(train_df_full, val_df_full, test_df_full, transforms_dict['vgg16'], batch_size=16)\\n\"\n",
    "      \"trainer = AccuracyFirstTrainer(model, train_loader, val_loader, device)\\n\"\n",
    "      \"trainer.run()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c53b6e",
   "metadata": {},
   "source": [
    "## 7. Training Strategy (Accuracy-First Phased Trainer)\n",
    "\n",
    "To simplify and clean the workflow we adopt a single unified trainer that:\n",
    "\n",
    "1. Runs an explicit classification-only warmup (freezes backbone, λ_reg = 0) to rapidly specialize the new multi-task head.\n",
    "2. Progressively unfreezes and fine-tunes the backbone while ramping the regression (valence/arousal) loss weight.\n",
    "3. Supports light Mixup / CutMix (configurable per phase) and label smoothing.\n",
    "4. Tracks best validation accuracy and restores those weights at the end.\n",
    "\n",
    "This replaces previous ad‑hoc trainer variants (ImprovedTrainer, legacy joint trainer) to keep the notebook coherent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6f908a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Task Loss Function\n",
    "class MultiTaskLoss(nn.Module):\n",
    "    \"\"\"Combined loss for classification and regression tasks\"\"\"\n",
    "    \n",
    "    def __init__(self, lambda_reg=1.0, class_weights=None):\n",
    "        super().__init__()\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.class_weights = class_weights\n",
    "        \n",
    "        # Classification loss\n",
    "        if class_weights is not None:\n",
    "            self.cls_criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        else:\n",
    "            self.cls_criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Regression loss (smooth L1 loss is more robust than MSE)\n",
    "        self.reg_criterion = nn.SmoothL1Loss()\n",
    "    \n",
    "    def forward(self, cls_logits, reg_outputs, cls_targets, reg_targets, valid_regression):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            cls_logits: Classification predictions [batch_size, num_classes]\n",
    "            reg_outputs: Regression predictions [batch_size, 2] (valence, arousal)\n",
    "            cls_targets: Classification ground truth [batch_size]\n",
    "            reg_targets: Regression ground truth [batch_size, 2] (valence, arousal)\n",
    "            valid_regression: Mask for valid regression samples [batch_size]\n",
    "        \"\"\"\n",
    "        # Classification loss (all samples)\n",
    "        cls_loss = self.cls_criterion(cls_logits, cls_targets)\n",
    "        \n",
    "        # Regression loss (only valid samples)\n",
    "        if valid_regression.sum() > 0:\n",
    "            valid_reg_pred = reg_outputs[valid_regression]\n",
    "            valid_reg_targets = reg_targets[valid_regression]\n",
    "            reg_loss = self.reg_criterion(valid_reg_pred, valid_reg_targets)\n",
    "        else:\n",
    "            reg_loss = torch.tensor(0.0, device=cls_logits.device)\n",
    "        \n",
    "        # Combined loss\n",
    "        total_loss = cls_loss + self.lambda_reg * reg_loss\n",
    "        \n",
    "        return total_loss, cls_loss, reg_loss\n",
    "\n",
    "# Fixed Multi-Task Model Factory\n",
    "def create_model(model_name, num_classes=8, pretrained=True, dropout_rate=0.3):\n",
    "    \"\"\"\n",
    "    Create multi-task model with proper feature dimension handling\n",
    "    \"\"\"\n",
    "    model_name = model_name.lower()\n",
    "    \n",
    "    if model_name == 'vgg16':\n",
    "        backbone = models.vgg16_bn(weights='IMAGENET1K_V1' if pretrained else None)\n",
    "        # VGG16 classifier: features -> avgpool -> classifier\n",
    "        # Feature dim from classifier[0] (first linear layer)\n",
    "        feature_dim = backbone.classifier[0].in_features  # Should be 25088\n",
    "        backbone.classifier = nn.Identity()  # Remove classifier\n",
    "        \n",
    "    elif model_name == 'efficientnet_b0':\n",
    "        backbone = timm.create_model('efficientnet_b0', pretrained=pretrained)\n",
    "        feature_dim = backbone.classifier.in_features\n",
    "        backbone.classifier = nn.Identity()\n",
    "        \n",
    "    elif model_name == 'efficientnet_b2':\n",
    "        backbone = timm.create_model('efficientnet_b2', pretrained=pretrained)  \n",
    "        feature_dim = backbone.classifier.in_features\n",
    "        backbone.classifier = nn.Identity()\n",
    "        \n",
    "    elif model_name == 'convnext_tiny':\n",
    "        backbone = timm.create_model('convnext_tiny', pretrained=pretrained)\n",
    "        feature_dim = backbone.head.fc.in_features\n",
    "        backbone.head = nn.Identity()\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model: {model_name}\")\n",
    "    \n",
    "    # Create multi-task head\n",
    "    head = MultiTaskHead(feature_dim, num_classes, dropout_rate)\n",
    "    \n",
    "    # Wrap in MultiTaskModel\n",
    "    model = MultiTaskModel(backbone, head)\n",
    "    \n",
    "    print(f\"Created {model_name} with feature_dim={feature_dim}\")\n",
    "    return model\n",
    "\n",
    "class Trainer:\n",
    "    \"\"\"Standard trainer for multi-task learning\"\"\"\n",
    "    \n",
    "    def __init__(self, model, train_loader, val_loader, device, config=None):\n",
    "        self.model = model.to(device)\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.device = device\n",
    "        \n",
    "        # Default config\n",
    "        default_config = {\n",
    "            'epochs': 30,\n",
    "            'lr': 3e-4,\n",
    "            'weight_decay': 1e-4,\n",
    "            'lambda_reg': 1.0,\n",
    "            'patience': 15,\n",
    "            'warmup_epochs': 2,\n",
    "        }\n",
    "        self.config = {**default_config, **(config or {})}\n",
    "        \n",
    "        # Initialize criterion and optimizer\n",
    "        self.criterion = MultiTaskLoss(lambda_reg=self.config['lambda_reg'])\n",
    "        self.optimizer = optim.AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=self.config['lr'],\n",
    "            weight_decay=self.config['weight_decay']\n",
    "        )\n",
    "        \n",
    "        # Learning rate scheduler\n",
    "        self.scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "            self.optimizer,\n",
    "            max_lr=self.config['lr'],\n",
    "            epochs=self.config['epochs'],\n",
    "            steps_per_epoch=len(self.train_loader),\n",
    "            pct_start=0.3,\n",
    "            anneal_strategy='cos'\n",
    "        )\n",
    "        \n",
    "        # Early stopping\n",
    "        self.best_ccc = -float('inf')\n",
    "        self.patience_counter = 0\n",
    "        self.best_model_state = None\n",
    "        \n",
    "        # Training history\n",
    "        self.history = {\n",
    "            'train_loss': [], 'val_loss': [],\n",
    "            'val_accuracy': [], 'val_f1': [], 'val_ccc': []\n",
    "        }\n",
    "    \n",
    "    def train_epoch(self):\n",
    "        \"\"\"Train for one epoch\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch in tqdm(self.train_loader, desc='Training'):\n",
    "            images = batch['image'].to(self.device)\n",
    "            expressions = batch['expression'].to(self.device)\n",
    "            va_values = batch['va_values'].to(self.device)\n",
    "            valid_regression = batch['valid_regression'].to(self.device)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            cls_logits, reg_outputs = self.model(images)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss, cls_loss, reg_loss = self.criterion(\n",
    "                cls_logits, reg_outputs, expressions, va_values, valid_regression\n",
    "            )\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "        \n",
    "        return total_loss / num_batches\n",
    "    \n",
    "    def validate_epoch(self):\n",
    "        \"\"\"Validate for one epoch\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        all_va_preds = []\n",
    "        all_va_true = []\n",
    "        all_valid_regression = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(self.val_loader, desc='Validation'):\n",
    "                images = batch['image'].to(self.device)\n",
    "                expressions = batch['expression'].to(self.device)\n",
    "                va_values = batch['va_values'].to(self.device)\n",
    "                valid_regression = batch['valid_regression'].to(self.device)\n",
    "                \n",
    "                cls_logits, reg_outputs = self.model(images)\n",
    "                loss, _, _ = self.criterion(\n",
    "                    cls_logits, reg_outputs, expressions, va_values, valid_regression\n",
    "                )\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                # Collect predictions\n",
    "                preds = torch.argmax(cls_logits, dim=1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(expressions.cpu().numpy())\n",
    "                all_va_preds.extend(reg_outputs.cpu().numpy())\n",
    "                all_va_true.extend(va_values.cpu().numpy())\n",
    "                all_valid_regression.extend(valid_regression.cpu().numpy())\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "        \n",
    "        # Calculate CCC for regression (only valid samples)\n",
    "        all_va_preds = np.array(all_va_preds)\n",
    "        all_va_true = np.array(all_va_true)\n",
    "        all_valid_regression = np.array(all_valid_regression)\n",
    "        \n",
    "        if np.any(all_valid_regression):\n",
    "            valid_mask = all_valid_regression\n",
    "            val_preds = all_va_preds[valid_mask, 0]  # valence\n",
    "            val_true = all_va_true[valid_mask, 0]\n",
    "            aro_preds = all_va_preds[valid_mask, 1]  # arousal\n",
    "            aro_true = all_va_true[valid_mask, 1]\n",
    "            \n",
    "            val_ccc = concordance_correlation_coefficient(val_true, val_preds)\n",
    "            aro_ccc = concordance_correlation_coefficient(aro_true, aro_preds)\n",
    "            combined_ccc = (val_ccc + aro_ccc) / 2\n",
    "        else:\n",
    "            val_ccc = aro_ccc = combined_ccc = 0.0\n",
    "        \n",
    "        return {\n",
    "            'val_loss': total_loss / len(self.val_loader),\n",
    "            'accuracy': accuracy,\n",
    "            'f1': f1,\n",
    "            'val_ccc': val_ccc,\n",
    "            'aro_ccc': aro_ccc,\n",
    "            'combined_ccc': combined_ccc\n",
    "        }\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Full training loop\"\"\"\n",
    "        print(f\"Starting training for {self.config['epochs']} epochs...\")\n",
    "        \n",
    "        # Warmup: freeze backbone for first few epochs\n",
    "        if self.config['warmup_epochs'] > 0:\n",
    "            self.model.freeze_backbone()\n",
    "            print(f\"Warmup: freezing backbone for {self.config['warmup_epochs']} epochs\")\n",
    "        \n",
    "        for epoch in range(1, self.config['epochs'] + 1):\n",
    "            print(f\"\\nEpoch {epoch}/{self.config['epochs']}\")\n",
    "            \n",
    "            # Unfreeze after warmup\n",
    "            if epoch == self.config['warmup_epochs'] + 1:\n",
    "                self.model.unfreeze_backbone()\n",
    "                print(\"Unfrozing backbone - training all parameters\")\n",
    "            \n",
    "            # Train and validate\n",
    "            train_loss = self.train_epoch()\n",
    "            val_metrics = self.validate_epoch()\n",
    "            \n",
    "            # Print metrics\n",
    "            print(f\"Train Loss: {train_loss:.4f}\")\n",
    "            print(f\"Val Loss: {val_metrics['val_loss']:.4f}\")\n",
    "            print(f\"Val Accuracy: {val_metrics['accuracy']:.4f}\")\n",
    "            print(f\"Val F1: {val_metrics['f1']:.4f}\")\n",
    "            print(f\"Val CCC - Valence: {val_metrics['val_ccc']:.4f}\")\n",
    "            print(f\"Val CCC - Arousal: {val_metrics['aro_ccc']:.4f}\")\n",
    "            print(f\"Combined CCC: {val_metrics['combined_ccc']:.4f}\")\n",
    "            \n",
    "            # Save history\n",
    "            self.history['train_loss'].append(train_loss)\n",
    "            self.history['val_loss'].append(val_metrics['val_loss'])\n",
    "            self.history['val_accuracy'].append(val_metrics['accuracy'])\n",
    "            self.history['val_f1'].append(val_metrics['f1'])\n",
    "            self.history['val_ccc'].append(val_metrics['combined_ccc'])\n",
    "            \n",
    "            # Early stopping and model saving\n",
    "            current_ccc = val_metrics['combined_ccc']\n",
    "            if current_ccc > self.best_ccc:\n",
    "                self.best_ccc = current_ccc\n",
    "                self.best_model_state = self.model.state_dict().copy()\n",
    "                self.patience_counter = 0\n",
    "                print(f\"💾 Best model saved (CCC: {current_ccc:.4f})\")\n",
    "            else:\n",
    "                self.patience_counter += 1\n",
    "                if self.patience_counter >= self.config['patience']:\n",
    "                    print(f\"Early stopping triggered after {epoch} epochs\")\n",
    "                    break\n",
    "        \n",
    "        # Load best model\n",
    "        if self.best_model_state is not None:\n",
    "            self.model.load_state_dict(self.best_model_state)\n",
    "            print(f\"Loaded best model with CCC: {self.best_ccc:.4f}\")\n",
    "        \n",
    "        return self.history\n",
    "\n",
    "print(\"✅ Fixed MultiTaskLoss, create_model factory, and standard Trainer classes ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7083eae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidated PhaseConfig + AccuracyFirstTrainer\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Tuple\n",
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "@dataclass\n",
    "class PhaseConfig:\n",
    "    name: str\n",
    "    epochs: int\n",
    "    freeze_backbone: bool\n",
    "    lr_head: float\n",
    "    lr_backbone: float\n",
    "    lambda_reg_max: float\n",
    "    lambda_reg_start: float = 0.0\n",
    "    mixup_alpha: float = 0.0\n",
    "    cutmix_alpha: float = 0.0\n",
    "    label_smoothing: float = 0.0\n",
    "    cosine: bool = True\n",
    "\n",
    "PHASES: List[PhaseConfig] = [\n",
    "    PhaseConfig(\n",
    "        name=\"warmup\",\n",
    "        epochs=2,\n",
    "        freeze_backbone=True,\n",
    "        lr_head=5e-4,\n",
    "        lr_backbone=1e-5,\n",
    "        lambda_reg_max=0.0,\n",
    "        lambda_reg_start=0.0,\n",
    "        label_smoothing=0.1,\n",
    "    ),\n",
    "    PhaseConfig(\n",
    "        name=\"head_focus\",\n",
    "        epochs=4,\n",
    "        freeze_backbone=False,\n",
    "        lr_head=4e-4,\n",
    "        lr_backbone=2e-5,\n",
    "        lambda_reg_max=0.05,\n",
    "        lambda_reg_start=0.0,\n",
    "        label_smoothing=0.1,\n",
    "        mixup_alpha=0.2,\n",
    "        cutmix_alpha=0.0,\n",
    "    ),\n",
    "    PhaseConfig(\n",
    "        name=\"joint_finetune\",\n",
    "        epochs=6,\n",
    "        freeze_backbone=False,\n",
    "        lr_head=3e-4,\n",
    "        lr_backbone=1e-5,\n",
    "        lambda_reg_max=0.1,\n",
    "        lambda_reg_start=0.05,\n",
    "        label_smoothing=0.05,\n",
    "        mixup_alpha=0.0,\n",
    "        cutmix_alpha=0.0,\n",
    "    ),\n",
    "]\n",
    "\n",
    "class AccuracyFirstTrainer:\n",
    "    \"\"\"Phased trainer focusing on classification accuracy first\"\"\"\n",
    "    \n",
    "    def __init__(self, model, train_loader, val_loader, device):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.device = device\n",
    "        self.history = []\n",
    "        self.best_acc = 0.0\n",
    "        \n",
    "    def train(self):\n",
    "        \"\"\"Run all training phases\"\"\"\n",
    "        global_epoch = 0\n",
    "        \n",
    "        for phase in PHASES:\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"Phase: {phase.name} ({phase.epochs} epochs)\")\n",
    "            print(f\"{'='*50}\")\n",
    "            \n",
    "            # Setup optimizer for this phase\n",
    "            if phase.freeze_backbone:\n",
    "                # Freeze backbone, only train head\n",
    "                for param in self.model.backbone.parameters():\n",
    "                    param.requires_grad = False\n",
    "                for param in self.model.head.parameters():\n",
    "                    param.requires_grad = True\n",
    "                    \n",
    "                optimizer = torch.optim.AdamW(\n",
    "                    self.model.head.parameters(),\n",
    "                    lr=phase.lr_head,\n",
    "                    weight_decay=0.01\n",
    "                )\n",
    "            else:\n",
    "                # Train both backbone and head with different LRs\n",
    "                for param in self.model.parameters():\n",
    "                    param.requires_grad = True\n",
    "                    \n",
    "                optimizer = torch.optim.AdamW([\n",
    "                    {'params': self.model.backbone.parameters(), 'lr': phase.lr_backbone},\n",
    "                    {'params': self.model.head.parameters(), 'lr': phase.lr_head}\n",
    "                ], weight_decay=0.01)\n",
    "            \n",
    "            # Setup scheduler\n",
    "            if phase.cosine:\n",
    "                scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "                    optimizer, T_max=phase.epochs, eta_min=1e-7\n",
    "                )\n",
    "            else:\n",
    "                scheduler = None\n",
    "            \n",
    "            # Train for this phase\n",
    "            for epoch in range(phase.epochs):\n",
    "                global_epoch += 1\n",
    "                \n",
    "                # Calculate lambda_reg for this epoch\n",
    "                if phase.epochs > 1:\n",
    "                    progress = epoch / (phase.epochs - 1)\n",
    "                else:\n",
    "                    progress = 1.0\n",
    "                lambda_reg = phase.lambda_reg_start + progress * (phase.lambda_reg_max - phase.lambda_reg_start)\n",
    "                \n",
    "                # Train one epoch\n",
    "                train_loss, train_acc = self._train_epoch(optimizer, phase, lambda_reg)\n",
    "                \n",
    "                # Validate\n",
    "                val_loss, val_acc, val_f1 = self._validate_epoch(lambda_reg)\n",
    "                \n",
    "                # Update scheduler\n",
    "                if scheduler:\n",
    "                    scheduler.step()\n",
    "                \n",
    "                # Save best model\n",
    "                if val_acc > self.best_acc:\n",
    "                    self.best_acc = val_acc\n",
    "                    print(f\"  ✅ New best accuracy: {val_acc:.4f}\")\n",
    "                \n",
    "                # Log results\n",
    "                print(f\"Epoch {global_epoch:2d} | Loss: {train_loss:.4f} | \"\n",
    "                      f\"TrainAcc: {train_acc:.4f} | ValAcc: {val_acc:.4f} | \"\n",
    "                      f\"ValF1: {val_f1:.4f} | λ: {lambda_reg:.3f}\")\n",
    "                \n",
    "                self.history.append({\n",
    "                    'epoch': global_epoch,\n",
    "                    'phase': phase.name,\n",
    "                    'train_loss': train_loss,\n",
    "                    'train_acc': train_acc,\n",
    "                    'val_loss': val_loss,\n",
    "                    'val_acc': val_acc,\n",
    "                    'val_f1': val_f1,\n",
    "                    'lambda_reg': lambda_reg\n",
    "                })\n",
    "        \n",
    "        print(f\"\\n🎉 Training completed! Best accuracy: {self.best_acc:.4f}\")\n",
    "        return self.history\n",
    "    \n",
    "    def _train_epoch(self, optimizer, phase, lambda_reg):\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch in self.train_loader:\n",
    "            images = batch['image'].to(self.device)\n",
    "            labels = batch['expression'].to(self.device)\n",
    "            va_values = batch['va_values'].to(self.device)\n",
    "            valid_reg = batch['valid_regression'].to(self.device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            cls_logits, reg_outputs = self.model(images)\n",
    "            \n",
    "            # Classification loss\n",
    "            if phase.label_smoothing > 0:\n",
    "                cls_loss = self._label_smooth_loss(cls_logits, labels, phase.label_smoothing)\n",
    "            else:\n",
    "                cls_loss = F.cross_entropy(cls_logits, labels)\n",
    "            \n",
    "            # Regression loss\n",
    "            if lambda_reg > 0 and valid_reg.sum() > 0:\n",
    "                reg_pred = reg_outputs[valid_reg]\n",
    "                reg_true = va_values[valid_reg]\n",
    "                reg_loss = F.smooth_l1_loss(reg_pred, reg_true)\n",
    "            else:\n",
    "                reg_loss = torch.tensor(0.0, device=self.device)\n",
    "            \n",
    "            # Combined loss\n",
    "            loss = cls_loss + lambda_reg * reg_loss\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Statistics\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = cls_logits.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        return total_loss / len(self.train_loader), correct / total\n",
    "    \n",
    "    def _validate_epoch(self, lambda_reg):\n",
    "        self.model.eval()\n",
    "        total_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in self.val_loader:\n",
    "                images = batch['image'].to(self.device)\n",
    "                labels = batch['expression'].to(self.device)\n",
    "                va_values = batch['va_values'].to(self.device)\n",
    "                valid_reg = batch['valid_regression'].to(self.device)\n",
    "                \n",
    "                cls_logits, reg_outputs = self.model(images)\n",
    "                \n",
    "                # Classification loss\n",
    "                cls_loss = F.cross_entropy(cls_logits, labels)\n",
    "                \n",
    "                # Regression loss\n",
    "                if lambda_reg > 0 and valid_reg.sum() > 0:\n",
    "                    reg_pred = reg_outputs[valid_reg]\n",
    "                    reg_true = va_values[valid_reg]\n",
    "                    reg_loss = F.smooth_l1_loss(reg_pred, reg_true)\n",
    "                else:\n",
    "                    reg_loss = torch.tensor(0.0, device=self.device)\n",
    "                \n",
    "                loss = cls_loss + lambda_reg * reg_loss\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                _, predicted = cls_logits.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "                \n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        accuracy = correct / total\n",
    "        f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "        \n",
    "        return total_loss / len(self.val_loader), accuracy, f1\n",
    "    \n",
    "    def _label_smooth_loss(self, logits, targets, smoothing=0.1):\n",
    "        \"\"\"Label smoothing cross entropy loss\"\"\"\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        num_classes = logits.size(-1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(log_probs)\n",
    "            true_dist.fill_(smoothing / (num_classes - 1))\n",
    "            true_dist.scatter_(1, targets.unsqueeze(1), 1.0 - smoothing)\n",
    "        \n",
    "        return (-true_dist * log_probs).sum(dim=-1).mean()\n",
    "\n",
    "print(\"✅ AccuracyFirstTrainer class ready!\")\n",
    "print(\"Usage: trainer = AccuracyFirstTrainer(model, train_loader, val_loader, device)\")\n",
    "print(\"       history = trainer.train()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a8fdc9",
   "metadata": {},
   "source": [
    "## 7. Losses & Optimizers\n",
    "\n",
    "Setting up multi-task loss functions, optimizers, and training utilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1866b8",
   "metadata": {},
   "source": [
    "## Accuracy-First Phased Trainer (Consolidated)\n",
    "This section introduces a structured phased training strategy prioritized for classification convergence before gradually integrating the regression objectives. It replaces earlier scattered draft cells.\n",
    "\n",
    "Phases:\n",
    "1. Warmup (classification only, backbone frozen)\n",
    "2. Head Focus (classification dominant, light regression ramp)\n",
    "3. Joint Fine-tune (balanced multi-task)\n",
    "\n",
    "Key features: differential learning rates, lambda_reg ramp, optional mixup/cutmix, cosine schedule per phase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9790ff60",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T18:01:07.339597Z",
     "iopub.status.busy": "2025-09-27T18:01:07.339280Z",
     "iopub.status.idle": "2025-09-27T18:01:07.366911Z",
     "shell.execute_reply": "2025-09-27T18:01:07.366145Z",
     "shell.execute_reply.started": "2025-09-27T18:01:07.339571Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MultiTaskLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Combined loss for multi-task learning\n",
    "    \"\"\"\n",
    "    def __init__(self, lambda_reg=1.0, class_weights=None):\n",
    "        super(MultiTaskLoss, self).__init__()\n",
    "        self.lambda_reg = lambda_reg\n",
    "        \n",
    "        # Classification loss\n",
    "        if class_weights is not None:\n",
    "            self.cls_criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        else:\n",
    "            self.cls_criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Regression loss (smooth L1 is less sensitive to outliers than MSE)\n",
    "        self.reg_criterion = nn.SmoothL1Loss(reduction='none')\n",
    "        \n",
    "    def forward(self, cls_logits, reg_outputs, cls_targets, reg_targets, valid_regression):\n",
    "        \"\"\"\n",
    "        Calculate combined loss\n",
    "        \n",
    "        Args:\n",
    "            cls_logits: Classification predictions (N, num_classes)\n",
    "            reg_outputs: Regression predictions (N, 2) for [valence, arousal]\n",
    "            cls_targets: Classification targets (N,)\n",
    "            reg_targets: Regression targets (N, 2)\n",
    "            valid_regression: Binary mask for valid regression samples (N,)\n",
    "        \"\"\"\n",
    "        # Classification loss (all samples)\n",
    "        cls_loss = self.cls_criterion(cls_logits, cls_targets)\n",
    "        \n",
    "        # Regression loss (only valid samples)\n",
    "        reg_loss = torch.tensor(0.0, device=cls_logits.device)\n",
    "        if valid_regression.any():\n",
    "            valid_mask = valid_regression.bool()\n",
    "            reg_loss_per_sample = self.reg_criterion(\n",
    "                reg_outputs[valid_mask], \n",
    "                reg_targets[valid_mask]\n",
    "            )\n",
    "            # Average over dimensions and samples\n",
    "            reg_loss = reg_loss_per_sample.mean()\n",
    "        \n",
    "        # Combined loss\n",
    "        total_loss = cls_loss + self.lambda_reg * reg_loss\n",
    "        \n",
    "        return {\n",
    "            'total_loss': total_loss,\n",
    "            'classification_loss': cls_loss,\n",
    "            'regression_loss': reg_loss\n",
    "        }\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping utility\"\"\"\n",
    "    def __init__(self, patience=10, min_delta=1e-4, mode='max'):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.mode = mode\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        \n",
    "    def __call__(self, score):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "        elif self.mode == 'max':\n",
    "            if score > self.best_score + self.min_delta:\n",
    "                self.best_score = score\n",
    "                self.counter = 0\n",
    "            else:\n",
    "                self.counter += 1\n",
    "        else:  # mode == 'min'\n",
    "            if score < self.best_score - self.min_delta:\n",
    "                self.best_score = score\n",
    "                self.counter = 0\n",
    "            else:\n",
    "                self.counter += 1\n",
    "        \n",
    "        if self.counter >= self.patience:\n",
    "            self.early_stop = True\n",
    "        \n",
    "        return self.early_stop\n",
    "\n",
    "class Trainer:\n",
    "    \"\"\"\n",
    "    Multi-task model trainer\n",
    "    \"\"\"\n",
    "    def __init__(self, model, train_loader, val_loader, device, \n",
    "                 lambda_reg=1.0, lr=3e-4, weight_decay=1e-4,\n",
    "                 patience=15, class_weights=None):\n",
    "        self.model = model.to(device)\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.device = device\n",
    "        \n",
    "        # Loss function\n",
    "        if class_weights is not None:\n",
    "            class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "        self.criterion = MultiTaskLoss(lambda_reg=lambda_reg, class_weights=class_weights)\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            model.parameters(), \n",
    "            lr=lr, \n",
    "            weight_decay=weight_decay\n",
    "        )\n",
    "        \n",
    "        # Learning rate scheduler\n",
    "        self.scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            self.optimizer,\n",
    "            max_lr=lr,\n",
    "            epochs=50,  # Will be updated in train method\n",
    "            steps_per_epoch=len(train_loader),\n",
    "            pct_start=0.1,\n",
    "            anneal_strategy='cos'\n",
    "        )\n",
    "        \n",
    "        # Early stopping\n",
    "        self.early_stopping = EarlyStopping(patience=patience, mode='max')\n",
    "        \n",
    "        # Metrics calculator\n",
    "        self.metrics_calculator = MetricsCalculator(num_classes=8)\n",
    "        \n",
    "        # Training history\n",
    "        self.history = {\n",
    "            'train_loss': [], 'val_loss': [],\n",
    "            'train_cls_loss': [], 'val_cls_loss': [],\n",
    "            'train_reg_loss': [], 'val_reg_loss': [],\n",
    "            'val_accuracy': [], 'val_f1': [],\n",
    "            'val_ccc_valence': [], 'val_ccc_arousal': [],\n",
    "            'val_combined_ccc': [], 'lr': []\n",
    "        }\n",
    "        \n",
    "        self.best_score = -float('inf')\n",
    "        self.best_epoch = 0\n",
    "        \n",
    "    def train_epoch(self):\n",
    "        \"\"\"Train for one epoch\"\"\"\n",
    "        self.model.train()\n",
    "        \n",
    "        total_loss = 0\n",
    "        total_cls_loss = 0\n",
    "        total_reg_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        pbar = tqdm(self.train_loader, desc='Training')\n",
    "        for batch in pbar:\n",
    "            # Move data to device\n",
    "            images = batch['image'].to(self.device)\n",
    "            expressions = batch['expression'].to(self.device)\n",
    "            va_values = batch['va_values'].to(self.device)\n",
    "            valid_regression = batch['valid_regression'].to(self.device)\n",
    "            \n",
    "            # Forward pass\n",
    "            cls_logits, reg_outputs = self.model(images)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss_dict = self.criterion(\n",
    "                cls_logits, reg_outputs, expressions, va_values, valid_regression\n",
    "            )\n",
    "            \n",
    "            total_loss_batch = loss_dict['total_loss']\n",
    "            \n",
    "            # Backward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            total_loss_batch.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "            \n",
    "            # Accumulate losses\n",
    "            total_loss += total_loss_batch.item()\n",
    "            total_cls_loss += loss_dict['classification_loss'].item()\n",
    "            total_reg_loss += loss_dict['regression_loss'].item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                'Loss': f'{total_loss_batch.item():.4f}',\n",
    "                'LR': f'{self.optimizer.param_groups[0][\"lr\"]:.2e}'\n",
    "            })\n",
    "        \n",
    "        return {\n",
    "            'train_loss': total_loss / num_batches,\n",
    "            'train_cls_loss': total_cls_loss / num_batches,\n",
    "            'train_reg_loss': total_reg_loss / num_batches\n",
    "        }\n",
    "    \n",
    "    def validate_epoch(self):\n",
    "        \"\"\"Validate for one epoch\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        total_loss = 0\n",
    "        total_cls_loss = 0\n",
    "        total_reg_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        all_cls_preds = []\n",
    "        all_cls_targets = []\n",
    "        all_cls_probs = []\n",
    "        all_reg_preds = []\n",
    "        all_reg_targets = []\n",
    "        all_valid_regression = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pbar = tqdm(self.val_loader, desc='Validation')\n",
    "            for batch in pbar:\n",
    "                # Move data to device\n",
    "                images = batch['image'].to(self.device)\n",
    "                expressions = batch['expression'].to(self.device)\n",
    "                va_values = batch['va_values'].to(self.device)\n",
    "                valid_regression = batch['valid_regression'].to(self.device)\n",
    "                \n",
    "                # Forward pass\n",
    "                cls_logits, reg_outputs = self.model(images)\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss_dict = self.criterion(\n",
    "                    cls_logits, reg_outputs, expressions, va_values, valid_regression\n",
    "                )\n",
    "                \n",
    "                # Accumulate losses\n",
    "                total_loss += loss_dict['total_loss'].item()\n",
    "                total_cls_loss += loss_dict['classification_loss'].item()\n",
    "                total_reg_loss += loss_dict['regression_loss'].item()\n",
    "                num_batches += 1\n",
    "                \n",
    "                # Get predictions\n",
    "                cls_probs = torch.softmax(cls_logits, dim=1)\n",
    "                cls_preds = torch.argmax(cls_logits, dim=1)\n",
    "                \n",
    "                # Store predictions and targets\n",
    "                all_cls_preds.append(cls_preds.cpu())\n",
    "                all_cls_targets.append(expressions.cpu())\n",
    "                all_cls_probs.append(cls_probs.cpu())\n",
    "                all_reg_preds.append(reg_outputs.cpu())\n",
    "                all_reg_targets.append(va_values.cpu())\n",
    "                all_valid_regression.append(valid_regression.cpu())\n",
    "        \n",
    "        # Concatenate all predictions\n",
    "        all_cls_preds = torch.cat(all_cls_preds)\n",
    "        all_cls_targets = torch.cat(all_cls_targets)\n",
    "        all_cls_probs = torch.cat(all_cls_probs)\n",
    "        all_reg_preds = torch.cat(all_reg_preds)\n",
    "        all_reg_targets = torch.cat(all_reg_targets)\n",
    "        all_valid_regression = torch.cat(all_valid_regression)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        predictions = {\n",
    "            'classification': all_cls_preds,\n",
    "            'classification_prob': all_cls_probs,\n",
    "            'regression': all_reg_preds\n",
    "        }\n",
    "        targets = {\n",
    "            'classification': all_cls_targets,\n",
    "            'regression': all_reg_targets\n",
    "        }\n",
    "        \n",
    "        metrics = self.metrics_calculator.calculate_all_metrics(\n",
    "            predictions, targets, all_valid_regression.bool()\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'val_loss': total_loss / num_batches,\n",
    "            'val_cls_loss': total_cls_loss / num_batches,\n",
    "            'val_reg_loss': total_reg_loss / num_batches,\n",
    "            'metrics': metrics\n",
    "        }\n",
    "    \n",
    "    def train(self, epochs, warmup_epochs=2, save_dir=None):\n",
    "        \"\"\"\n",
    "        Complete training loop\n",
    "        \"\"\"\n",
    "        print(f\"Starting training for {epochs} epochs...\")\n",
    "        print(f\"Warmup epochs (frozen backbone): {warmup_epochs}\")\n",
    "        \n",
    "        if save_dir:\n",
    "            save_dir = Path(save_dir)\n",
    "            save_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Freeze backbone for warmup\n",
    "        if warmup_epochs > 0:\n",
    "            self.model.freeze_backbone()\n",
    "        \n",
    "        # Update scheduler for actual number of epochs\n",
    "        self.scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            self.optimizer,\n",
    "            max_lr=self.optimizer.param_groups[0]['lr'],\n",
    "            epochs=epochs,\n",
    "            steps_per_epoch=len(self.train_loader),\n",
    "            pct_start=0.1,\n",
    "            anneal_strategy='cos'\n",
    "        )\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            print(f\"\\\\nEpoch {epoch+1}/{epochs}\")\n",
    "            \n",
    "            # Unfreeze backbone after warmup\n",
    "            if epoch == warmup_epochs:\n",
    "                self.model.unfreeze_backbone()\n",
    "                print(\"Unfroze backbone - training all parameters\")\n",
    "            \n",
    "            # Training\n",
    "            train_metrics = self.train_epoch()\n",
    "            \n",
    "            # Validation\n",
    "            val_metrics = self.validate_epoch()\n",
    "            \n",
    "            # Store history\n",
    "            self.history['train_loss'].append(train_metrics['train_loss'])\n",
    "            self.history['val_loss'].append(val_metrics['val_loss'])\n",
    "            self.history['train_cls_loss'].append(train_metrics['train_cls_loss'])\n",
    "            self.history['val_cls_loss'].append(val_metrics['val_cls_loss'])\n",
    "            self.history['train_reg_loss'].append(train_metrics['train_reg_loss'])\n",
    "            self.history['val_reg_loss'].append(val_metrics['val_reg_loss'])\n",
    "            self.history['lr'].append(self.optimizer.param_groups[0]['lr'])\n",
    "            \n",
    "            # Extract key metrics\n",
    "            val_acc = val_metrics['metrics']['classification']['accuracy']\n",
    "            val_f1 = val_metrics['metrics']['classification']['f1_macro']\n",
    "            val_ccc_val = val_metrics['metrics']['valence']['ccc']\n",
    "            val_ccc_aro = val_metrics['metrics']['arousal']['ccc']\n",
    "            val_combined_ccc = val_metrics['metrics']['combined_ccc']\n",
    "            \n",
    "            self.history['val_accuracy'].append(val_acc)\n",
    "            self.history['val_f1'].append(val_f1)\n",
    "            self.history['val_ccc_valence'].append(val_ccc_val)\n",
    "            self.history['val_ccc_arousal'].append(val_ccc_aro)\n",
    "            self.history['val_combined_ccc'].append(val_combined_ccc)\n",
    "            \n",
    "            # Print metrics\n",
    "            print(f\"Train Loss: {train_metrics['train_loss']:.4f} | Val Loss: {val_metrics['val_loss']:.4f}\")\n",
    "            print(f\"Val Accuracy: {val_acc:.4f} | Val F1: {val_f1:.4f}\")\n",
    "            print(f\"Val CCC - Valence: {val_ccc_val:.4f} | Arousal: {val_ccc_aro:.4f} | Combined: {val_combined_ccc:.4f}\")\n",
    "            \n",
    "            # Check for best model (using combined CCC)\n",
    "            current_score = val_combined_ccc if not np.isnan(val_combined_ccc) else -1\n",
    "            if current_score > self.best_score:\n",
    "                self.best_score = current_score\n",
    "                self.best_epoch = epoch\n",
    "                \n",
    "                if save_dir:\n",
    "                    torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'model_state_dict': self.model.state_dict(),\n",
    "                        'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                        'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "                        'best_score': self.best_score,\n",
    "                        'history': self.history\n",
    "                    }, save_dir / 'best_model.pt')\n",
    "                    print(f\"Saved best model (CCC: {current_score:.4f})\")\n",
    "            \n",
    "            # Early stopping\n",
    "            if self.early_stopping(current_score):\n",
    "                print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "                break\n",
    "        \n",
    "        print(f\"\\\\nTraining completed!\")\n",
    "        print(f\"Best validation CCC: {self.best_score:.4f} at epoch {self.best_epoch+1}\")\n",
    "        \n",
    "        return self.history\n",
    "\n",
    "print(\"Training infrastructure ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dafc98e",
   "metadata": {},
   "source": [
    "## 8. Trainer\n",
    "\n",
    "Training all models and comparing performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374a1823",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T18:01:07.367890Z",
     "iopub.status.busy": "2025-09-27T18:01:07.367719Z",
     "iopub.status.idle": "2025-09-27T18:22:00.137803Z",
     "shell.execute_reply": "2025-09-27T18:22:00.137029Z",
     "shell.execute_reply.started": "2025-09-27T18:01:07.367877Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Training Configuration\n",
    "TRAINING_CONFIG = {\n",
    "    'batch_size': 32,\n",
    "    'num_workers': 0,  # Set to 0 to avoid pickle issues\n",
    "    'epochs': 10,  # Reduced for faster testing\n",
    "    'warmup_epochs': 1,  # Reduced for faster testing\n",
    "    'lambda_reg': 1.0,  # Balance between classification and regression loss\n",
    "    'lr': 3e-4,\n",
    "    'weight_decay': 1e-4,\n",
    "    'patience': 8  # Reduced for faster testing\n",
    "}\n",
    "\n",
    "# Models to train (start with just one for testing)\n",
    "MODELS_TO_TRAIN = [\n",
    "    'vgg16',\n",
    "    'efficientnet_b0', \n",
    "    'efficientnet_b2',\n",
    "    'convnext_tiny'\n",
    "]\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "for key, value in TRAINING_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\nModels to train: {MODELS_TO_TRAIN}\")\n",
    "\n",
    "# Calculate class weights for balanced training (optional)\n",
    "class_counts = train_df_full['expression'].value_counts().sort_index()\n",
    "total_samples = len(train_df_full)\n",
    "class_weights = []\n",
    "for i in range(len(EMOTION_LABELS)):\n",
    "    if i in class_counts.index:\n",
    "        weight = total_samples / (len(EMOTION_LABELS) * class_counts[i])\n",
    "        class_weights.append(weight)\n",
    "    else:\n",
    "        class_weights.append(1.0)\n",
    "\n",
    "print(f\"\\nClass weights (for balanced training):\")\n",
    "for i, (emotion, weight) in enumerate(zip(EMOTION_LABELS.values(), class_weights)):\n",
    "    print(f\"  {i} ({emotion}): {weight:.3f}\")\n",
    "\n",
    "# Training results storage\n",
    "training_results = {}\n",
    "trained_models = {}\n",
    "\n",
    "# Train each model\n",
    "for model_name in MODELS_TO_TRAIN:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training {model_name.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    try:\n",
    "        # Create model\n",
    "        model = create_model(model_name, num_classes=len(EMOTION_LABELS))\n",
    "        print(f\"Created {model_name} model\")\n",
    "        \n",
    "        # Get appropriate transforms\n",
    "        if model_name in transforms_dict:\n",
    "            train_transform, val_transform, test_transform = transforms_dict[model_name]\n",
    "        else:\n",
    "            train_transform, val_transform, test_transform = transforms_dict['vgg16']  # fallback\n",
    "        \n",
    "        # Create data loaders with num_workers=0\n",
    "        train_loader, val_loader, test_loader, datasets = create_dataloaders(\n",
    "            train_df_full, val_df_full, test_df_full,\n",
    "            (train_transform, val_transform, test_transform),\n",
    "            batch_size=TRAINING_CONFIG['batch_size'],\n",
    "            num_workers=TRAINING_CONFIG['num_workers']\n",
    "        )\n",
    "        \n",
    "        print(f\"Created data loaders:\")\n",
    "        print(f\"  Train: {len(train_loader)} batches\")\n",
    "        print(f\"  Val: {len(val_loader)} batches\") \n",
    "        print(f\"  Test: {len(test_loader)} batches\")\n",
    "        \n",
    "        # Create trainer\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            device=device,\n",
    "            lambda_reg=TRAINING_CONFIG['lambda_reg'],\n",
    "            lr=TRAINING_CONFIG['lr'],\n",
    "            weight_decay=TRAINING_CONFIG['weight_decay'],\n",
    "            patience=TRAINING_CONFIG['patience'],\n",
    "            class_weights=class_weights\n",
    "        )\n",
    "        \n",
    "        # Create save directory\n",
    "        save_dir = ARTIFACTS_DIR / model_name\n",
    "        save_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Train the model\n",
    "        print(f\"Starting training...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        history = trainer.train(\n",
    "            epochs=TRAINING_CONFIG['epochs'],\n",
    "            warmup_epochs=TRAINING_CONFIG['warmup_epochs'],\n",
    "            save_dir=save_dir\n",
    "        )\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        print(f\"Training completed in {training_time:.2f} seconds\")\n",
    "        print(f\"Average time per epoch: {training_time/len(history['train_loss']):.2f} seconds\")\n",
    "        \n",
    "        # Store results\n",
    "        training_results[model_name] = {\n",
    "            'history': history,\n",
    "            'training_time': training_time,\n",
    "            'best_score': trainer.best_score,\n",
    "            'best_epoch': trainer.best_epoch,\n",
    "            'total_epochs': len(history['train_loss'])\n",
    "        }\n",
    "        \n",
    "        trained_models[model_name] = {\n",
    "            'model': model,\n",
    "            'trainer': trainer,\n",
    "            'test_loader': test_loader\n",
    "        }\n",
    "        \n",
    "        # Save training history\n",
    "        with open(save_dir / 'training_history.json', 'w') as f:\n",
    "            # Convert numpy arrays to lists for JSON serialization\n",
    "            history_json = {}\n",
    "            for key, values in history.items():\n",
    "                if isinstance(values, list):\n",
    "                    history_json[key] = [float(v) if not np.isnan(v) else None for v in values]\n",
    "                else:\n",
    "                    history_json[key] = float(values) if not np.isnan(values) else None\n",
    "            json.dump(history_json, f, indent=2)\n",
    "        \n",
    "        print(f\"Saved training history to {save_dir / 'training_history.json'}\")\n",
    "        \n",
    "        # Quick validation summary\n",
    "        print(f\"\\nTraining Summary for {model_name}:\")\n",
    "        print(f\"  Best validation CCC: {trainer.best_score:.4f}\")\n",
    "        print(f\"  Best epoch: {trainer.best_epoch + 1}\")\n",
    "        print(f\"  Total epochs trained: {len(history['train_loss'])}\")\n",
    "        print(f\"  Final validation accuracy: {history['val_accuracy'][-1]:.4f}\")\n",
    "        print(f\"  Final validation F1: {history['val_f1'][-1]:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR training {model_name}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"TRAINING COMPLETED\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(f\"Successfully trained {len(training_results)} models:\")\n",
    "for model_name in training_results.keys():\n",
    "    result = training_results[model_name]\n",
    "    print(f\"  {model_name}: Best CCC = {result['best_score']:.4f} (epoch {result['best_epoch']+1})\")\n",
    "\n",
    "# Save overall results\n",
    "with open(ARTIFACTS_DIR / 'training_summary.json', 'w') as f:\n",
    "    summary = {}\n",
    "    for model_name, result in training_results.items():\n",
    "        summary[model_name] = {\n",
    "            'best_score': float(result['best_score']),\n",
    "            'best_epoch': int(result['best_epoch']),\n",
    "            'total_epochs': int(result['total_epochs']),\n",
    "            'training_time': float(result['training_time'])\n",
    "        }\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"\\nSaved training summary to {ARTIFACTS_DIR / 'training_summary.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de461a1c",
   "metadata": {},
   "source": [
    "## 9. Evaluation & Plots\n",
    "\n",
    "Comprehensive evaluation on test set and visualization of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b451b89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-27T18:22:00.139310Z",
     "iopub.status.busy": "2025-09-27T18:22:00.138794Z",
     "iopub.status.idle": "2025-09-27T18:22:00.538109Z",
     "shell.execute_reply": "2025-09-27T18:22:00.537180Z",
     "shell.execute_reply.started": "2025-09-27T18:22:00.139292Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model_on_test(model, test_loader, device, metrics_calculator):\n",
    "    \"\"\"Evaluate a model on the test set\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    all_cls_preds = []\n",
    "    all_cls_targets = []\n",
    "    all_cls_probs = []\n",
    "    all_reg_preds = []\n",
    "    all_reg_targets = []\n",
    "    all_valid_regression = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc='Testing'):\n",
    "            images = batch['image'].to(device)\n",
    "            expressions = batch['expression'].to(device)\n",
    "            va_values = batch['va_values'].to(device)\n",
    "            valid_regression = batch['valid_regression'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            cls_logits, reg_outputs = model(images)\n",
    "            \n",
    "            # Get predictions\n",
    "            cls_probs = torch.softmax(cls_logits, dim=1)\n",
    "            cls_preds = torch.argmax(cls_logits, dim=1)\n",
    "            \n",
    "            # Store predictions and targets\n",
    "            all_cls_preds.append(cls_preds.cpu())\n",
    "            all_cls_targets.append(expressions.cpu())\n",
    "            all_cls_probs.append(cls_probs.cpu())\n",
    "            all_reg_preds.append(reg_outputs.cpu())\n",
    "            all_reg_targets.append(va_values.cpu())\n",
    "            all_valid_regression.append(valid_regression.cpu())\n",
    "    \n",
    "    # Concatenate all predictions\n",
    "    all_cls_preds = torch.cat(all_cls_preds)\n",
    "    all_cls_targets = torch.cat(all_cls_targets)\n",
    "    all_cls_probs = torch.cat(all_cls_probs)\n",
    "    all_reg_preds = torch.cat(all_reg_preds)\n",
    "    all_reg_targets = torch.cat(all_reg_targets)\n",
    "    all_valid_regression = torch.cat(all_valid_regression)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    predictions = {\n",
    "        'classification': all_cls_preds,\n",
    "        'classification_prob': all_cls_probs,\n",
    "        'regression': all_reg_preds\n",
    "    }\n",
    "    targets = {\n",
    "        'classification': all_cls_targets,\n",
    "        'regression': all_reg_targets\n",
    "    }\n",
    "    \n",
    "    metrics = metrics_calculator.calculate_all_metrics(\n",
    "        predictions, targets, all_valid_regression.bool()\n",
    "    )\n",
    "    \n",
    "    return metrics, predictions, targets, all_valid_regression\n",
    "\n",
    "def plot_training_curves(training_results, save_path=None):\n",
    "    \"\"\"Plot training curves for all models\"\"\"\n",
    "    n_models = len(training_results)\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, n_models))\n",
    "    \n",
    "    # Plot 1: Loss curves\n",
    "    ax = axes[0]\n",
    "    for i, (model_name, result) in enumerate(training_results.items()):\n",
    "        history = result['history']\n",
    "        epochs = range(1, len(history['train_loss']) + 1)\n",
    "        ax.plot(epochs, history['train_loss'], '-', color=colors[i], alpha=0.7, label=f'{model_name} Train')\n",
    "        ax.plot(epochs, history['val_loss'], '--', color=colors[i], alpha=0.9, label=f'{model_name} Val')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title('Training and Validation Loss')\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Classification metrics\n",
    "    ax = axes[1]\n",
    "    for i, (model_name, result) in enumerate(training_results.items()):\n",
    "        history = result['history']\n",
    "        epochs = range(1, len(history['val_accuracy']) + 1)\n",
    "        ax.plot(epochs, history['val_accuracy'], '-', color=colors[i], alpha=0.8, label=f'{model_name} Accuracy')\n",
    "        ax.plot(epochs, history['val_f1'], '--', color=colors[i], alpha=0.8, label=f'{model_name} F1')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.set_title('Classification Metrics')\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Regression metrics (CCC)\n",
    "    ax = axes[2]\n",
    "    for i, (model_name, result) in enumerate(training_results.items()):\n",
    "        history = result['history']\n",
    "        epochs = range(1, len(history['val_ccc_valence']) + 1)\n",
    "        ax.plot(epochs, history['val_ccc_valence'], '-', color=colors[i], alpha=0.8, label=f'{model_name} Valence CCC')\n",
    "        ax.plot(epochs, history['val_ccc_arousal'], '--', color=colors[i], alpha=0.8, label=f'{model_name} Arousal CCC')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('CCC')\n",
    "    ax.set_title('Regression Metrics (CCC)')\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Learning rate\n",
    "    ax = axes[3]\n",
    "    for i, (model_name, result) in enumerate(training_results.items()):\n",
    "        history = result['history']\n",
    "        epochs = range(1, len(history['lr']) + 1)\n",
    "        ax.semilogy(epochs, history['lr'], color=colors[i], alpha=0.8, label=model_name)\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Learning Rate')\n",
    "    ax.set_title('Learning Rate Schedule')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrices(test_results, save_path=None):\n",
    "    \"\"\"Plot confusion matrices for all models\"\"\"\n",
    "    n_models = len(test_results)\n",
    "    cols = min(3, n_models)\n",
    "    rows = (n_models + cols - 1) // cols\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(5*cols, 4*rows))\n",
    "    if n_models == 1:\n",
    "        axes = [axes]\n",
    "    elif rows == 1:\n",
    "        axes = [axes]\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    for i, (model_name, (metrics, predictions, targets, valid_regression)) in enumerate(test_results.items()):\n",
    "        if i >= len(axes):\n",
    "            break\n",
    "            \n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Calculate confusion matrix\n",
    "        cm = confusion_matrix(targets['classification'], predictions['classification'])\n",
    "        \n",
    "        # Plot confusion matrix\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "                   xticklabels=[EMOTION_LABELS[j] for j in range(len(EMOTION_LABELS))],\n",
    "                   yticklabels=[EMOTION_LABELS[j] for j in range(len(EMOTION_LABELS))])\n",
    "        \n",
    "        ax.set_title(f'{model_name.replace(\"_\", \"-\").title()}\\\\nAccuracy: {metrics[\"classification\"][\"accuracy\"]:.3f}')\n",
    "        ax.set_xlabel('Predicted')\n",
    "        ax.set_ylabel('True')\n",
    "        \n",
    "        # Rotate labels for better readability\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "        ax.set_yticklabels(ax.get_yticklabels(), rotation=0)\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for i in range(len(test_results), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_regression_results(test_results, save_path=None):\n",
    "    \"\"\"Plot regression results (valence/arousal predictions)\"\"\"\n",
    "    n_models = len(test_results)\n",
    "    fig, axes = plt.subplots(n_models, 2, figsize=(12, 4*n_models))\n",
    "    \n",
    "    if n_models == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i, (model_name, (metrics, predictions, targets, valid_regression)) in enumerate(test_results.items()):\n",
    "        # Filter valid regression samples\n",
    "        valid_mask = valid_regression.bool()\n",
    "        if not valid_mask.any():\n",
    "            continue\n",
    "            \n",
    "        reg_pred = predictions['regression'][valid_mask]\n",
    "        reg_target = targets['regression'][valid_mask]\n",
    "        \n",
    "        # Valence plot\n",
    "        ax_val = axes[i, 0]\n",
    "        ax_val.scatter(reg_target[:, 0], reg_pred[:, 0], alpha=0.6, s=20)\n",
    "        ax_val.plot([-1, 1], [-1, 1], 'r--', alpha=0.8)  # Perfect prediction line\n",
    "        ax_val.set_xlabel('True Valence')\n",
    "        ax_val.set_ylabel('Predicted Valence')\n",
    "        ax_val.set_title(f'{model_name} - Valence\\\\nCCC: {metrics[\"valence\"][\"ccc\"]:.3f}, RMSE: {metrics[\"valence\"][\"rmse\"]:.3f}')\n",
    "        ax_val.set_xlim(-1.1, 1.1)\n",
    "        ax_val.set_ylim(-1.1, 1.1)\n",
    "        ax_val.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Arousal plot\n",
    "        ax_aro = axes[i, 1]\n",
    "        ax_aro.scatter(reg_target[:, 1], reg_pred[:, 1], alpha=0.6, s=20)\n",
    "        ax_aro.plot([-1, 1], [-1, 1], 'r--', alpha=0.8)  # Perfect prediction line\n",
    "        ax_aro.set_xlabel('True Arousal')\n",
    "        ax_aro.set_ylabel('Predicted Arousal')\n",
    "        ax_aro.set_title(f'{model_name} - Arousal\\\\nCCC: {metrics[\"arousal\"][\"ccc\"]:.3f}, RMSE: {metrics[\"arousal\"][\"rmse\"]:.3f}')\n",
    "        ax_aro.set_xlim(-1.1, 1.1)\n",
    "        ax_aro.set_ylim(-1.1, 1.1)\n",
    "        ax_aro.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Evaluate all trained models on test set\n",
    "test_results = {}\n",
    "\n",
    "print(\"Evaluating models on test set...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for model_name in trained_models.keys():\n",
    "    print(f\"\\\\nEvaluating {model_name}...\")\n",
    "    \n",
    "    model_info = trained_models[model_name]\n",
    "    model = model_info['model']\n",
    "    test_loader = model_info['test_loader']\n",
    "    \n",
    "    # Load best model checkpoint\n",
    "    checkpoint_path = ARTIFACTS_DIR / model_name / 'best_model.pt'\n",
    "    if checkpoint_path.exists():\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(f\"Loaded best checkpoint from epoch {checkpoint['epoch'] + 1}\")\n",
    "    \n",
    "    # Evaluate\n",
    "    metrics, predictions, targets, valid_regression = evaluate_model_on_test(\n",
    "        model, test_loader, device, metrics_calculator\n",
    "    )\n",
    "    \n",
    "    test_results[model_name] = (metrics, predictions, targets, valid_regression)\n",
    "    \n",
    "    # Print key metrics\n",
    "    print(f\"Test Results for {model_name}:\")\n",
    "    print(f\"  Classification:\")\n",
    "    print(f\"    Accuracy: {metrics['classification']['accuracy']:.4f}\")\n",
    "    print(f\"    F1-macro: {metrics['classification']['f1_macro']:.4f}\")\n",
    "    print(f\"    Cohen's κ: {metrics['classification']['cohen_kappa']:.4f}\")\n",
    "    if not np.isnan(metrics['classification']['roc_auc_ovr']):\n",
    "        print(f\"    ROC-AUC: {metrics['classification']['roc_auc_ovr']:.4f}\")\n",
    "    print(f\"  Regression:\")\n",
    "    print(f\"    Valence CCC: {metrics['valence']['ccc']:.4f}\")\n",
    "    print(f\"    Arousal CCC: {metrics['arousal']['ccc']:.4f}\")\n",
    "    print(f\"    Combined CCC: {metrics['combined_ccc']:.4f}\")\n",
    "\n",
    "print(f\"\\\\nTest evaluation completed for {len(test_results)} models!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae7a8af",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-09-27T18:22:00.538684Z",
     "iopub.status.idle": "2025-09-27T18:22:00.538997Z",
     "shell.execute_reply": "2025-09-27T18:22:00.538834Z",
     "shell.execute_reply.started": "2025-09-27T18:22:00.538811Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Generate visualization plots\n",
    "print(\"\\nGenerating visualization plots...\")\n",
    "\n",
    "# Check if we have training results and test results\n",
    "if 'training_results' not in globals() or not training_results:\n",
    "    print(\"WARNING: No training results found. Models need to be trained first.\")\n",
    "    print(\"Available variables:\")\n",
    "    available_vars = [var for var in globals().keys() if not var.startswith('_')]\n",
    "    print(f\"  Available: {available_vars[:10]}...\")  # Show first 10 variables\n",
    "    \n",
    "if 'test_results' not in globals() or not test_results:\n",
    "    print(\"WARNING: No test results found. Models need to be evaluated first.\")\n",
    "\n",
    "# Only generate plots if we have results\n",
    "if 'training_results' in globals() and training_results:\n",
    "    # 1. Training curves\n",
    "    print(\"Plotting training curves...\")\n",
    "    plot_training_curves(training_results, save_path=FIGURES_DIR / 'training_curves.png')\n",
    "else:\n",
    "    print(\"Skipping training curves - no training results available\")\n",
    "\n",
    "if 'test_results' in globals() and test_results:\n",
    "    # 2. Confusion matrices\n",
    "    print(\"Plotting confusion matrices...\")\n",
    "    plot_confusion_matrices(test_results, save_path=FIGURES_DIR / 'confusion_matrices.png')\n",
    "    \n",
    "    # 3. Regression results\n",
    "    print(\"Plotting regression results...\")\n",
    "    plot_regression_results(test_results, save_path=FIGURES_DIR / 'regression_results.png')\n",
    "else:\n",
    "    print(\"Skipping confusion matrices and regression plots - no test results available\")\n",
    "\n",
    "print(\"\\nVisualization plots completed (or skipped due to missing data)!\")\n",
    "print(\"Note: You need to run the training cells first to generate the models and results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e083abbc",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-09-27T18:22:00.540381Z",
     "iopub.status.idle": "2025-09-27T18:22:00.540691Z",
     "shell.execute_reply": "2025-09-27T18:22:00.540593Z",
     "shell.execute_reply.started": "2025-09-27T18:22:00.540581Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Diagnostic check - verify required variables exist\n",
    "print(\"Checking required variables for plotting...\")\n",
    "\n",
    "# Check if required dictionaries exist and have data\n",
    "variables_to_check = ['training_results', 'test_results', 'trained_models', 'FIGURES_DIR']\n",
    "\n",
    "for var_name in variables_to_check:\n",
    "    if var_name in globals():\n",
    "        var_value = globals()[var_name]\n",
    "        if isinstance(var_value, dict):\n",
    "            print(f\"✓ {var_name}: {len(var_value)} entries - {list(var_value.keys())}\")\n",
    "        else:\n",
    "            print(f\"✓ {var_name}: {var_value}\")\n",
    "    else:\n",
    "        print(f\"✗ {var_name}: NOT FOUND\")\n",
    "\n",
    "# If variables are missing, we need to run previous sections first\n",
    "if 'training_results' not in globals() or len(training_results) == 0:\n",
    "    print(\"\\n⚠️  WARNING: training_results is empty or missing!\")\n",
    "    print(\"   You need to run the training section (## 8. Trainer) first.\")\n",
    "    \n",
    "if 'test_results' not in globals() or len(test_results) == 0:\n",
    "    print(\"\\n⚠️  WARNING: test_results is empty or missing!\")\n",
    "    print(\"   You need to run the evaluation section first.\")\n",
    "    \n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58189fb4",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-09-27T18:22:00.541501Z",
     "iopub.status.idle": "2025-09-27T18:22:00.541708Z",
     "shell.execute_reply": "2025-09-27T18:22:00.541621Z",
     "shell.execute_reply.started": "2025-09-27T18:22:00.541613Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Fix for the poor performance and loading issues\n",
    "\n",
    "# 1. PERFORMANCE FIXES\n",
    "# The poor accuracy (33%) suggests several potential issues:\n",
    "print(\"PERFORMANCE ANALYSIS:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Check if we have the required variables from training\n",
    "if 'train_df_full' not in globals():\n",
    "    print(\"Training data not available - need to run earlier cells\")\n",
    "else:\n",
    "    print(\"✓ Training data available\")\n",
    "    print(f\"  Training samples: {len(train_df_full)}\")\n",
    "    \n",
    "    # Check class distribution\n",
    "    class_dist = train_df_full['expression'].value_counts().sort_index()\n",
    "    print(f\"  Class distribution: {dict(class_dist)}\")\n",
    "    \n",
    "    # Check for class imbalance\n",
    "    min_class = class_dist.min()\n",
    "    max_class = class_dist.max()\n",
    "    imbalance_ratio = max_class / min_class\n",
    "    print(f\"  Imbalance ratio: {imbalance_ratio:.2f} (should be < 5 for good performance)\")\n",
    "    \n",
    "    if imbalance_ratio > 5:\n",
    "        print(\"  ⚠️ Severe class imbalance detected - this explains poor performance!\")\n",
    "\n",
    "# Check if models directory exists\n",
    "import os\n",
    "if os.path.exists('artifacts'):\n",
    "    models_trained = [d for d in os.listdir('artifacts') if os.path.isdir(os.path.join('artifacts', d))]\n",
    "    print(f\"\\nModels found in artifacts: {models_trained}\")\n",
    "else:\n",
    "    print(\"\\nNo artifacts directory found\")\n",
    "\n",
    "print(\"\\nIDENTIFIED ISSUES:\")\n",
    "print(\"-\" * 30)\n",
    "print(\"1. Low accuracy (33%) suggests:\")\n",
    "print(\"   - Learning rate too high/low\")\n",
    "print(\"   - Insufficient training epochs\")\n",
    "print(\"   - Class imbalance\")\n",
    "print(\"   - Poor data augmentation\")\n",
    "\n",
    "print(\"\\n2. PyTorch loading error:\")\n",
    "print(\"   - torch.load needs weights_only=False\")\n",
    "\n",
    "print(\"\\nPROPOSED FIXES:\")\n",
    "print(\"-\" * 15)\n",
    "print(\"1. Increase training epochs (50 → 100)\")\n",
    "print(\"2. Lower learning rate (3e-4 → 1e-4)\")  \n",
    "print(\"3. Add class weights for imbalanced data\")\n",
    "print(\"4. Fix torch.load with weights_only=False\")\n",
    "print(\"5. Reduce batch size for better gradients\")\n",
    "print(\"6. Add more regularization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25994b8",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-09-27T18:22:00.542700Z",
     "iopub.status.idle": "2025-09-27T18:22:00.542935Z",
     "shell.execute_reply": "2025-09-27T18:22:00.542839Z",
     "shell.execute_reply.started": "2025-09-27T18:22:00.542824Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# IMPROVED TRAINING CONFIGURATION\n",
    "# Based on the poor performance (33% accuracy), let's create better configs\n",
    "\n",
    "# First, let's reload the necessary variables from earlier cells if they're missing\n",
    "if 'EMOTION_LABELS' not in globals():\n",
    "    EMOTION_LABELS = {\n",
    "        0: 'Neutral', 1: 'Happy', 2: 'Sad', 3: 'Angry',\n",
    "        4: 'Fearful', 5: 'Disgusted', 6: 'Surprised', 7: 'Contemptuous'\n",
    "    }\n",
    "\n",
    "if 'ARTIFACTS_DIR' not in globals():\n",
    "    from pathlib import Path\n",
    "    ARTIFACTS_DIR = Path('./artifacts')\n",
    "    FIGURES_DIR = Path('./figures')\n",
    "\n",
    "if 'device' not in globals():\n",
    "    import torch\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Device: {device}\")\n",
    "\n",
    "# IMPROVED TRAINING CONFIG (fixing the poor performance)\n",
    "IMPROVED_TRAINING_CONFIG = {\n",
    "    'batch_size': 16,        # Reduced from 32 for better gradients\n",
    "    'num_workers': 0,        # Keep at 0 to avoid pickle issues\n",
    "    'epochs': 30,           # Reduced from 50 but with better learning\n",
    "    'warmup_epochs': 3,     # More warmup\n",
    "    'lambda_reg': 0.5,      # Reduced regression weight\n",
    "    'lr': 1e-4,             # Lower learning rate (was 3e-4)\n",
    "    'weight_decay': 5e-4,   # More regularization\n",
    "    'patience': 10,         # More patience\n",
    "    'dropout_rate': 0.4     # More dropout\n",
    "}\n",
    "\n",
    "print(\"IMPROVED TRAINING CONFIGURATION:\")\n",
    "print(\"=\"*40)\n",
    "for key, value in IMPROVED_TRAINING_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\nKey changes from original config:\")\n",
    "print(f\"  • Batch size: 32 → 16 (better gradients)\")\n",
    "print(f\"  • Learning rate: 3e-4 → 1e-4 (more stable)\")\n",
    "print(f\"  • Epochs: 50 → 30 (sufficient with better config)\")\n",
    "print(f\"  • Lambda reg: 1.0 → 0.5 (focus more on classification)\")\n",
    "print(f\"  • Weight decay: 1e-4 → 5e-4 (more regularization)\")\n",
    "print(f\"  • Dropout: 0.3 → 0.4 (reduce overfitting)\")\n",
    "\n",
    "# Fix the torch.load issue\n",
    "def safe_load_checkpoint(checkpoint_path, device):\n",
    "    \"\"\"Safely load checkpoint with proper weights_only setting\"\"\"\n",
    "    try:\n",
    "        # Try with weights_only=False for compatibility\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "        return checkpoint\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading checkpoint: {e}\")\n",
    "        return None\n",
    "\n",
    "print(f\"\\n✓ Created safe_load_checkpoint function to fix loading issues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf928881",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-09-27T18:22:00.543664Z",
     "iopub.status.idle": "2025-09-27T18:22:00.543881Z",
     "shell.execute_reply": "2025-09-27T18:22:00.543775Z",
     "shell.execute_reply.started": "2025-09-27T18:22:00.543766Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# QUICK EVALUATION OF EXISTING MODELS\n",
    "# Load and evaluate the models that were already trained\n",
    "\n",
    "print(\"LOADING EXISTING TRAINED MODELS:\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Check which models exist\n",
    "import os\n",
    "import json\n",
    "available_models = []\n",
    "\n",
    "if os.path.exists('artifacts'):\n",
    "    for model_dir in os.listdir('artifacts'):\n",
    "        model_path = os.path.join('artifacts', model_dir)\n",
    "        if os.path.isdir(model_path):\n",
    "            checkpoint_file = os.path.join(model_path, 'best_model.pt')\n",
    "            if os.path.exists(checkpoint_file):\n",
    "                available_models.append(model_dir)\n",
    "                print(f\"✓ Found {model_dir} checkpoint\")\n",
    "            else:\n",
    "                print(f\"No checkpoint for {model_dir}\")\n",
    "\n",
    "print(f\"\\nFound {len(available_models)} trained models: {available_models}\")\n",
    "\n",
    "# Load training summaries if available\n",
    "training_summary_path = 'artifacts/training_summary.json'\n",
    "if os.path.exists(training_summary_path):\n",
    "    with open(training_summary_path, 'r') as f:\n",
    "        training_summary = json.load(f)\n",
    "    \n",
    "    print(f\"\\nTRAINING RESULTS SUMMARY:\")\n",
    "    print(\"-\" * 30)\n",
    "    for model_name, results in training_summary.items():\n",
    "        print(f\"{model_name}:\")\n",
    "        print(f\"  Best CCC: {results['best_score']:.4f}\")\n",
    "        print(f\"  Best Epoch: {results['best_epoch']}\")\n",
    "        print(f\"  Training Time: {results['training_time']:.1f}s\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"No training summary found\")\n",
    "\n",
    "# Quick fix for the current evaluation issue:\n",
    "# The main problem is that the models were trained but variables were lost.\n",
    "# Let's create a minimal evaluation cell that can work with existing checkpoints.\n",
    "\n",
    "print(\"NEXT STEPS:\")\n",
    "print(\"-\" * 15)\n",
    "print(\"1. To run evaluation on existing models:\")\n",
    "print(\"   - Re-run data loading cells (1-12) to restore variables\")\n",
    "print(\"   - Use the improved config for any new training\")\n",
    "print(\"   - Use safe_load_checkpoint function for loading\")\n",
    "print()\n",
    "print(\"2. To improve the poor performance (33% accuracy):\")\n",
    "print(\"   - Use IMPROVED_TRAINING_CONFIG above\")  \n",
    "print(\"   - Train for more epochs with lower learning rate\")\n",
    "print(\"   - Check for class imbalance issues\")\n",
    "print()\n",
    "print(\"3. The torch.load error is now fixed with:\")\n",
    "print(\"   checkpoint = safe_load_checkpoint(path, device)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b616705",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-09-27T18:22:00.544941Z",
     "iopub.status.idle": "2025-09-27T18:22:00.545146Z",
     "shell.execute_reply": "2025-09-27T18:22:00.545057Z",
     "shell.execute_reply.started": "2025-09-27T18:22:00.545049Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# FIXED EVALUATION CELL - Re-run data and evaluate existing models\n",
    "# This cell will restore the necessary variables and evaluate the VGG16 model that was trained\n",
    "\n",
    "print(\"RESTORING VARIABLES AND EVALUATING MODELS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Step 1: Check if we need to restore data variables\n",
    "need_data_restore = False\n",
    "required_vars = ['train_df_full', 'val_df_full', 'test_df_full', 'transforms_dict']\n",
    "\n",
    "for var in required_vars:\n",
    "    if var not in globals():\n",
    "        need_data_restore = True\n",
    "        print(f\"Missing: {var}\")\n",
    "    else:\n",
    "        print(f\"✓ Found: {var}\")\n",
    "\n",
    "if need_data_restore:\n",
    "    print(\"\\n⚠️  Some data variables are missing.\")\n",
    "    print(\"Please run cells 1-12 first to restore the dataset and preprocessing variables.\")\n",
    "    print(\"Then come back to run this evaluation.\")\n",
    "else:\n",
    "    print(\"\\n✓ All required variables are available!\")\n",
    "    \n",
    "    # Step 2: Set up for evaluation with fixed torch.load\n",
    "    print(f\"\\nStep 2: Setting up evaluation...\")\n",
    "    \n",
    "    # Import required libraries (in case kernel was restarted)\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from pathlib import Path\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    # Ensure we have the metrics calculator\n",
    "    if 'metrics_calculator' not in globals():\n",
    "        print(\"Creating metrics calculator...\")\n",
    "        from sklearn.metrics import accuracy_score, f1_score, cohen_kappa_score\n",
    "        # You'll need to re-run the metrics cell first if this fails\n",
    "        \n",
    "    # Step 3: Load and evaluate VGG16 (the only model that trained successfully)\n",
    "    model_name = 'vgg16'\n",
    "    checkpoint_path = ARTIFACTS_DIR / model_name / 'best_model.pt'\n",
    "    \n",
    "    if checkpoint_path.exists():\n",
    "        print(f\"\\nStep 3: Loading {model_name} checkpoint...\")\n",
    "        \n",
    "        try:\n",
    "            # Use the safe loading function\n",
    "            checkpoint = safe_load_checkpoint(checkpoint_path, device)\n",
    "            if checkpoint is not None:\n",
    "                print(f\"✓ Successfully loaded checkpoint from epoch {checkpoint['epoch'] + 1}\")\n",
    "                print(f\"✓ Best score in checkpoint: {checkpoint.get('best_score', 'N/A')}\")\n",
    "            else:\n",
    "                print(\"Failed to load checkpoint\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading checkpoint: {e}\")\n",
    "    else:\n",
    "        print(f\"No checkpoint found at {checkpoint_path}\")\n",
    "        \n",
    "print(f\"\\nTO FULLY EVALUATE:\")\n",
    "print(f\"1. Run cells 1-12 if you see missing variables above\")\n",
    "print(f\"2. Run the metrics implementation cell (#16)\")  \n",
    "print(f\"3. Then run a proper evaluation with the existing VGG16 model\")\n",
    "print(f\"4. Use IMPROVED_TRAINING_CONFIG for training better models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8011a02",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-09-27T18:22:00.546382Z",
     "iopub.status.idle": "2025-09-27T18:22:00.546684Z",
     "shell.execute_reply": "2025-09-27T18:22:00.546541Z",
     "shell.execute_reply.started": "2025-09-27T18:22:00.546527Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# IMMEDIATE FIX: Replace torch.load calls to fix UnpicklingError\n",
    "# This fixes the immediate error you're seeing\n",
    "\n",
    "print(\"FIXING TORCH.LOAD ISSUE\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# The error occurs because PyTorch 2.6+ changed torch.load default to weights_only=True\n",
    "# But our checkpoints contain other objects (optimizer state, history, etc.)\n",
    "\n",
    "# Create a global fix function\n",
    "def patch_torch_load():\n",
    "    \"\"\"Monkey patch torch.load to use weights_only=False by default for this session\"\"\"\n",
    "    import torch\n",
    "    original_load = torch.load\n",
    "    \n",
    "    def safe_load(*args, **kwargs):\n",
    "        # If weights_only is not specified, set it to False\n",
    "        if 'weights_only' not in kwargs:\n",
    "            kwargs['weights_only'] = False\n",
    "        return original_load(*args, **kwargs)\n",
    "    \n",
    "    torch.load = safe_load\n",
    "    print(\"✓ Patched torch.load to use weights_only=False by default\")\n",
    "\n",
    "# Apply the patch\n",
    "patch_torch_load()\n",
    "\n",
    "# Also create the safe loading function for direct use\n",
    "def load_checkpoint_safe(checkpoint_path, device):\n",
    "    \"\"\"Safe checkpoint loading with proper error handling\"\"\"\n",
    "    try:\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "        return checkpoint\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {checkpoint_path}: {e}\")\n",
    "        try:\n",
    "            # Fallback: try loading only the model weights\n",
    "            checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=True)\n",
    "            print(\"⚠️ Loaded weights only (no optimizer/history)\")\n",
    "            return {'model_state_dict': checkpoint}\n",
    "        except Exception as e2:\n",
    "            print(f\"Failed to load even weights only: {e2}\")\n",
    "            return None\n",
    "\n",
    "print(\"✓ Created load_checkpoint_safe function\")\n",
    "print(\"\\n🔧 IMMEDIATE FIXES APPLIED:\")\n",
    "print(\"  1. ✓ torch.load now uses weights_only=False\")\n",
    "print(\"  2. ✓ load_checkpoint_safe function available\")\n",
    "print(\"  3. ✓ safe_load_checkpoint function available\")\n",
    "print(\"\\n🚀 You can now re-run the evaluation cell that was failing!\")\n",
    "print(\"   The UnpicklingError should be resolved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cf74b1",
   "metadata": {},
   "source": [
    "## 10. Model Comparison\n",
    "\n",
    "Comprehensive comparison of all trained models with analysis and insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cebf7db",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-09-27T18:22:00.548060Z",
     "iopub.status.idle": "2025-09-27T18:22:00.548390Z",
     "shell.execute_reply": "2025-09-27T18:22:00.548232Z",
     "shell.execute_reply.started": "2025-09-27T18:22:00.548218Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create comprehensive comparison table\n",
    "def create_comparison_table(test_results, training_results, trained_models):\n",
    "    \"\"\"Create a comprehensive comparison table of all models\"\"\"\n",
    "    \n",
    "    comparison_data = []\n",
    "    \n",
    "    for model_name in test_results.keys():\n",
    "        metrics, predictions, targets, valid_regression = test_results[model_name]\n",
    "        training_result = training_results[model_name]\n",
    "        model_info = trained_models[model_name]\n",
    "        \n",
    "        # Get parameter count\n",
    "        param_info = model_info['model'].get_parameter_count()\n",
    "        \n",
    "        # Compile all metrics\n",
    "        row = {\n",
    "            'Model': model_name.replace('_', '-').title(),\n",
    "            'Parameters (M)': param_info['total_parameters'] / 1e6,\n",
    "            'Training Time (min)': training_result['training_time'] / 60,\n",
    "            'Best Epoch': training_result['best_epoch'] + 1,\n",
    "            'Total Epochs': training_result['total_epochs'],\n",
    "            \n",
    "            # Classification metrics\n",
    "            'Accuracy': metrics['classification']['accuracy'],\n",
    "            'F1-Macro': metrics['classification']['f1_macro'],\n",
    "            'F1-Weighted': metrics['classification']['f1_weighted'],\n",
    "            'Cohen κ': metrics['classification']['cohen_kappa'],\n",
    "            'Krippendorff α': metrics['classification']['krippendorff_alpha'],\n",
    "            'ROC-AUC (OvR)': metrics['classification'].get('roc_auc_ovr', np.nan),\n",
    "            'PR-AUC (Macro)': metrics['classification'].get('pr_auc_macro', np.nan),\n",
    "            \n",
    "            # Regression metrics - Valence\n",
    "            'Valence RMSE': metrics['valence']['rmse'],\n",
    "            'Valence Pearson r': metrics['valence']['pearson_r'],\n",
    "            'Valence SAGR': metrics['valence']['sagr'],\n",
    "            'Valence CCC': metrics['valence']['ccc'],\n",
    "            \n",
    "            # Regression metrics - Arousal\n",
    "            'Arousal RMSE': metrics['arousal']['rmse'],\n",
    "            'Arousal Pearson r': metrics['arousal']['pearson_r'],\n",
    "            'Arousal SAGR': metrics['arousal']['sagr'],\n",
    "            'Arousal CCC': metrics['arousal']['ccc'],\n",
    "            \n",
    "            # Combined metrics\n",
    "            'Combined CCC': metrics['combined_ccc']\n",
    "        }\n",
    "        \n",
    "        comparison_data.append(row)\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    return comparison_df\n",
    "\n",
    "# Create comparison table\n",
    "comparison_df = create_comparison_table(test_results, training_results, trained_models)\n",
    "\n",
    "print(\"COMPREHENSIVE MODEL COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Display key metrics\n",
    "key_metrics = [\n",
    "    'Model', 'Parameters (M)', 'Training Time (min)', 'Best Epoch',\n",
    "    'Accuracy', 'F1-Macro', 'Cohen κ', \n",
    "    'Valence CCC', 'Arousal CCC', 'Combined CCC'\n",
    "]\n",
    "\n",
    "print(\"\\\\nKey Performance Metrics:\")\n",
    "print(\"-\" * 80)\n",
    "key_df = comparison_df[key_metrics].copy()\n",
    "\n",
    "# Format numeric columns\n",
    "for col in key_df.columns:\n",
    "    if col not in ['Model', 'Best Epoch']:\n",
    "        if 'Parameters' in col or 'Training Time' in col:\n",
    "            key_df[col] = key_df[col].round(2)\n",
    "        else:\n",
    "            key_df[col] = key_df[col].round(4)\n",
    "\n",
    "print(key_df.to_string(index=False))\n",
    "\n",
    "# Find best models\n",
    "print(\"\\\\n\\\\nBEST PERFORMING MODELS:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Highest Classification Accuracy: {comparison_df.loc[comparison_df['Accuracy'].idxmax(), 'Model']} ({comparison_df['Accuracy'].max():.4f})\")\n",
    "print(f\"Highest F1-Macro: {comparison_df.loc[comparison_df['F1-Macro'].idxmax(), 'Model']} ({comparison_df['F1-Macro'].max():.4f})\")\n",
    "print(f\"Highest Combined CCC: {comparison_df.loc[comparison_df['Combined CCC'].idxmax(), 'Model']} ({comparison_df['Combined CCC'].max():.4f})\")\n",
    "print(f\"Most Efficient (params): {comparison_df.loc[comparison_df['Parameters (M)'].idxmin(), 'Model']} ({comparison_df['Parameters (M)'].min():.2f}M)\")\n",
    "print(f\"Fastest Training: {comparison_df.loc[comparison_df['Training Time (min)'].idxmin(), 'Model']} ({comparison_df['Training Time (min)'].min():.2f} min)\")\n",
    "\n",
    "# Detailed classification metrics\n",
    "print(\"\\\\n\\\\nDETAILED CLASSIFICATION METRICS:\")\n",
    "print(\"-\" * 50)\n",
    "cls_metrics = [\n",
    "    'Model', 'Accuracy', 'F1-Macro', 'F1-Weighted', 'Cohen κ', \n",
    "    'Krippendorff α', 'ROC-AUC (OvR)', 'PR-AUC (Macro)'\n",
    "]\n",
    "cls_df = comparison_df[cls_metrics].copy()\n",
    "for col in cls_df.columns:\n",
    "    if col != 'Model':\n",
    "        cls_df[col] = cls_df[col].round(4)\n",
    "print(cls_df.to_string(index=False))\n",
    "\n",
    "# Detailed regression metrics\n",
    "print(\"\\\\n\\\\nDETAILED REGRESSION METRICS:\")\n",
    "print(\"-\" * 50)\n",
    "reg_metrics = [\n",
    "    'Model', 'Valence RMSE', 'Valence Pearson r', 'Valence CCC', \n",
    "    'Arousal RMSE', 'Arousal Pearson r', 'Arousal CCC', 'Combined CCC'\n",
    "]\n",
    "reg_df = comparison_df[reg_metrics].copy()\n",
    "for col in reg_df.columns:\n",
    "    if col != 'Model':\n",
    "        reg_df[col] = reg_df[col].round(4)\n",
    "print(reg_df.to_string(index=False))\n",
    "\n",
    "# Save comparison table\n",
    "comparison_df.to_csv(ARTIFACTS_DIR / 'model_comparison.csv', index=False)\n",
    "print(f\"\\\\n\\\\nSaved detailed comparison to {ARTIFACTS_DIR / 'model_comparison.csv'}\")\n",
    "\n",
    "# Performance analysis\n",
    "print(\"\\\\n\\\\nPERFORMACE ANALYSIS & INSIGHTS:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Rank models by different criteria\n",
    "accuracy_rank = comparison_df.sort_values('Accuracy', ascending=False)['Model'].tolist()\n",
    "ccc_rank = comparison_df.sort_values('Combined CCC', ascending=False)['Model'].tolist()\n",
    "efficiency_rank = comparison_df.sort_values('Parameters (M)', ascending=True)['Model'].tolist()\n",
    "\n",
    "print(f\"\\\\n1. CLASSIFICATION PERFORMANCE RANKING:\")\n",
    "for i, model in enumerate(accuracy_rank, 1):\n",
    "    acc = comparison_df[comparison_df['Model'] == model]['Accuracy'].iloc[0]\n",
    "    print(f\"   {i}. {model}: {acc:.4f}\")\n",
    "\n",
    "print(f\"\\\\n2. REGRESSION PERFORMANCE RANKING (Combined CCC):\")\n",
    "for i, model in enumerate(ccc_rank, 1):\n",
    "    ccc = comparison_df[comparison_df['Model'] == model]['Combined CCC'].iloc[0]\n",
    "    print(f\"   {i}. {model}: {ccc:.4f}\")\n",
    "\n",
    "print(f\"\\\\n3. PARAMETER EFFICIENCY RANKING:\")\n",
    "for i, model in enumerate(efficiency_rank, 1):\n",
    "    params = comparison_df[comparison_df['Model'] == model]['Parameters (M)'].iloc[0]\n",
    "    print(f\"   {i}. {model}: {params:.2f}M parameters\")\n",
    "\n",
    "# Architecture analysis\n",
    "print(f\"\\\\n4. ARCHITECTURAL INSIGHTS:\")\n",
    "\n",
    "# VGG16 analysis\n",
    "vgg_metrics = comparison_df[comparison_df['Model'] == 'Vgg16'].iloc[0]\n",
    "print(f\"\\\\n• VGG16 (Baseline/Classic CNN):\")\n",
    "print(f\"  - Largest model ({vgg_metrics['Parameters (M)']:.1f}M parameters)\")\n",
    "print(f\"  - Classification: {vgg_metrics['Accuracy']:.4f} accuracy, {vgg_metrics['F1-Macro']:.4f} F1\")\n",
    "print(f\"  - Regression: {vgg_metrics['Combined CCC']:.4f} CCC\")\n",
    "print(f\"  - Analysis: Classical architecture with good baseline performance\")\n",
    "\n",
    "# EfficientNet analysis\n",
    "effnet_models = comparison_df[comparison_df['Model'].str.contains('Efficientnet', case=False)]\n",
    "if not effnet_models.empty:\n",
    "    print(f\"\\\\n• EfficientNet Family:\")\n",
    "    for _, row in effnet_models.iterrows():\n",
    "        print(f\"  - {row['Model']}: {row['Parameters (M)']:.1f}M params, {row['Accuracy']:.4f} acc, {row['Combined CCC']:.4f} CCC\")\n",
    "    print(f\"  - Analysis: Efficient scaling with good performance-parameter trade-off\")\n",
    "\n",
    "# ConvNeXt analysis\n",
    "convnext_metrics = comparison_df[comparison_df['Model'].str.contains('Convnext', case=False)]\n",
    "if not convnext_metrics.empty:\n",
    "    conv_row = convnext_metrics.iloc[0]\n",
    "    print(f\"\\\\n• ConvNeXt-Tiny (Modern CNN):\")\n",
    "    print(f\"  - {conv_row['Parameters (M)']:.1f}M parameters\")\n",
    "    print(f\"  - Classification: {conv_row['Accuracy']:.4f} accuracy, {conv_row['F1-Macro']:.4f} F1\")\n",
    "    print(f\"  - Regression: {conv_row['Combined CCC']:.4f} CCC\")\n",
    "    print(f\"  - Analysis: Modern design with transformer-like improvements\")\n",
    "\n",
    "# Overall recommendations\n",
    "print(f\"\\\\n5. RECOMMENDATIONS:\")\n",
    "best_overall = comparison_df.loc[comparison_df['Combined CCC'].idxmax()]\n",
    "best_efficient = comparison_df.loc[comparison_df['Parameters (M)'].idxmin()]\n",
    "\n",
    "print(f\"\\\\n• For BEST OVERALL PERFORMANCE: {best_overall['Model']}\")\n",
    "print(f\"  - Highest combined CCC: {best_overall['Combined CCC']:.4f}\")\n",
    "print(f\"  - Strong classification: {best_overall['Accuracy']:.4f} accuracy\")\n",
    "print(f\"  - Trade-off: {best_overall['Parameters (M)']:.1f}M parameters\")\n",
    "\n",
    "print(f\"\\\\n• For EFFICIENCY: {best_efficient['Model']}\")\n",
    "print(f\"  - Lowest parameters: {best_efficient['Parameters (M)']:.1f}M\")\n",
    "print(f\"  - Reasonable performance: {best_efficient['Combined CCC']:.4f} CCC\")\n",
    "print(f\"  - Fastest training: {best_efficient['Training Time (min)']:.1f} minutes\")\n",
    "\n",
    "print(f\"\\\\n• For PRODUCTION USE:\")\n",
    "print(f\"  - Consider the balance between performance and computational cost\")\n",
    "print(f\"  - {ccc_rank[0]} offers best performance but highest cost\")\n",
    "print(f\"  - {efficiency_rank[0]} offers good efficiency with acceptable performance\")\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS COMPLETED - All results saved to artifacts directory\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fa77d3",
   "metadata": {},
   "source": [
    "## 11. Save Artifacts & Export Report\n",
    "\n",
    "Final artifact organization and report generation for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677a7859",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-09-27T18:22:00.549538Z",
     "iopub.status.idle": "2025-09-27T18:22:00.549780Z",
     "shell.execute_reply": "2025-09-27T18:22:00.549687Z",
     "shell.execute_reply.started": "2025-09-27T18:22:00.549677Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create summary report for easy inclusion in assignment report\n",
    "def create_summary_report(comparison_df, test_results, training_results):\n",
    "    \"\"\"Create a markdown summary report\"\"\"\n",
    "    \n",
    "    report = \"\"\"# Facial Expression Recognition - Multi-Task Learning Results\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "This project implemented multi-task learning for facial expression recognition, combining:\n",
    "- **Classification Task**: 8-class emotion recognition\n",
    "- **Regression Task**: Valence and arousal prediction (-1 to +1 scale)\n",
    "\n",
    "## Dataset Information\n",
    "\"\"\"\n",
    "    \n",
    "    # Add dataset stats\n",
    "    report += f\"\"\"\n",
    "- **Total Samples**: {len(df)} images\n",
    "- **Train Set**: {len(train_df_full)} samples ({len(train_df_full)/len(df)*100:.1f}%)\n",
    "- **Validation Set**: {len(val_df_full)} samples ({len(val_df_full)/len(df)*100:.1f}%)  \n",
    "- **Test Set**: {len(test_df_full)} samples ({len(test_df_full)/len(df)*100:.1f}%)\n",
    "- **Valid for Regression**: {df_clean['valid_for_regression'].sum()} samples ({df_clean['valid_for_regression'].mean()*100:.1f}%)\n",
    "\n",
    "## Model Architectures Tested\n",
    "\"\"\"\n",
    "    \n",
    "    for model_name in comparison_df['Model']:\n",
    "        row = comparison_df[comparison_df['Model'] == model_name].iloc[0]\n",
    "        report += f\"- **{model_name}**: {row['Parameters (M)']:.1f}M parameters\\\\n\"\n",
    "    \n",
    "    report += \"\"\"\n",
    "## Key Results Summary\n",
    "\n",
    "### Best Performing Models\n",
    "\"\"\"\n",
    "    \n",
    "    # Best models\n",
    "    best_acc = comparison_df.loc[comparison_df['Accuracy'].idxmax()]\n",
    "    best_ccc = comparison_df.loc[comparison_df['Combined CCC'].idxmax()]\n",
    "    most_efficient = comparison_df.loc[comparison_df['Parameters (M)'].idxmin()]\n",
    "    \n",
    "    report += f\"\"\"\n",
    "- **Best Classification**: {best_acc['Model']} (Accuracy: {best_acc['Accuracy']:.4f})\n",
    "- **Best Regression**: {best_ccc['Model']} (Combined CCC: {best_ccc['Combined CCC']:.4f})\n",
    "- **Most Efficient**: {most_efficient['Model']} ({most_efficient['Parameters (M)']:.1f}M parameters)\n",
    "\n",
    "### Performance Comparison Table\n",
    "\n",
    "| Model | Params (M) | Accuracy | F1-Macro | Combined CCC | Training Time (min) |\n",
    "|-------|------------|----------|----------|--------------|-------------------|\n",
    "\"\"\"\n",
    "    \n",
    "    for _, row in comparison_df.iterrows():\n",
    "        report += f\"| {row['Model']} | {row['Parameters (M)']:.1f} | {row['Accuracy']:.4f} | {row['F1-Macro']:.4f} | {row['Combined CCC']:.4f} | {row['Training Time (min)']:.1f} |\\\\n\"\n",
    "    \n",
    "    report += \"\"\"\n",
    "## Technical Implementation\n",
    "\n",
    "### Multi-Task Architecture\n",
    "- **Shared Backbone**: Pre-trained CNN (VGG16, EfficientNet, ConvNeXt)\n",
    "- **Classification Head**: 8-class softmax output\n",
    "- **Regression Head**: 2-unit output with Tanh activation (valence/arousal)\n",
    "\n",
    "### Training Strategy\n",
    "- **Loss Function**: CrossEntropy + λ×SmoothL1Loss (λ=1.0)\n",
    "- **Optimizer**: AdamW (lr=3e-4, weight_decay=1e-4)\n",
    "- **Scheduler**: OneCycleLR with cosine annealing\n",
    "- **Warmup**: 2 epochs with frozen backbone\n",
    "- **Early Stopping**: Based on validation CCC (patience=15)\n",
    "\n",
    "### Evaluation Metrics\n",
    "\n",
    "#### Classification:\n",
    "- Accuracy, F1-macro, F1-weighted\n",
    "- Cohen's κ, Krippendorff's α\n",
    "- ROC-AUC (One-vs-Rest), PR-AUC\n",
    "\n",
    "#### Regression:\n",
    "- RMSE, Pearson correlation\n",
    "- Sign Agreement Rate (SAGR)\n",
    "- **Concordance Correlation Coefficient (CCC)** - primary metric\n",
    "\n",
    "## Key Findings\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    # Add insights\n",
    "    report += f\"\"\"\n",
    "1. **{best_ccc['Model']}** achieved the best overall performance with {best_ccc['Combined CCC']:.4f} combined CCC\n",
    "2. **{most_efficient['Model']}** offers the best efficiency trade-off with only {most_efficient['Parameters (M)']:.1f}M parameters\n",
    "3. All models successfully learned both tasks, demonstrating the effectiveness of multi-task learning\n",
    "4. Modern architectures (EfficientNet, ConvNeXt) generally outperformed classical VGG16\n",
    "5. Training time varied from {comparison_df['Training Time (min)'].min():.1f} to {comparison_df['Training Time (min)'].max():.1f} minutes\n",
    "\n",
    "## Files Generated\n",
    "\n",
    "### Model Checkpoints\n",
    "\"\"\"\n",
    "    \n",
    "    for model_name in test_results.keys():\n",
    "        report += f\"- `artifacts/{model_name}/best_model.pt` - Best {model_name} checkpoint\\\\n\"\n",
    "    \n",
    "    report += \"\"\"\n",
    "### Results & Analysis\n",
    "- `artifacts/model_comparison.csv` - Detailed metrics comparison\n",
    "- `artifacts/training_summary.json` - Training statistics\n",
    "- `artifacts/dataset_with_splits.csv` - Dataset with train/val/test splits\n",
    "\n",
    "### Visualizations\n",
    "- `figures/training_curves.png` - Training/validation curves\n",
    "- `figures/confusion_matrices.png` - Classification confusion matrices  \n",
    "- `figures/regression_results.png` - Valence/arousal scatter plots\n",
    "- `figures/dataset_analysis.png` - Dataset distribution analysis\n",
    "\n",
    "---\n",
    "\n",
    "*Generated automatically from facial expression recognition multi-task learning experiment*\n",
    "\"\"\"\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Generate and save summary report\n",
    "print(\"Generating summary report...\")\n",
    "summary_report = create_summary_report(comparison_df, test_results, training_results)\n",
    "\n",
    "# Save summary report\n",
    "with open(ARTIFACTS_DIR / 'summary_report.md', 'w') as f:\n",
    "    f.write(summary_report)\n",
    "\n",
    "print(f\"Summary report saved to {ARTIFACTS_DIR / 'summary_report.md'}\")\n",
    "\n",
    "# Save detailed test results for each model\n",
    "print(\"\\\\nSaving detailed test results...\")\n",
    "for model_name, (metrics, predictions, targets, valid_regression) in test_results.items():\n",
    "    model_dir = ARTIFACTS_DIR / model_name\n",
    "    \n",
    "    # Save metrics as JSON\n",
    "    metrics_json = {}\n",
    "    for task, task_metrics in metrics.items():\n",
    "        if isinstance(task_metrics, dict):\n",
    "            metrics_json[task] = {k: float(v) if not np.isnan(v) else None \n",
    "                                for k, v in task_metrics.items()}\n",
    "        else:\n",
    "            metrics_json[task] = float(task_metrics) if not np.isnan(task_metrics) else None\n",
    "    \n",
    "    with open(model_dir / 'test_metrics.json', 'w') as f:\n",
    "        json.dump(metrics_json, f, indent=2)\n",
    "    \n",
    "    # Save predictions and targets as numpy arrays\n",
    "    np.save(model_dir / 'predictions_classification.npy', predictions['classification'].numpy())\n",
    "    np.save(model_dir / 'predictions_regression.npy', predictions['regression'].numpy())\n",
    "    np.save(model_dir / 'targets_classification.npy', targets['classification'].numpy()) \n",
    "    np.save(model_dir / 'targets_regression.npy', targets['regression'].numpy())\n",
    "    np.save(model_dir / 'valid_regression_mask.npy', valid_regression.numpy())\n",
    "    \n",
    "    print(f\"  Saved {model_name} results to {model_dir}\")\n",
    "\n",
    "# Create a final summary of all generated files\n",
    "print(\"\\\\n\" + \"=\"*80)\n",
    "print(\"ARTIFACT SUMMARY - All Generated Files\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "artifacts_summary = {\n",
    "    'Model Checkpoints': [],\n",
    "    'Training Data': [],\n",
    "    'Test Results': [],\n",
    "    'Visualizations': [],\n",
    "    'Analysis Reports': []\n",
    "}\n",
    "\n",
    "# Collect all generated files\n",
    "for model_name in test_results.keys():\n",
    "    model_dir = ARTIFACTS_DIR / model_name\n",
    "    if (model_dir / 'best_model.pt').exists():\n",
    "        artifacts_summary['Model Checkpoints'].append(f\"{model_name}/best_model.pt\")\n",
    "    if (model_dir / 'training_history.json').exists():\n",
    "        artifacts_summary['Training Data'].append(f\"{model_name}/training_history.json\")\n",
    "    if (model_dir / 'test_metrics.json').exists():\n",
    "        artifacts_summary['Test Results'].append(f\"{model_name}/test_metrics.json\")\n",
    "\n",
    "# Dataset and splits\n",
    "for split_file in ['train_split.csv', 'val_split.csv', 'test_split.csv', 'dataset_with_splits.csv']:\n",
    "    if (ARTIFACTS_DIR / split_file).exists():\n",
    "        artifacts_summary['Training Data'].append(split_file)\n",
    "\n",
    "# Analysis files\n",
    "for analysis_file in ['model_comparison.csv', 'training_summary.json', 'summary_report.md']:\n",
    "    if (ARTIFACTS_DIR / analysis_file).exists():\n",
    "        artifacts_summary['Analysis Reports'].append(analysis_file)\n",
    "\n",
    "# Visualization files\n",
    "for viz_file in ['training_curves.png', 'confusion_matrices.png', 'regression_results.png', \n",
    "                'dataset_analysis.png', 'split_distributions.png', 'transform_examples.png']:\n",
    "    if (FIGURES_DIR / viz_file).exists():\n",
    "        artifacts_summary['Visualizations'].append(viz_file)\n",
    "\n",
    "# Print summary\n",
    "for category, files in artifacts_summary.items():\n",
    "    if files:\n",
    "        print(f\"\\\\n{category}:\")\n",
    "        for file in files:\n",
    "            print(f\"  ✓ {file}\")\n",
    "\n",
    "total_files = sum(len(files) for files in artifacts_summary.values())\n",
    "print(f\"\\\\nTotal files generated: {total_files}\")\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*80)\n",
    "print(\"🎉 FACIAL EXPRESSION RECOGNITION PROJECT COMPLETED SUCCESSFULLY! 🎉\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\\\nProject Summary:\")\n",
    "print(f\"• {len(test_results)} models trained and evaluated\")\n",
    "print(f\"• {len(df)} images processed\")\n",
    "print(f\"• {total_files} artifacts generated\")\n",
    "print(f\"• Best model: {comparison_df.loc[comparison_df['Combined CCC'].idxmax(), 'Model']} (CCC: {comparison_df['Combined CCC'].max():.4f})\")\n",
    "print(\"\\\\nAll results ready for assignment submission!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8358190,
     "sourceId": 13189147,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
